<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ernestas Poškus.io</title>
    <link>http://out13.com/index.xml</link>
    <description>Recent content on Ernestas Poškus.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 Jun 2017 18:11:57 +0300</lastBuildDate>
    <atom:link href="http://out13.com/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Maglev: A Fast and Reliable Software Network Load Balancer</title>
      <link>http://out13.com/paper/maglev-a-fast-and-reliable-software-network-load-balancer/</link>
      <pubDate>Thu, 29 Jun 2017 18:11:57 +0300</pubDate>
      
      <guid>http://out13.com/paper/maglev-a-fast-and-reliable-software-network-load-balancer/</guid>
      <description>

&lt;h2 id=&#34;maglev-fast-and-reliable-network-balancer&#34;&gt;Maglev - fast and reliable network balancer&lt;/h2&gt;

&lt;p&gt;Packets are distributed through ECMP.&lt;/p&gt;

&lt;p&gt;Serves traffic for Google services &amp;amp; GCP.&lt;/p&gt;

&lt;p&gt;Every Google service has 1 or more VIP&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;Maglev associates each VIP with a set of service endpoints and announces it
to the router over BGP.
The router, in turn, announces the VIP to Google backbone.&lt;/p&gt;

&lt;p&gt;Router receives a VIP packet it forwards the packet to 1 of Maglev machines in
the cluster through ECMP since all Maglev machines announce the VIP with the same cost.
When Maglev receives it selects and endpoint from the set of service endpoints associated
with the VIP and encapsulates the packet using GRE.
When packet arrives at the selected service endpoint, it is decapsulated
and consumed. The response when ready is put into an IP packet with source address
being the VIP and the destination being the IP of the user.&lt;/p&gt;

&lt;h3 id=&#34;forwarder&#34;&gt;Forwarder&lt;/h3&gt;

&lt;p&gt;Forwarder receives packets from the NIC, rewrites them with proper GRE/IP headers
and then sends them back to the NIC (Linux kernel is not involved).&lt;/p&gt;

&lt;p&gt;Packets received by the NIC are first processed by the steering module
of the forwarder, which calculates the 5 tuple hash of the packets and
assigns them to different receiving queues depending on the hash value.
Each receiving queue is attached to a packet rewriter thread.&lt;/p&gt;

&lt;p&gt;First packet thread recomputes hash and tries to match each packet to a configured VIP to filter
out unwanted packets.&lt;/p&gt;

&lt;p&gt;Then it looks up the hash value in connection tracking table (hash is recomputed
to avoid cross-thread sync).&lt;/p&gt;

&lt;p&gt;The connection table stores backend selection results for recent connections.
If a match is found and the selected backend is still healthy, the result
reused. Otherwise, thread consults the consistent hashing module and selects
new backend for the packet; it also adds an entry to the connection table for
future packets with the same 5-tuple.
A packet is dropped if no backend is available.&lt;/p&gt;

&lt;p&gt;The forwarder maintains one connection table per packet thread to avoid
access contention.&lt;/p&gt;

&lt;p&gt;After a backend is selected, the packet thread encapsulates the packet with
proper GRE/IP headers and sends it to the attached transmission queue.
The muxing module then pools all transmission queues and passes the
packets to the NIC.&lt;/p&gt;

&lt;h3 id=&#34;structure&#34;&gt;Structure&lt;/h3&gt;

&lt;p&gt;Maglev is a userspace application running on commodity Linux servers.
Since the Linux kernel network stack is rather computationally expensive,
and Maglev doesn&amp;rsquo;t require any of the Linux stack&amp;rsquo;s features, it is
desirable to make Maglev bypass the kernel entirely for packet processing.&lt;/p&gt;

&lt;h4 id=&#34;hashing&#34;&gt;Hashing&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;load balancing: each backend will receive an almost equal number of connections.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;minimal disruption: when the set of backends changes,
a connection will likely be sent to the same backend as it was before.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Maglev hashing is to assign a preference list of all the lookup table positions to each backend.
Then all the backends take turns filling their most-preferred table positions that are still empty,
until the lookup table is completely filled in.&lt;/p&gt;

&lt;h4 id=&#34;failover&#34;&gt;Failover&lt;/h4&gt;

&lt;p&gt;Active-passive pairs provide failure resilience.
Only active machines serve traffic in normal situations.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;ECMP - Equal cost multipath&lt;/p&gt;

&lt;p&gt;DSR - Direct server return&lt;/p&gt;

&lt;p&gt;VIP - Virtual IP address&lt;/p&gt;

&lt;p&gt;GRE - Generic routing encapsulation&lt;/p&gt;

&lt;p&gt;NIC - Network interface card&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>slabtop - kernel slab cache information in real time</title>
      <link>http://out13.com/tools/slabtop-kernel-slab-cache-information-in-real-time/</link>
      <pubDate>Wed, 28 Jun 2017 18:57:58 +0300</pubDate>
      
      <guid>http://out13.com/tools/slabtop-kernel-slab-cache-information-in-real-time/</guid>
      <description>&lt;p&gt;Displays detailed kernel slab cache information by aggregating &lt;code&gt;/proc/slabinfo&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Tool shows a glimpse into kernel data structures.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Sample output below.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;root@ow:~# slabtop -osc | head -n 20
 Active / Total Objects (% used)    : 4649227 / 4694474 (99.0%)
 Active / Total Slabs (% used)      : 153429 / 153429 (100.0%)
 Active / Total Caches (% used)     : 82 / 118 (69.5%)
 Active / Total Size (% used)       : 1259115.61K / 1273939.45K (98.8%)
 Minimum / Average / Maximum Object : 0.01K / 0.27K / 18.50K

  OBJS ACTIVE   USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME
327090 325687   99%    1.05K  10903       30    348896K ext4_inode_cache
1639344 1639083 99%    0.19K  78064       21    312256K dentry
217504 216070   99%    1.00K   6797       32    217504K ecryptfs_inode_cache
196352 192576   98%    0.61K   7552       26    120832K proc_inode_cache
519792 519564   99%    0.10K  13328       39     53312K buffer_head
410976 401655   97%    0.12K  12843       32     51372K kmalloc-128
 76020  72290   95%    0.57K   2715       28     43440K radix_tree_node
 19808  18066   91%    1.00K    619       32     19808K kmalloc-1024
 24668  24322   98%    0.55K    881       28     14096K inode_cache
432640 432640  100%    0.03K   3380      128     13520K kmalloc-32
337416 335756   99%    0.04K   3308      102     13232K ext4_extent_status
 56220  51332   91%    0.20K   2811       20     11244K vm_area_struct
122368 116329   95%    0.06K   1912       64      7648K kmalloc-64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interesting object are &lt;code&gt;ext4_inode_cache&lt;/code&gt; and &lt;code&gt;dentry&lt;/code&gt;.
These are fs cache objects they speed up fs file/directory access.
The &lt;code&gt;ext4_inode_cache&lt;/code&gt; is underlying fs &lt;code&gt;kmem_cache&lt;/code&gt; structure cache.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Initialized in &lt;code&gt;fs/ext4/super.c&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;static int __init init_inodecache(void)
{
	ext4_inode_cachep = kmem_cache_create(&amp;quot;ext4_inode_cache&amp;quot;,
					     sizeof(struct ext4_inode_info),
					     0, (SLAB_RECLAIM_ACCOUNT|
						SLAB_MEM_SPREAD|SLAB_ACCOUNT),
					     init_once);
	if (ext4_inode_cachep == NULL)
		return -ENOMEM;
	return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;dentry&lt;/code&gt; is &lt;code&gt;kmem_cache&lt;/code&gt; structure cache.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Initialized in &lt;code&gt;kernel/fs/dcache.c&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;static void __init dcache_init(void)
{
	unsigned int loop;

	/*
	 * A constructor could be added for stable state like the lists,
	 * but it is probably not worth it because of the cache nature
	 * of the dcache.
	 */
	dentry_cache = KMEM_CACHE(dentry,
		SLAB_RECLAIM_ACCOUNT|SLAB_PANIC|SLAB_MEM_SPREAD|SLAB_ACCOUNT);

	/* Hash may have been set up in dcache_init_early */
	if (!hashdist)
		return;

	dentry_hashtable =
		alloc_large_system_hash(&amp;quot;Dentry cache&amp;quot;,
					sizeof(struct hlist_bl_head),
					dhash_entries,
					13,
					0,
					&amp;amp;d_hash_shift,
					&amp;amp;d_hash_mask,
					0,
					0);

	for (loop = 0; loop &amp;lt; (1U &amp;lt;&amp;lt; d_hash_shift); loop++)
		INIT_HLIST_BL_HEAD(dentry_hashtable + loop);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These objects are freed automatically by kernel if there is memory pressure.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;To forcefully clean system slab cache.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;echo 3 &amp;gt; /proc/sys/vm/drop_caches # free pagecache, dentries and inodes
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Spanner: Google’s Globally-Distributed Database</title>
      <link>http://out13.com/paper/spanner-google-globally-distributed-database/</link>
      <pubDate>Thu, 25 May 2017 19:53:19 +0300</pubDate>
      
      <guid>http://out13.com/paper/spanner-google-globally-distributed-database/</guid>
      <description>

&lt;h2 id=&#34;spanner&#34;&gt;Spanner&lt;/h2&gt;

&lt;p&gt;Spanner is a scalable, globally-distributed database designed, built, and deployed at Google.&lt;/p&gt;

&lt;p&gt;At the highest level of abstraction, it is a database that shards data across many sets of Paxos state machines.&lt;/p&gt;

&lt;p&gt;Replication is used for global availability and geographic locality.&lt;/p&gt;

&lt;p&gt;Spanner is designed to scale up to millions of machines across hundreds of datacenters and trillions of database rows.&lt;/p&gt;

&lt;p&gt;Data is stored in schematized semi-relational tables; data is versioned,
and each version is automatically timestamped with its commit time; old versions of
data are subject to configurable garbage-collection policies; and applications can read data at old timestamps.&lt;/p&gt;

&lt;p&gt;Major features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;externally consistent reads and writes&lt;/li&gt;
&lt;li&gt;globally-consistent reads across the database at a timestamp&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;implementation&#34;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;A Spanner deployment is called a universe. Given that Spanner manages data globally,
there will be only a handful of running universes.&lt;/p&gt;

&lt;p&gt;Spanner is organized as a set of zones, where each zone is the rough analog of a deployment of Bigtable.&lt;/p&gt;

&lt;p&gt;Zones are the unit of administrative deployment.&lt;/p&gt;

&lt;p&gt;Zones are also the unit of physical isolation: there may be one or more zones in a datacenter, for example,
if different applications’ data must be partitioned across different sets of servers in the same datacenter.&lt;/p&gt;

&lt;p&gt;Each spanserver is responsible for between 100 and 1000 instances of a data structure called a tablet.
A tablet is similar to Bigtable’s tablet abstraction, in that it implements a bag of the following mappings:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(key:string, timestamp:int64) → string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unlike Bigtable, Spanner assigns timestamps to data, which is an important way in which Spanner is more
like a multi-version database than a key-value store.&lt;/p&gt;

&lt;p&gt;Tablet state is stored in set of B-tree-like files and a write-ahead log,
all on a distributed file system called Colossus.&lt;/p&gt;

&lt;p&gt;To support replication, each spanserver implements a
single Paxos state machine on top of each tablet.&lt;/p&gt;

&lt;h3 id=&#34;truetime&#34;&gt;TrueTime&lt;/h3&gt;

&lt;p&gt;TrueTime explicitly represents time as a TTinterval, which is an interval with bounded time
uncertainty (unlike standard time interfaces that give clients no notion of uncertainty).&lt;/p&gt;

&lt;p&gt;The time epoch is anal-
ogous to UNIX time with leap-second smearing. De-
fine the instantaneous error bound as ϵ, which is half of
the interval’s width, and the average error bound as ϵ.&lt;/p&gt;

&lt;p&gt;TrueTime is implemented by a set of time master machines per datacenter and a timeslave daemon per machine.
The majority of masters have GPS receivers with dedicated antennas;
these masters are separated physically to reduce the effects of antenna failures, radio interference, and spoofing.&lt;/p&gt;

&lt;p&gt;The remaining masters (which we refer to as Armageddon masters) are equipped with
atomic clocks. An atomic clock is not that expensive: the cost of an Armageddon master
is of the same order as that of a GPS master.&lt;/p&gt;

&lt;p&gt;Between synchronizations, a daemon advertises a slowly increasing time uncertainty.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Colossus - the successor of GFS&lt;/p&gt;

&lt;p&gt;GFS - Google File System&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>In Search of an Understandable Consensus Algorithm</title>
      <link>http://out13.com/paper/in-search-of-an-understandable-consensus-algorithm/</link>
      <pubDate>Thu, 20 Apr 2017 19:13:57 +0300</pubDate>
      
      <guid>http://out13.com/paper/in-search-of-an-understandable-consensus-algorithm/</guid>
      <description>

&lt;h2 id=&#34;raft&#34;&gt;Raft&lt;/h2&gt;

&lt;p&gt;Consensus algorithm for managing a replicated log.&lt;/p&gt;

&lt;p&gt;Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces
a stronger degree of coherency to reduce the number of states that must be considered.&lt;/p&gt;

&lt;p&gt;Paxos first defines a protocol capable of reaching agreement on a single decision,
such as a single replicated log entry.&lt;/p&gt;

&lt;p&gt;Raft implements consensus by first electing a distinguished leader,
then giving the leader complete responsibility for managing the replicated log.
The leader accepts log entries from clients, replicates them on other servers,
and tells servers when it is safe to apply log entries to
their state machines.&lt;/p&gt;

&lt;p&gt;A leader can fail or become disconnected from the other servers, in which case
a new leader is elected.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Election Safety: at most one leader can be elected in a given term.&lt;/li&gt;
&lt;li&gt;Leader Append-Only: a leader never overwrites or deletes entries in its log; it only appends new entries.&lt;/li&gt;
&lt;li&gt;Log Matching: if two logs contain an entry with the same index and term, then the logs are identical in all entries up through the given index.&lt;/li&gt;
&lt;li&gt;Leader Completeness: if a log entry is committed in a given term, then that entry will be present in the logs of the leaders for all higher-numbered terms.&lt;/li&gt;
&lt;li&gt;State Machine Safety: if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;basics&#34;&gt;Basics&lt;/h3&gt;

&lt;p&gt;A Raft cluster contains several servers; five is a typical number, which allows the system to tolerate two failures.
At any given time each server is in one of three states: leader, follower, or candidate.&lt;/p&gt;

&lt;p&gt;To prevent split votes in the first place, election timeouts are chosen randomly from a fixed interval (e.g., 150–300ms).&lt;/p&gt;

&lt;p&gt;Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LIRS: An Efficient Low Inter-reference Recency Set Replacement Policy to Improve Buffer Cache Performance</title>
      <link>http://out13.com/paper/lirs-efficient-low-inter-reference-recency-set-replacement-policy-to-improve-buffer-cache-performance/</link>
      <pubDate>Thu, 09 Mar 2017 19:34:52 +0200</pubDate>
      
      <guid>http://out13.com/paper/lirs-efficient-low-inter-reference-recency-set-replacement-policy-to-improve-buffer-cache-performance/</guid>
      <description>

&lt;h3 id=&#34;lirs&#34;&gt;LIRS&lt;/h3&gt;

&lt;p&gt;LRU replacement policy has been commonly used in the buffer cache management,
it is well known for its inability to cope with access patterns with weak locality.&lt;/p&gt;

&lt;p&gt;LIRS effectively addresses the limits of LRU by using recency to evaluate Inter-Reference
Recency (IRR) for making a replacement decision.&lt;/p&gt;

&lt;h4 id=&#34;lru-inefficiency&#34;&gt;LRU inefficiency&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Under the LRU policy, a burst of references to infrequently used blocks such
as “sequential scans” through a large file, may cause
replacement of commonly referenced blocks in the cache.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For a cyclic (loop-like) pattern of accesses to a file that is only slightly
larger than the cache size, LRU always mistakenly evicts the blocks that will
be accessed soonest, because these blocks have not been accessed for the longest time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The reason for LRU to behave poorly in these situations is
that LRU makes a bold assumption – a block that has not
been accessed the longest would wait for relatively longest
time to be accessed again.&lt;/p&gt;

&lt;h4 id=&#34;implementation&#34;&gt;Implementation&lt;/h4&gt;

&lt;p&gt;IRR as the recorded history information of each block, where IRR of a block
refers to the number of other blocks accessed between two consecutive references
to the block.&lt;/p&gt;

&lt;p&gt;Specifically, the recency refers to the number of other blocks accessed from
last reference to the current time.&lt;/p&gt;

&lt;p&gt;It is assumed that if the IRR of a block is large,
the next IRR of the block is likely to be large again.
Following this assumption, we select the blocks with large IRRs
for replacement, because these blocks are highly possible to
be evicted later by LRU before being referenced again under our assumption.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;LIRS - low inter reference set&lt;/p&gt;

&lt;p&gt;IRR - inter reference recency&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Serving fresh assets using Nginx location rewrite</title>
      <link>http://out13.com/posts/serving-fresh-assets-using-nginx-rewrite/</link>
      <pubDate>Wed, 08 Mar 2017 08:44:03 +0200</pubDate>
      
      <guid>http://out13.com/posts/serving-fresh-assets-using-nginx-rewrite/</guid>
      <description>

&lt;p&gt;Recently I have stumbled upon a problem to serve fresh/new assets for user web application.&lt;/p&gt;

&lt;p&gt;As Phil Karlton said:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;There are only two hard things in Computer Science: cache invalidation and naming things.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Historically fresh assets problem was approached either by appending appending
url query params (?v=20130102) or renaming/hashing asset file completely (/css/default-2j9alkjan2k2.css).&lt;/p&gt;

&lt;p&gt;Former is most popular one but not elegant since it brings explicit dependency
for backend application what fresh/new asset file to include thus requires exact
name file to be present on web server.&lt;/p&gt;

&lt;p&gt;This draws 5 main disadvantages of completely hashed asset name:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;explicit dependency to include&lt;/li&gt;
&lt;li&gt;no fallback mechanism&lt;/li&gt;
&lt;li&gt;hashed asset name&lt;/li&gt;
&lt;li&gt;exact file name presence on web server&lt;/li&gt;
&lt;li&gt;removal of stale assets&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I came with solution to use Nginx rewrite block that implicitly drops hash of requested file and serves requested asset.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;   location @css_assets {
       rewrite ^/css/(.*)\..*\.(.*)$ /css/$1.$2 last;
   }
   location /css/ {
       try_files $uri $uri/ @css_assets;
   }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First &lt;code&gt;location /css/ {&lt;/code&gt; block matches path of /css/ which later executes
&lt;code&gt;try_files&lt;/code&gt; followed by &lt;code&gt;location @css_assets {&lt;/code&gt; location block.&lt;/p&gt;

&lt;p&gt;Secondly this rewrite &lt;code&gt;rewrite ^/css/(.*)\..*\.(.*)$ /css/$1.$2 last;&lt;/code&gt; matches
beginning of /css/ path followed by 2 tracked matches.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;/css/app.117c7f2fa4b6ea7a2c077a3dbc9662e6b1c278bd.css&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In above example first match tracks (app) and second one (css).
Matched information constructs new implicitly requested file like below.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;/css/app.css&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Newly constructed file will be processed by Nginx without redirects and browser
knowing original file name.&lt;/p&gt;

&lt;h3 id=&#34;constructing-asset-hash&#34;&gt;Constructing asset hash&lt;/h3&gt;

&lt;p&gt;To tell your application which asset must be served use ENVIRONMENT variable and checksum
of asset to be included. Or you can dynamically invalidate/create asset hash for
example hourly or daily depending on release cycle.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;/css/app.$CSS_ASSET_HASH.css&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In my case I use simple function below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;lazy_static! {
    static ref CSS_ASSETS_HASH: String = {
        match env::var(&amp;quot;CSS_ASSETS_HASH&amp;quot;) {
            Ok(hash) =&amp;gt; format!(&amp;quot;.{}.&amp;quot;, hash),
            Err(_) =&amp;gt; &amp;quot;.&amp;quot;.to_string(),
        }
    };
}

html! {
  (Css(format!(&amp;quot;/css/app{}css&amp;quot;, *CSS_ASSETS_HASH)))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Above example is actual code used in production. It tries to statically initialize
&lt;code&gt;CSS_ASSETS_HASH&lt;/code&gt; variable, if expected environment is not defined it fallbacks to
dot &lt;code&gt;.&lt;/code&gt; else it appends 2 dots between supplied environment variable &lt;code&gt;.ENVIRONMENT_VARIABLE.&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;My solution eliminates almost all of main disadvantages of most popular way of asset
inclusion.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;del&gt;explicit dependency to include&lt;/del&gt; - depends if hash is generated
dynamically or real checksum of asset file is used.&lt;/li&gt;
&lt;li&gt;&lt;del&gt;no fallback mechanism&lt;/del&gt; - if environment is not defined or any hash is
supplied it still fallbacks to original requested asset due to regex
catchall.&lt;/li&gt;
&lt;li&gt;&lt;del&gt;hashed asset name&lt;/del&gt; - asset name is explicit and easily understood&lt;/li&gt;
&lt;li&gt;exact file name presence on web server&lt;/li&gt;
&lt;li&gt;&lt;del&gt;removal of stale assets&lt;/del&gt; - only original file is deployed&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Regards.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Large-scale cluster management at Google with Borg</title>
      <link>http://out13.com/paper/large-scale-cluster-management-at-google-with-borg/</link>
      <pubDate>Thu, 09 Feb 2017 20:27:52 +0200</pubDate>
      
      <guid>http://out13.com/paper/large-scale-cluster-management-at-google-with-borg/</guid>
      <description>

&lt;h2 id=&#34;borg&#34;&gt;Borg&lt;/h2&gt;

&lt;p&gt;Cluster manager that runs hundreds of thousands of jobs, from many thousands of
different applications, across a number of clusters each with up to tens of thousands of machines.&lt;/p&gt;

&lt;p&gt;3 main benefits:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;hides the details of resource management and failure handling so its users can
focus on application development instead&lt;/li&gt;
&lt;li&gt;operates with very high reliability and availability, and supports applications that do the same&lt;/li&gt;
&lt;li&gt;lets us run workloads across tens of thousands of machines effectively&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A key design feature in Borg is that already-running tasks
continue to run even if the Borgmaster or a task’s Borglet
goes down. But keeping the master up is still important
because when it is down new jobs cannot be submitted
or existing ones updated, and tasks from failed machines
cannot be rescheduled.&lt;/p&gt;

&lt;p&gt;Each job runs in one Borg cell, a set of machines that are managed as a unit.&lt;/p&gt;

&lt;p&gt;The machines in a cell belong to a single cluster. A cluster lives inside a single datacenter
building, and a collection of buildings makes up a site.&lt;/p&gt;

&lt;p&gt;Median cell size is about 10 k machines after excluding test cells; some are
much larger.
The machines in a cell are heterogeneous in many dimensions: sizes (CPU, RAM,
disk, network), processor type, performance, and capabilities such as an
external IP address or flash storage.
Borg isolates users from most of these differences by determining where in a
cell to run tasks, allocating their resources, installing their programs and
other dependencies, monitoring their health, and restarting them if they fail.&lt;/p&gt;

&lt;h3 id=&#34;jobs&#34;&gt;Jobs&lt;/h3&gt;

&lt;p&gt;A Borg alloc (short for allocation) is a reserved set of resources on a machine
in which one or more tasks can be run; the resources remain assigned whether or
not they are used.&lt;/p&gt;

&lt;p&gt;Quota is used to decide which jobs to admit for scheduling.
Quota is expressed as a vector of resource quantities (CPU, RAM, disk, etc.)
at a given priority, for a period of time (typically months).&lt;/p&gt;

&lt;p&gt;Every job has a priority, a small positive integer. A high priority task
can obtain resources at the expense of a lower priority one,
even if that involves preempting (killing) the latter.&lt;/p&gt;

&lt;h3 id=&#34;naming-and-monitoring&#34;&gt;Naming and monitoring&lt;/h3&gt;

&lt;p&gt;BNS (DNS) for Borg jobs for each task that includes the cell name, job name, and task number.
Borg writes the task’s hostname and port into a consistent,
highly-available file in Chubby with this name, which
is used by our RPC system to find the task endpoint.&lt;/p&gt;

&lt;p&gt;Borg also writes job size and task health information into
Chubby whenever it changes, so load balancers can see
where to route requests to.&lt;/p&gt;

&lt;p&gt;Borg monitors the health-check URL and restarts
tasks that do not respond promptly or return an HTTP error code.&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;A Borg cell consists of a set of machines, a logically centralized
controller called the Borgmaster, and an agent process
called the Borglet that runs on each machine in a cell.&lt;/p&gt;

&lt;p&gt;Borgmaster process handles client RPCs that either
mutate state (e.g., create job) or provide read-only access
to data (e.g., lookup job).&lt;/p&gt;

&lt;h2 id=&#34;scheduling&#34;&gt;Scheduling&lt;/h2&gt;

&lt;p&gt;The scheduling algorithm has two parts: feasibility checking, to find
machines on which the task could run, and scoring, which picks
one of the feasible machines.&lt;/p&gt;

&lt;p&gt;To reduce task startup time, the scheduler prefers to assign
tasks to machines that already have the necessary packages.&lt;/p&gt;

&lt;p&gt;Borg distributes packages to machines in parallel using tree-
and torrent-like protocols.&lt;/p&gt;

&lt;h2 id=&#34;borglet&#34;&gt;Borglet&lt;/h2&gt;

&lt;p&gt;Borglet is a local Borg agent that is present on every
machine in a cell. It starts and stops tasks; restarts them if
they fail; manages local resources by manipulating OS kernel settings;
rolls over debug logs; and reports the state of the
machine to the Borgmaster and other monitoring systems.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;BNS - Borg name system&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Aerospike: Architecture of a Real-Time Operational DBMS</title>
      <link>http://out13.com/paper/aerospike-architecture-of-a-real-time-operational-dbms/</link>
      <pubDate>Sun, 29 Jan 2017 13:47:18 +0200</pubDate>
      
      <guid>http://out13.com/paper/aerospike-architecture-of-a-real-time-operational-dbms/</guid>
      <description>

&lt;h2 id=&#34;aerospike-architecture&#34;&gt;Aerospike architecture&lt;/h2&gt;

&lt;p&gt;Modeled on the classic shared-nothing database architecture&lt;/p&gt;

&lt;p&gt;Objectives of the cluster management subsystem:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Arrive at a single consistent view of current cluster members across all nodes in the cluster.&lt;/li&gt;
&lt;li&gt;Automatically detect new node arrival/departure and seamless cluster reconfiguration.&lt;/li&gt;
&lt;li&gt;Detect network faults and be resilient to such network flakiness.&lt;/li&gt;
&lt;li&gt;Minimize time to detect and adapt to cluster membership changes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;discovery&#34;&gt;Discovery&lt;/h3&gt;

&lt;p&gt;Node arrival or departure is detected via heartbeat messages
exchanged periodically between nodes.&lt;/p&gt;

&lt;h3 id=&#34;surrogate-heartbeats&#34;&gt;Surrogate heartbeats&lt;/h3&gt;

&lt;p&gt;In addition to regular heartbeat messages, nodes use other messages that are regularly exchanged
between nodes as an alternative secondary heartbeat mechanism.&lt;/p&gt;

&lt;h3 id=&#34;node-health-score&#34;&gt;Node Health Score&lt;/h3&gt;

&lt;p&gt;Every node in the cluster evaluates the health score of each of its
neighboring nodes by computing the average message loss, which
is an estimate of how many incoming messages from that node are lost.&lt;/p&gt;

&lt;h3 id=&#34;data-distribution&#34;&gt;Data Distribution&lt;/h3&gt;

&lt;p&gt;A record’s primary key is hashed into a 160-byte digest using the RipeMD160 algorithm.&lt;/p&gt;

&lt;p&gt;Colocated indexes and data to avoid any cross-node traffic when running read operations or queries.&lt;/p&gt;

&lt;p&gt;A partition assignment algorithm generates a replication list for every
partition. The replication list is a permutation of the cluster succession list.&lt;/p&gt;

&lt;p&gt;Reads can also be uniformly spread across all the
replicas via a runtime configuration setting.&lt;/p&gt;

&lt;h3 id=&#34;master-partition-without-data&#34;&gt;Master Partition Without Data&lt;/h3&gt;

&lt;p&gt;An empty node newly added to a running cluster will be master
for a proportional fraction of the partitions and have no data for
those partitions.&lt;/p&gt;

&lt;h3 id=&#34;migration-ordering&#34;&gt;Migration Ordering&lt;/h3&gt;

&lt;h4 id=&#34;smallest-partition-first&#34;&gt;Smallest Partition First&lt;/h4&gt;

&lt;p&gt;Migration is coordinated in such a manner as to let nodes with the
fewest records in their partition versions start migration first. This
strategy quickly reduces the number of different copies of a
specific partition, and does this faster than any other strategy.&lt;/p&gt;

&lt;h4 id=&#34;hottest-partition-first&#34;&gt;Hottest Partition First&lt;/h4&gt;

&lt;p&gt;At times, client accesses are skewed to a very small number of
keys from the key space. Therefore the latency on these accesses
could be improved quickly by migrating these hot partitions
before other partitions.&lt;/p&gt;

&lt;h3 id=&#34;defragmentation&#34;&gt;Defragmentation&lt;/h3&gt;

&lt;p&gt;Aerospike uses a log-structured file system with a copy-on-write
mechanism. Hence, it needs to reclaim space by continuously
running a background defragmentation process. Each device
stores a MAP of block and information relating to the fill-factor of
each block. The fill-factor of the block is the block fraction
utilized by valid records. At boot time, this information is loaded
and kept up-to-date on every write. When the fill-factor of a block
falls below a certain threshold, the block becomes a candidate for
defragmentation and is then queued up for the defragmentation
process.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Approximating Data with the Count-Min Data Structure</title>
      <link>http://out13.com/paper/approximating-data-with-the-count-min-data-structure/</link>
      <pubDate>Thu, 29 Dec 2016 20:25:26 +0200</pubDate>
      
      <guid>http://out13.com/paper/approximating-data-with-the-count-min-data-structure/</guid>
      <description>

&lt;h2 id=&#34;count-min-data-structure&#34;&gt;Count-Min Data Structure&lt;/h2&gt;

&lt;p&gt;Algorithmic problems such as tracking the contents of a set arise frequently in the course of building
systems. Given the variety of possible solutions, the choice of appropriate data structures for
such tasks is at the heart of building efficient and effective software.&lt;/p&gt;

&lt;p&gt;The Count-Min sketch provides a different kind of solution to count tracking.
It allocates a fixed amount of space to store count information, which does not vary over time even
as more and more counts are updated.&lt;/p&gt;

&lt;h3 id=&#34;implementation&#34;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;With all data structures, it is important to understand the data organization
and algorithms for updating the structure, to make clear the relative merits of different choices of
structure for a given task. The Count-Min Sketch data structure primarily consists of a fixed array
of counters, of width w and depth d. The counters are initialized to all zeros. Each row of counters
is associated with a different hash function.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TAO: Facebook’s Distributed Data Store for the Social Graph</title>
      <link>http://out13.com/paper/tao-facebooks-distributed-data-store-for-the-social-graph/</link>
      <pubDate>Thu, 15 Dec 2016 19:36:32 +0200</pubDate>
      
      <guid>http://out13.com/paper/tao-facebooks-distributed-data-store-for-the-social-graph/</guid>
      <description>

&lt;h2 id=&#34;distributed-data-store-for-social-graph&#34;&gt;Distributed data store for social graph&lt;/h2&gt;

&lt;p&gt;TAO is geographically distributed data store that provides efficient and timely
access to the social graph using a fixed set of queries.
Read optimized, persisted in MySQL.&lt;/p&gt;

&lt;p&gt;Inefficient edge lists: A key-value cache is not a good
semantic fit for lists of edges; queries must always fetch
the entire edge list and changes to a single edge require
the entire list to be reloaded.&lt;/p&gt;

&lt;p&gt;Distributed control logic: In a lookaside cache architecture
the control logic is run on clients that don’t communicate
with each other. This increases the number of
failure modes, and makes it difficult to avoid thundering herds.&lt;/p&gt;

&lt;p&gt;Expensive read-after-write consistency: Facebook
uses asynchronous master/slave replication for MySQL,
which poses a problem for caches in data centers using a
replica. Writes are forwarded to the master, but some
time will elapse before they are reflected in the local
replica. By restricting the data model
to objects and associations we can update the replica’s
cache at write time, then use graph semantics to interpret
cache maintenance messages from concurrent updates.&lt;/p&gt;

&lt;h3 id=&#34;data-model-and-api&#34;&gt;Data model and API&lt;/h3&gt;

&lt;p&gt;Facebook focuses on people, actions, and relationships.
We model these entities and connections as nodes and
edges in a graph. This representation is very flexible;
it directly models real-life objects, and can also be used
to store an application’s internal implementation-specific
data.&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;TAO needs to handle a far larger volume of data than can be stored on a
single MySQL server, therefore data is divided into logical shards.&lt;/p&gt;

&lt;h3 id=&#34;mysql-mapping&#34;&gt;MySQL mapping&lt;/h3&gt;

&lt;p&gt;Each shard is assigned to a logical MySQL database
that has a table for objects and a table
for associations. All of the fields of an object are serialized into a
single ‘data‘ column. This approach allows
us to store objects of different types within the same table,
Objects that benefit from separate data management
polices are stored in separate custom tables.
Associations are stored similarly to objects, but to support
range queries, their tables have an additional index
based on id1, atype, and time. To avoid potentially expensive
SELECT COUNT queries, association counts
are stored in a separate table.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Efficient Reconciliation and Flow Control for Anti-Entropy Protocols</title>
      <link>http://out13.com/paper/efficient-reconciliation-and-flow-control-for-anti-entropy-protocols/</link>
      <pubDate>Thu, 01 Dec 2016 16:05:39 +0200</pubDate>
      
      <guid>http://out13.com/paper/efficient-reconciliation-and-flow-control-for-anti-entropy-protocols/</guid>
      <description>

&lt;h2 id=&#34;flow-gossip&#34;&gt;Flow Gossip&lt;/h2&gt;

&lt;p&gt;Anti-entropy, or gossip, is an attractive way of replicating state that does not have strong consistency requirements.
With few limitations, updates spread in expected time that grows logarithmic in the number of participating hosts, even in the face of host failures and message loss.
The behavior of update propagation is easily modeled with well-known epidemic analysis techniques.&lt;/p&gt;

&lt;h3 id=&#34;gossip-basics&#34;&gt;Gossip basics&lt;/h3&gt;

&lt;p&gt;There are two classes of gossip: anti-entropy and rumor mongering protocols.
Anti-entropy protocols gossip information until it is made obsolete by newer information,
and are useful for reliably sharing information among a group of participants.
Rumor-mongering has participants gossip information for some amount of time chosen sufficiently
high so that with high likelihood all participants receive the information.&lt;/p&gt;

&lt;p&gt;3 Gossip styles:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;push: push everything and apply everything&lt;/li&gt;
&lt;li&gt;pull: sends its state with values removed, leaving only keys and version numbers, then returns only necessary updates&lt;/li&gt;
&lt;li&gt;push-pull: like pull but sends a list of participant-key pairs for which if has outdated entries (most efficient)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;precise-reconciliation&#34;&gt;Precise reconciliation&lt;/h3&gt;

&lt;p&gt;The two participants in a gossip exchange send exactly those mappings that are more recent
than those of the peer. Thus, if the participants are p and q, p sends to q the set of deltas.&lt;/p&gt;

&lt;h3 id=&#34;scuttlebutt-reconciliation&#34;&gt;Scuttlebutt reconciliation&lt;/h3&gt;

&lt;p&gt;A gossiper never transmits updates that were already known at the receiver.
If gossip messages were unlimited in size, then the sets contains the exact differences, just like with precise reconciliation.
If a set does not fit in the gossip message, then it is not allowed to use an arbitrary subsetas in precise reconciliation.&lt;/p&gt;

&lt;h3 id=&#34;flow-control&#34;&gt;Flow control&lt;/h3&gt;

&lt;p&gt;The objective of a flow control mechanism for gossip is to determine, adaptively,
the maximum rate at which a participant can submit updates without creating a backlog of updates.
A flow control mechanism should be fair, and under high load afford each participant that wants to submit updates the same update rate.
As there is no global oversight, the flow control mechanism has to be decentralized,
where the desired behavior emerges from participants responding to local events.&lt;/p&gt;

&lt;h3 id=&#34;local-adaptation&#34;&gt;Local adaptation&lt;/h3&gt;

&lt;p&gt;For local adaptation, we use an approach inspired by TCP flow control.
In TCP, the send window adapts according to a strategy called Additive Increase Multiplicative decrease.&lt;/p&gt;

&lt;p&gt;In this strategy, window size grows linearly with each successful transmission,
but is decreased by a certain factor whenever overflow occurs.
In the case of TCP, the overflow signal is the absence of an acknowledgment.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Anti-entropy - gossip information until it is made obsolete.&lt;/p&gt;

&lt;p&gt;Rumor-mongering - gossip information for some of high amount of time with high likelihood all participants received the information.&lt;/p&gt;

&lt;p&gt;AIMD - additive increase multiplicative decrease&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>SEDA: An Architecture for Well-Conditioned, Scalable Internet Services</title>
      <link>http://out13.com/paper/seda-an-architecture-for-well-conditioned-scalable-internet-services/</link>
      <pubDate>Thu, 24 Nov 2016 19:50:13 +0200</pubDate>
      
      <guid>http://out13.com/paper/seda-an-architecture-for-well-conditioned-scalable-internet-services/</guid>
      <description>

&lt;h2 id=&#34;seda-staged-event-driven-architecture&#34;&gt;SEDA - staged event driven architecture&lt;/h2&gt;

&lt;p&gt;A SEDA is intended to support massive concurrency demands and simplify the construction of well-conditioned services.
In SEDA, applications consist of a network of event-driven stages connected by explicit queues.
This architecture allows services to be well-conditioned to load, preventing resources from being overcommitted when demand exceeds service capacity.&lt;/p&gt;

&lt;p&gt;SEDA combines aspects of threads and event-based programming models to manage the concurrency, I/O, scheduling, and resource management needs of Internet services.&lt;/p&gt;

&lt;p&gt;Applications are constructed as a network of stages, each with an associated incoming event queue.
Each stage represents a robust building block that may be individually conditioned to load by thresholding or filtering its event queue.&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;Service is well-conditioned if it behaves like a simple pipeline, where the depth of the pipeline is determined by the path through the network and the processing stages within the service itself.
As the offered load increases, the delivered throughput increases proportionally until the pipeline is full and the throughput saturates; additional load should not degrade throughput.&lt;/p&gt;

&lt;h4 id=&#34;thread-based-concurrency&#34;&gt;Thread based concurrency&lt;/h4&gt;

&lt;p&gt;Operating system overlaps computation and I/O by transparently switching among threads.
Although relatively easy to program, the overheads associated with threading — including cache and TLB misses, scheduling overhead,
and lock contention — can lead to serious performance degradation when the number of threads is large.&lt;/p&gt;

&lt;h4 id=&#34;bounded-thread-pools&#34;&gt;Bounded thread pools&lt;/h4&gt;

&lt;p&gt;To avoid the overuse of threads, a number of systems adopt a coarse form of load conditioning that serves to bound the size of the thread
pool associated with a service. When the number of requests in the server exceeds some fixed limit, additional connections are not accepted.
This approach is used by Web servers such as Apache, IIS, and Netscape Enterprise Server.
By limiting the number of concurrent threads, the server can avoid throughput degradation,
and the overall performance is more robust than the unconstrained thread-per-task model.&lt;/p&gt;

&lt;h4 id=&#34;event-driven-concurrency&#34;&gt;Event-driven concurrency&lt;/h4&gt;

&lt;p&gt;Server consists of a small number of threads (typically one per CPU) that loop continuously, processing events of different types from a queue.
Events may be generated by the operating system or internally by the application,
and generally correspond to network and disk I/O readiness and completion notifications, timers, or other application-specific events.&lt;/p&gt;

&lt;p&gt;Certain I/O operations (in this case, filesystem access) do not have asynchronous interfaces, the main server
process handles these events by dispatching them to helper processes via IPC.
Helper processes issue (blocking) I/O requests and return an event to the main process upon completion.&lt;/p&gt;

&lt;p&gt;Important limitation of this model is that it assumes that event handling threads do not block,
and for this reason nonblocking I/O mechanisms must be employed.&lt;/p&gt;

&lt;h4 id=&#34;structured-event-queues&#34;&gt;Structured event queues&lt;/h4&gt;

&lt;p&gt;Common aspect of these designs is to structure an event-driven application using a
set of event queues to improve code modularity and simplify application design.&lt;/p&gt;

&lt;h4 id=&#34;staged-event-driven-architecture&#34;&gt;Staged event driven architecture&lt;/h4&gt;

&lt;p&gt;Support massive concurrency: To avoid performance degradation due to threads,
SEDA makes use of event-driven execution wherever possible.
This also requires that the system provide efficient and scalable I/O primitives.&lt;/p&gt;

&lt;p&gt;Simplify the construction of well-conditioned services: To reduce the complexity of building Internet services,
SEDA shields application programmers from many of the details of scheduling and resource management.
The design also supports modular construction of these applications, and provides support for debugging and performance profiling.&lt;/p&gt;

&lt;p&gt;Enable introspection: Applications should be able to analyze the request stream to adapt behavior to
changing load conditions. For example, the system should be able to
prioritize and filter requests to support degraded service under heavy load.&lt;/p&gt;

&lt;p&gt;Support self-tuning resource management: Rather than mandate a priori
knowledge of application resource requirements and client load
characteristics, the system should adjust its resource management parameters dynamically
to meet performance targets. For example, the number of threads allocated to
a stage can be determined automatically based on perceived concurrency demands,
rather than hard-coded by the programmer or administrator.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Building blocks&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The fundamental unit of processing within SEDA is the stage.
Stage is a self-contained application component consisting of an event handler, an incoming event queue, and a thread pool.&lt;/p&gt;

&lt;p&gt;The core logic for each stage is provided by the event handler, the input to which is a batch of multiple events.
Event handlers do not have direct control over queue operations or threads.&lt;/p&gt;

&lt;p&gt;Event queues in SEDA is that they may be finite: that is, an enqueue operation may fail
if the queue wishes to reject new entries, say, because it has reached a threshold.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Interaction of Buffer Size and TCP Protocol Handling and its Impact</title>
      <link>http://out13.com/paper/the-interaction-of-buffer-size-and-tcp-protocol-handling/</link>
      <pubDate>Thu, 17 Nov 2016 19:23:07 +0200</pubDate>
      
      <guid>http://out13.com/paper/the-interaction-of-buffer-size-and-tcp-protocol-handling/</guid>
      <description>

&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;

&lt;p&gt;Miercom was engaged by Cisco Systems to conduct independent testing of two vendors’ top of the line,
data-center switch-routers, including the Cisco Nexus 92160YC-X and Nexus 9272Q switches and the Arista 7280SE-72 switch.&lt;/p&gt;

&lt;h4 id=&#34;tcp-congestion-control-versus-system-buffer-management&#34;&gt;TCP Congestion Control versus System Buffer Management&lt;/h4&gt;

&lt;p&gt;TCP congestion control. The Transmission Control Protocol (TCP) is the Layer-4 control
protocol (atop IP at Layer 3) that ensures a block of data that’s sent is received intact.
Invented 35 years ago, TCP handles how blocks of data are broken up, sequenced, sent,
reconstructed and verified at the recipient’s end. The congestion-control mechanism
was added to TCP in 1988 to avoid network congestion meltdown. It makes sure data
transfers are accelerated or slowed down, exploiting the bandwidth that’s available,
depending on network conditions.&lt;/p&gt;

&lt;p&gt;System buffer management. Every network device that transports data has buffers,
usually statically allocated on a per-port basis or dynamically shared by multiple ports, so
that periodic data bursts can be accommodated without having to drop packets.
Network systems such as switch-routers are architected differently, however, and can
vary significantly in the size of their buffers and how they manage different traffic flows.&lt;/p&gt;

&lt;h4 id=&#34;deep-buffer-vs-intelligent-buffer&#34;&gt;Deep buffer vs Intelligent buffer&lt;/h4&gt;

&lt;p&gt;A common practice is to put in as much buffer as possible. However, since the
buffer space is a common resource shared by the inevitable mixture of elephant and mice flows,
how to use this shared resource can significantly impact applications’ performance.&lt;/p&gt;

&lt;p&gt;The deeper the buffer, the longer the queue and the longer the latency. So more buffer does
not necessarily guarantee better small-flow performance, it often leads to longer queuing delay
and hence longer flow completion time.&lt;/p&gt;

&lt;p&gt;Therefore, no one benefits from simple deep buffering: mice flows aren’t guaranteed buffer
resources and can suffer from long queuing delays and bandwidth hungry elephant flows suffer
because large buffers do not create more link bandwidth.&lt;/p&gt;

&lt;h4 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;Since mice flows are often mission critical (including, for example, control and alarm messages,
Hadoop application communications, etc.), giving these flows a priority buffer pathway enables
them to complete faster and their applications to perform better overall. The above test results
show that expediting mice flows and regulating the elephant flows early under the intelligent
buffer architecture on the Cisco Nexus 92160YC-X and 9272Q switches can bring orders of
magnitude better performance for mission critical flows without causing elephant flows to slow
down.&lt;/p&gt;

&lt;p&gt;Intelligent buffering allows the elephant and mice flows to share network buffers gracefully:
there is enough buffer space for the bursts of mice flows while the elephant flows are properly
regulated to fully utilize the link capacity. Simple, deep buffering can lead to collateral damage
in the form of longer queuing latency, and hence longer flow completion time for all flow types.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Elephant - big flows&lt;/p&gt;

&lt;p&gt;Mice - small flows&lt;/p&gt;

&lt;p&gt;FCT - flow completion time&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Replication Under Scalable Hashing: A Family of Algorithms for Scalable Decentralized Data Distribution</title>
      <link>http://out13.com/paper/replication-under-scalable-hashing--a-family-of-algorithms-for-scalable-decentralized-data-distribution/</link>
      <pubDate>Thu, 10 Nov 2016 22:27:23 +0200</pubDate>
      
      <guid>http://out13.com/paper/replication-under-scalable-hashing--a-family-of-algorithms-for-scalable-decentralized-data-distribution/</guid>
      <description>

&lt;h2 id=&#34;replication-under-scalable-hashing&#34;&gt;Replication Under Scalable Hashing&lt;/h2&gt;

&lt;p&gt;Typical algorithms for decentralized data distribution work best in a system that is fully built before it first used;
adding or removing components results in either extensive reorganization of data or load imbalance in the system.&lt;/p&gt;

&lt;p&gt;RUSH variants also support weighting, allowing disks of different vintages to be added to a system.&lt;/p&gt;

&lt;p&gt;RUSH variants is optimal or near-optimal reorganization. When new disks are added to the system,
or old disks are retired, RUSH variants minimize the number of objects that need to
be moved in order to bring the system back into balance.&lt;/p&gt;

&lt;p&gt;RUSH variants can perform reorganization online without locking the filesystem for a long time to relocate data.&lt;/p&gt;

&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;

&lt;p&gt;Subcluster in a system managed by RUSH t must have at least as many disks as an object has replicas.&lt;/p&gt;

&lt;p&gt;RUSH t is the best algorithms for distributing data over very large clusters of disks.&lt;/p&gt;

&lt;p&gt;RUSH r may be the best option for systems which need to remove disks one at a time from the system.&lt;/p&gt;

&lt;p&gt;RUSH p may be the best option for smaller systems where storage space is at a premium.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;RUSH t - RUSH tree&lt;/p&gt;

&lt;p&gt;RUSH r - RUSH support for removal&lt;/p&gt;

&lt;p&gt;PUSH p - RUSH placement using prime numbers&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Dynamo: Amazon’s Highly Available Key-value Store</title>
      <link>http://out13.com/paper/dynamo-amazon-highly-available-key-value-store/</link>
      <pubDate>Sun, 06 Nov 2016 12:32:44 +0200</pubDate>
      
      <guid>http://out13.com/paper/dynamo-amazon-highly-available-key-value-store/</guid>
      <description>

&lt;h2 id=&#34;dynamo&#34;&gt;Dynamo&lt;/h2&gt;

&lt;p&gt;Dynamo sacrifices Consistency for Availability under certain failure scenarios.
It makes extensive use of object versioning and application-assisted conflict resolution in a manner that provides a novel interface for developers to use.&lt;/p&gt;

&lt;p&gt;Gossip based distributed failure detection and membership protocol.&lt;/p&gt;

&lt;h3 id=&#34;query-model&#34;&gt;Query Model&lt;/h3&gt;

&lt;p&gt;Read &amp;amp; Write operations to data item that is uniquely identified by a key.
State is stored as blobs.
Targets application that store objects up to 1MB.&lt;/p&gt;

&lt;h3 id=&#34;acid&#34;&gt;ACID&lt;/h3&gt;

&lt;p&gt;Dynamo targets applications that operate with weaker consistency (the “C” in ACID) if this results in high availability.&lt;/p&gt;

&lt;p&gt;No isolation guarantees. Permits only single key updates.&lt;/p&gt;

&lt;h3 id=&#34;design&#34;&gt;Design&lt;/h3&gt;

&lt;p&gt;Incremental scalability: Dynamo should be able to scale out one storage host (henceforth, referred to as “node”) at a time,
with minimal impact on both operators of the system and the system itself.&lt;/p&gt;

&lt;p&gt;Symmetry: Every node in Dynamo should have the same set of responsibilities as its peers; there should be no distinguished node
or nodes that take special roles or extra set of responsibilities. In our experience, symmetry simplifies the process of system
provisioning and maintenance.&lt;/p&gt;

&lt;p&gt;Decentralization: An extension of symmetry, the design should favor decentralized peer-to-peer techniques over centralized
control. In the past, centralized control has resulted in outages and the goal is to avoid it as much as possible. This leads to a simpler,
more scalable, and more available system.&lt;/p&gt;

&lt;p&gt;Heterogeneity: The system needs to be able to exploit heterogeneity in the infrastructure it runs on. e.g. the work
distribution must be proportional to the capabilities of the individual servers. This is essential in adding new nodes with
higher capacity without having to upgrade all hosts at once.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>