<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ernestas Poškus.io</title>
    <link>http://out13.com/index.xml</link>
    <description>Recent content on Ernestas Poškus.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Apr 2017 19:13:57 +0300</lastBuildDate>
    <atom:link href="http://out13.com/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>In Search of an Understandable Consensus Algorithm</title>
      <link>http://out13.com/paper/in-search-of-an-understandable-consensus-algorithm/</link>
      <pubDate>Thu, 20 Apr 2017 19:13:57 +0300</pubDate>
      
      <guid>http://out13.com/paper/in-search-of-an-understandable-consensus-algorithm/</guid>
      <description>

&lt;h2 id=&#34;raft&#34;&gt;Raft&lt;/h2&gt;

&lt;p&gt;Consensus algorithm for managing a replicated log.&lt;/p&gt;

&lt;p&gt;Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces
a stronger degree of coherency to reduce the number of states that must be considered.&lt;/p&gt;

&lt;p&gt;Paxos first defines a protocol capable of reaching agreement on a single decision,
such as a single replicated log entry.&lt;/p&gt;

&lt;p&gt;Raft implements consensus by first electing a distinguished leader,
then giving the leader complete responsibility for managing the replicated log.
The leader accepts log entries from clients, replicates them on other servers,
and tells servers when it is safe to apply log entries to
their state machines.&lt;/p&gt;

&lt;p&gt;A leader can fail or become disconnected from the other servers, in which case
a new leader is elected.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Election Safety: at most one leader can be elected in a given term.&lt;/li&gt;
&lt;li&gt;Leader Append-Only: a leader never overwrites or deletes entries in its log; it only appends new entries.&lt;/li&gt;
&lt;li&gt;Log Matching: if two logs contain an entry with the same index and term, then the logs are identical in all entries up through the given index.&lt;/li&gt;
&lt;li&gt;Leader Completeness: if a log entry is committed in a given term, then that entry will be present in the logs of the leaders for all higher-numbered terms.&lt;/li&gt;
&lt;li&gt;State Machine Safety: if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;basics&#34;&gt;Basics&lt;/h3&gt;

&lt;p&gt;A Raft cluster contains several servers; five is a typical number, which allows the system to tolerate two failures.
At any given time each server is in one of three states: leader, follower, or candidate.&lt;/p&gt;

&lt;p&gt;To prevent split votes in the first place, election timeouts are chosen randomly from a fixed interval (e.g., 150–300ms).&lt;/p&gt;

&lt;p&gt;Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LIRS: An Efficient Low Inter-reference Recency Set Replacement Policy to Improve Buffer Cache Performance</title>
      <link>http://out13.com/paper/lirs-efficient-low-inter-reference-recency-set-replacement-policy-to-improve-buffer-cache-performance/</link>
      <pubDate>Thu, 09 Mar 2017 19:34:52 +0200</pubDate>
      
      <guid>http://out13.com/paper/lirs-efficient-low-inter-reference-recency-set-replacement-policy-to-improve-buffer-cache-performance/</guid>
      <description>

&lt;h3 id=&#34;lirs&#34;&gt;LIRS&lt;/h3&gt;

&lt;p&gt;LRU replacement policy has been commonly used in the buffer cache management,
it is well known for its inability to cope with access patterns with weak locality.&lt;/p&gt;

&lt;p&gt;LIRS effectively addresses the limits of LRU by using recency to evaluate Inter-Reference
Recency (IRR) for making a replacement decision.&lt;/p&gt;

&lt;h4 id=&#34;lru-inefficiency&#34;&gt;LRU inefficiency&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Under the LRU policy, a burst of references to infrequently used blocks such
as “sequential scans” through a large file, may cause
replacement of commonly referenced blocks in the cache.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For a cyclic (loop-like) pattern of accesses to a file that is only slightly
larger than the cache size, LRU always mistakenly evicts the blocks that will
be accessed soonest, because these blocks have not been accessed for the longest time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The reason for LRU to behave poorly in these situations is
that LRU makes a bold assumption – a block that has not
been accessed the longest would wait for relatively longest
time to be accessed again.&lt;/p&gt;

&lt;h4 id=&#34;implementation&#34;&gt;Implementation&lt;/h4&gt;

&lt;p&gt;IRR as the recorded history information of each block, where IRR of a block
refers to the number of other blocks accessed between two consecutive references
to the block.&lt;/p&gt;

&lt;p&gt;Specifically, the recency refers to the number of other blocks accessed from
last reference to the current time.&lt;/p&gt;

&lt;p&gt;It is assumed that if the IRR of a block is large,
the next IRR of the block is likely to be large again.
Following this assumption, we select the blocks with large IRRs
for replacement, because these blocks are highly possible to
be evicted later by LRU before being referenced again under our assumption.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;LIRS - low inter reference set&lt;/p&gt;

&lt;p&gt;IRR - inter reference recency&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Serving fresh assets using Nginx location rewrite</title>
      <link>http://out13.com/posts/serving-fresh-assets-using-nginx-rewrite/</link>
      <pubDate>Wed, 08 Mar 2017 08:44:03 +0200</pubDate>
      
      <guid>http://out13.com/posts/serving-fresh-assets-using-nginx-rewrite/</guid>
      <description>

&lt;p&gt;Recently I have stumbled upon a problem to serve fresh/new assets for user web application.&lt;/p&gt;

&lt;p&gt;As Phil Karlton said:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;There are only two hard things in Computer Science: cache invalidation and naming things.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Historically fresh assets problem was approached either by appending appending
url query params (?v=20130102) or renaming/hashing asset file completely (/css/default-2j9alkjan2k2.css).&lt;/p&gt;

&lt;p&gt;Former is most popular one but not elegant since it brings explicit dependency
for backend application what fresh/new asset file to include thus requires exact
name file to be present on web server.&lt;/p&gt;

&lt;p&gt;This draws 5 main disadvantages of completely hashed asset name:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;explicit dependency to include&lt;/li&gt;
&lt;li&gt;no fallback mechanism&lt;/li&gt;
&lt;li&gt;hashed asset name&lt;/li&gt;
&lt;li&gt;exact file name presence on web server&lt;/li&gt;
&lt;li&gt;removal of stale assets&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I came with solution to use Nginx rewrite block that implicitly drops hash of requested file and serves requested asset.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;   location @css_assets {
       rewrite ^/css/(.*)\..*\.(.*)$ /css/$1.$2 last;
   }
   location /css/ {
       try_files $uri $uri/ @css_assets;
   }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First &lt;code&gt;location /css/ {&lt;/code&gt; block matches path of /css/ which later executes
&lt;code&gt;try_files&lt;/code&gt; followed by &lt;code&gt;location @css_assets {&lt;/code&gt; location block.&lt;/p&gt;

&lt;p&gt;Secondly this rewrite &lt;code&gt;rewrite ^/css/(.*)\..*\.(.*)$ /css/$1.$2 last;&lt;/code&gt; matches
beginning of /css/ path followed by 2 tracked matches.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;/css/app.117c7f2fa4b6ea7a2c077a3dbc9662e6b1c278bd.css&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In above example first match tracks (app) and second one (css).
Matched information constructs new implicitly requested file like below.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;/css/app.css&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Newly constructed file will be processed by Nginx without redirects and browser
knowing original file name.&lt;/p&gt;

&lt;h3 id=&#34;constructing-asset-hash&#34;&gt;Constructing asset hash&lt;/h3&gt;

&lt;p&gt;To tell your application which asset must be served use ENVIRONMENT variable and checksum
of asset to be included. Or you can dynamically invalidate/create asset hash for
example hourly or daily depending on release cycle.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;/css/app.$CSS_ASSET_HASH.css&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In my case I use simple function below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;lazy_static! {
    static ref CSS_ASSETS_HASH: String = {
        match env::var(&amp;quot;CSS_ASSETS_HASH&amp;quot;) {
            Ok(hash) =&amp;gt; format!(&amp;quot;.{}.&amp;quot;, hash),
            Err(_) =&amp;gt; &amp;quot;.&amp;quot;.to_string(),
        }
    };
}

html! {
  (Css(format!(&amp;quot;/css/app{}css&amp;quot;, *CSS_ASSETS_HASH)))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Above example is actual code used in production. It tries to statically initialize
&lt;code&gt;CSS_ASSETS_HASH&lt;/code&gt; variable, if expected environment is not defined it fallbacks to
dot &lt;code&gt;.&lt;/code&gt; else it appends 2 dots between supplied environment variable &lt;code&gt;.ENVIRONMENT_VARIABLE.&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;My solution eliminates almost all of main disadvantages of most popular way of asset
inclusion.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;del&gt;explicit dependency to include&lt;/del&gt; - depends if hash is generated
dynamically or real checksum of asset file is used.&lt;/li&gt;
&lt;li&gt;&lt;del&gt;no fallback mechanism&lt;/del&gt; - if environment is not defined or any hash is
supplied it still fallbacks to original requested asset due to regex
catchall.&lt;/li&gt;
&lt;li&gt;&lt;del&gt;hashed asset name&lt;/del&gt; - asset name is explicit and easily understood&lt;/li&gt;
&lt;li&gt;exact file name presence on web server&lt;/li&gt;
&lt;li&gt;&lt;del&gt;removal of stale assets&lt;/del&gt; - only original file is deployed&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Regards.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Large-scale cluster management at Google with Borg</title>
      <link>http://out13.com/paper/large-scale-cluster-management-at-google-with-borg/</link>
      <pubDate>Thu, 09 Feb 2017 20:27:52 +0200</pubDate>
      
      <guid>http://out13.com/paper/large-scale-cluster-management-at-google-with-borg/</guid>
      <description>

&lt;h2 id=&#34;borg&#34;&gt;Borg&lt;/h2&gt;

&lt;p&gt;Cluster manager that runs hundreds of thousands of jobs, from many thousands of
different applications, across a number of clusters each with up to tens of thousands of machines.&lt;/p&gt;

&lt;p&gt;3 main benefits:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;hides the details of resource management and failure handling so its users can
focus on application development instead&lt;/li&gt;
&lt;li&gt;operates with very high reliability and availability, and supports applications that do the same&lt;/li&gt;
&lt;li&gt;lets us run workloads across tens of thousands of machines effectively&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A key design feature in Borg is that already-running tasks
continue to run even if the Borgmaster or a task’s Borglet
goes down. But keeping the master up is still important
because when it is down new jobs cannot be submitted
or existing ones updated, and tasks from failed machines
cannot be rescheduled.&lt;/p&gt;

&lt;p&gt;Each job runs in one Borg cell, a set of machines that are managed as a unit.&lt;/p&gt;

&lt;p&gt;The machines in a cell belong to a single cluster. A cluster lives inside a single datacenter
building, and a collection of buildings makes up a site.&lt;/p&gt;

&lt;p&gt;Median cell size is about 10 k machines after excluding test cells; some are
much larger.
The machines in a cell are heterogeneous in many dimensions: sizes (CPU, RAM,
disk, network), processor type, performance, and capabilities such as an
external IP address or flash storage.
Borg isolates users from most of these differences by determining where in a
cell to run tasks, allocating their resources, installing their programs and
other dependencies, monitoring their health, and restarting them if they fail.&lt;/p&gt;

&lt;h3 id=&#34;jobs&#34;&gt;Jobs&lt;/h3&gt;

&lt;p&gt;A Borg alloc (short for allocation) is a reserved set of resources on a machine
in which one or more tasks can be run; the resources remain assigned whether or
not they are used.&lt;/p&gt;

&lt;p&gt;Quota is used to decide which jobs to admit for scheduling.
Quota is expressed as a vector of resource quantities (CPU, RAM, disk, etc.)
at a given priority, for a period of time (typically months).&lt;/p&gt;

&lt;p&gt;Every job has a priority, a small positive integer. A high priority task
can obtain resources at the expense of a lower priority one,
even if that involves preempting (killing) the latter.&lt;/p&gt;

&lt;h3 id=&#34;naming-and-monitoring&#34;&gt;Naming and monitoring&lt;/h3&gt;

&lt;p&gt;BNS (DNS) for Borg jobs for each task that includes the cell name, job name, and task number.
Borg writes the task’s hostname and port into a consistent,
highly-available file in Chubby with this name, which
is used by our RPC system to find the task endpoint.&lt;/p&gt;

&lt;p&gt;Borg also writes job size and task health information into
Chubby whenever it changes, so load balancers can see
where to route requests to.&lt;/p&gt;

&lt;p&gt;Borg monitors the health-check URL and restarts
tasks that do not respond promptly or return an HTTP error code.&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;A Borg cell consists of a set of machines, a logically centralized
controller called the Borgmaster, and an agent process
called the Borglet that runs on each machine in a cell.&lt;/p&gt;

&lt;p&gt;Borgmaster process handles client RPCs that either
mutate state (e.g., create job) or provide read-only access
to data (e.g., lookup job).&lt;/p&gt;

&lt;h2 id=&#34;scheduling&#34;&gt;Scheduling&lt;/h2&gt;

&lt;p&gt;The scheduling algorithm has two parts: feasibility checking, to find
machines on which the task could run, and scoring, which picks
one of the feasible machines.&lt;/p&gt;

&lt;p&gt;To reduce task startup time, the scheduler prefers to assign
tasks to machines that already have the necessary packages.&lt;/p&gt;

&lt;p&gt;Borg distributes packages to machines in parallel using tree-
and torrent-like protocols.&lt;/p&gt;

&lt;h2 id=&#34;borglet&#34;&gt;Borglet&lt;/h2&gt;

&lt;p&gt;Borglet is a local Borg agent that is present on every
machine in a cell. It starts and stops tasks; restarts them if
they fail; manages local resources by manipulating OS kernel settings;
rolls over debug logs; and reports the state of the
machine to the Borgmaster and other monitoring systems.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;BNS - Borg name system&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Aerospike: Architecture of a Real-Time Operational DBMS</title>
      <link>http://out13.com/paper/aerospike-architecture-of-a-real-time-operational-dbms/</link>
      <pubDate>Sun, 29 Jan 2017 13:47:18 +0200</pubDate>
      
      <guid>http://out13.com/paper/aerospike-architecture-of-a-real-time-operational-dbms/</guid>
      <description>

&lt;h2 id=&#34;aerospike-architecture&#34;&gt;Aerospike architecture&lt;/h2&gt;

&lt;p&gt;Modeled on the classic shared-nothing database architecture&lt;/p&gt;

&lt;p&gt;Objectives of the cluster management subsystem:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Arrive at a single consistent view of current cluster members across all nodes in the cluster.&lt;/li&gt;
&lt;li&gt;Automatically detect new node arrival/departure and seamless cluster reconfiguration.&lt;/li&gt;
&lt;li&gt;Detect network faults and be resilient to such network flakiness.&lt;/li&gt;
&lt;li&gt;Minimize time to detect and adapt to cluster membership changes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;discovery&#34;&gt;Discovery&lt;/h3&gt;

&lt;p&gt;Node arrival or departure is detected via heartbeat messages
exchanged periodically between nodes.&lt;/p&gt;

&lt;h3 id=&#34;surrogate-heartbeats&#34;&gt;Surrogate heartbeats&lt;/h3&gt;

&lt;p&gt;In addition to regular heartbeat messages, nodes use other messages that are regularly exchanged
between nodes as an alternative secondary heartbeat mechanism.&lt;/p&gt;

&lt;h3 id=&#34;node-health-score&#34;&gt;Node Health Score&lt;/h3&gt;

&lt;p&gt;Every node in the cluster evaluates the health score of each of its
neighboring nodes by computing the average message loss, which
is an estimate of how many incoming messages from that node are lost.&lt;/p&gt;

&lt;h3 id=&#34;data-distribution&#34;&gt;Data Distribution&lt;/h3&gt;

&lt;p&gt;A record’s primary key is hashed into a 160-byte digest using the RipeMD160 algorithm.&lt;/p&gt;

&lt;p&gt;Colocated indexes and data to avoid any cross-node traffic when running read operations or queries.&lt;/p&gt;

&lt;p&gt;A partition assignment algorithm generates a replication list for every
partition. The replication list is a permutation of the cluster succession list.&lt;/p&gt;

&lt;p&gt;Reads can also be uniformly spread across all the
replicas via a runtime configuration setting.&lt;/p&gt;

&lt;h3 id=&#34;master-partition-without-data&#34;&gt;Master Partition Without Data&lt;/h3&gt;

&lt;p&gt;An empty node newly added to a running cluster will be master
for a proportional fraction of the partitions and have no data for
those partitions.&lt;/p&gt;

&lt;h3 id=&#34;migration-ordering&#34;&gt;Migration Ordering&lt;/h3&gt;

&lt;h4 id=&#34;smallest-partition-first&#34;&gt;Smallest Partition First&lt;/h4&gt;

&lt;p&gt;Migration is coordinated in such a manner as to let nodes with the
fewest records in their partition versions start migration first. This
strategy quickly reduces the number of different copies of a
specific partition, and does this faster than any other strategy.&lt;/p&gt;

&lt;h4 id=&#34;hottest-partition-first&#34;&gt;Hottest Partition First&lt;/h4&gt;

&lt;p&gt;At times, client accesses are skewed to a very small number of
keys from the key space. Therefore the latency on these accesses
could be improved quickly by migrating these hot partitions
before other partitions.&lt;/p&gt;

&lt;h3 id=&#34;defragmentation&#34;&gt;Defragmentation&lt;/h3&gt;

&lt;p&gt;Aerospike uses a log-structured file system with a copy-on-write
mechanism. Hence, it needs to reclaim space by continuously
running a background defragmentation process. Each device
stores a MAP of block and information relating to the fill-factor of
each block. The fill-factor of the block is the block fraction
utilized by valid records. At boot time, this information is loaded
and kept up-to-date on every write. When the fill-factor of a block
falls below a certain threshold, the block becomes a candidate for
defragmentation and is then queued up for the defragmentation
process.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Approximating Data with the Count-Min Data Structure</title>
      <link>http://out13.com/paper/approximating-data-with-the-count-min-data-structure/</link>
      <pubDate>Thu, 29 Dec 2016 20:25:26 +0200</pubDate>
      
      <guid>http://out13.com/paper/approximating-data-with-the-count-min-data-structure/</guid>
      <description>

&lt;h2 id=&#34;count-min-data-structure&#34;&gt;Count-Min Data Structure&lt;/h2&gt;

&lt;p&gt;Algorithmic problems such as tracking the contents of a set arise frequently in the course of building
systems. Given the variety of possible solutions, the choice of appropriate data structures for
such tasks is at the heart of building efficient and effective software.&lt;/p&gt;

&lt;p&gt;The Count-Min sketch provides a different kind of solution to count tracking.
It allocates a fixed amount of space to store count information, which does not vary over time even
as more and more counts are updated.&lt;/p&gt;

&lt;h3 id=&#34;implementation&#34;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;With all data structures, it is important to understand the data organization
and algorithms for updating the structure, to make clear the relative merits of different choices of
structure for a given task. The Count-Min Sketch data structure primarily consists of a fixed array
of counters, of width w and depth d. The counters are initialized to all zeros. Each row of counters
is associated with a different hash function.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TAO: Facebook’s Distributed Data Store for the Social Graph</title>
      <link>http://out13.com/paper/tao-facebooks-distributed-data-store-for-the-social-graph/</link>
      <pubDate>Thu, 15 Dec 2016 19:36:32 +0200</pubDate>
      
      <guid>http://out13.com/paper/tao-facebooks-distributed-data-store-for-the-social-graph/</guid>
      <description>

&lt;h2 id=&#34;distributed-data-store-for-social-graph&#34;&gt;Distributed data store for social graph&lt;/h2&gt;

&lt;p&gt;TAO is geographically distributed data store that provides efficient and timely
access to the social graph using a fixed set of queries.
Read optimized, persisted in MySQL.&lt;/p&gt;

&lt;p&gt;Inefficient edge lists: A key-value cache is not a good
semantic fit for lists of edges; queries must always fetch
the entire edge list and changes to a single edge require
the entire list to be reloaded.&lt;/p&gt;

&lt;p&gt;Distributed control logic: In a lookaside cache architecture
the control logic is run on clients that don’t communicate
with each other. This increases the number of
failure modes, and makes it difficult to avoid thundering herds.&lt;/p&gt;

&lt;p&gt;Expensive read-after-write consistency: Facebook
uses asynchronous master/slave replication for MySQL,
which poses a problem for caches in data centers using a
replica. Writes are forwarded to the master, but some
time will elapse before they are reflected in the local
replica. By restricting the data model
to objects and associations we can update the replica’s
cache at write time, then use graph semantics to interpret
cache maintenance messages from concurrent updates.&lt;/p&gt;

&lt;h3 id=&#34;data-model-and-api&#34;&gt;Data model and API&lt;/h3&gt;

&lt;p&gt;Facebook focuses on people, actions, and relationships.
We model these entities and connections as nodes and
edges in a graph. This representation is very flexible;
it directly models real-life objects, and can also be used
to store an application’s internal implementation-specific
data.&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;TAO needs to handle a far larger volume of data than can be stored on a
single MySQL server, therefore data is divided into logical shards.&lt;/p&gt;

&lt;h3 id=&#34;mysql-mapping&#34;&gt;MySQL mapping&lt;/h3&gt;

&lt;p&gt;Each shard is assigned to a logical MySQL database
that has a table for objects and a table
for associations. All of the fields of an object are serialized into a
single ‘data‘ column. This approach allows
us to store objects of different types within the same table,
Objects that benefit from separate data management
polices are stored in separate custom tables.
Associations are stored similarly to objects, but to support
range queries, their tables have an additional index
based on id1, atype, and time. To avoid potentially expensive
SELECT COUNT queries, association counts
are stored in a separate table.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Efficient Reconciliation and Flow Control for Anti-Entropy Protocols</title>
      <link>http://out13.com/paper/efficient-reconciliation-and-flow-control-for-anti-entropy-protocols/</link>
      <pubDate>Thu, 01 Dec 2016 16:05:39 +0200</pubDate>
      
      <guid>http://out13.com/paper/efficient-reconciliation-and-flow-control-for-anti-entropy-protocols/</guid>
      <description>

&lt;h2 id=&#34;flow-gossip&#34;&gt;Flow Gossip&lt;/h2&gt;

&lt;p&gt;Anti-entropy, or gossip, is an attractive way of replicating state that does not have strong consistency requirements.
With few limitations, updates spread in expected time that grows logarithmic in the number of participating hosts, even in the face of host failures and message loss.
The behavior of update propagation is easily modeled with well-known epidemic analysis techniques.&lt;/p&gt;

&lt;h3 id=&#34;gossip-basics&#34;&gt;Gossip basics&lt;/h3&gt;

&lt;p&gt;There are two classes of gossip: anti-entropy and rumor mongering protocols.
Anti-entropy protocols gossip information until it is made obsolete by newer information,
and are useful for reliably sharing information among a group of participants.
Rumor-mongering has participants gossip information for some amount of time chosen sufficiently
high so that with high likelihood all participants receive the information.&lt;/p&gt;

&lt;p&gt;3 Gossip styles:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;push: push everything and apply everything&lt;/li&gt;
&lt;li&gt;pull: sends its state with values removed, leaving only keys and version numbers, then returns only necessary updates&lt;/li&gt;
&lt;li&gt;push-pull: like pull but sends a list of participant-key pairs for which if has outdated entries (most efficient)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;precise-reconciliation&#34;&gt;Precise reconciliation&lt;/h3&gt;

&lt;p&gt;The two participants in a gossip exchange send exactly those mappings that are more recent
than those of the peer. Thus, if the participants are p and q, p sends to q the set of deltas.&lt;/p&gt;

&lt;h3 id=&#34;scuttlebutt-reconciliation&#34;&gt;Scuttlebutt reconciliation&lt;/h3&gt;

&lt;p&gt;A gossiper never transmits updates that were already known at the receiver.
If gossip messages were unlimited in size, then the sets contains the exact differences, just like with precise reconciliation.
If a set does not fit in the gossip message, then it is not allowed to use an arbitrary subsetas in precise reconciliation.&lt;/p&gt;

&lt;h3 id=&#34;flow-control&#34;&gt;Flow control&lt;/h3&gt;

&lt;p&gt;The objective of a flow control mechanism for gossip is to determine, adaptively,
the maximum rate at which a participant can submit updates without creating a backlog of updates.
A flow control mechanism should be fair, and under high load afford each participant that wants to submit updates the same update rate.
As there is no global oversight, the flow control mechanism has to be decentralized,
where the desired behavior emerges from participants responding to local events.&lt;/p&gt;

&lt;h3 id=&#34;local-adaptation&#34;&gt;Local adaptation&lt;/h3&gt;

&lt;p&gt;For local adaptation, we use an approach inspired by TCP flow control.
In TCP, the send window adapts according to a strategy called Additive Increase Multiplicative decrease.&lt;/p&gt;

&lt;p&gt;In this strategy, window size grows linearly with each successful transmission,
but is decreased by a certain factor whenever overflow occurs.
In the case of TCP, the overflow signal is the absence of an acknowledgment.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Anti-entropy - gossip information until it is made obsolete.&lt;/p&gt;

&lt;p&gt;Rumor-mongering - gossip information for some of high amount of time with high likelihood all participants received the information.&lt;/p&gt;

&lt;p&gt;AIMD - additive increase multiplicative decrease&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>SEDA: An Architecture for Well-Conditioned, Scalable Internet Services</title>
      <link>http://out13.com/paper/seda-an-architecture-for-well-conditioned-scalable-internet-services/</link>
      <pubDate>Thu, 24 Nov 2016 19:50:13 +0200</pubDate>
      
      <guid>http://out13.com/paper/seda-an-architecture-for-well-conditioned-scalable-internet-services/</guid>
      <description>

&lt;h2 id=&#34;seda-staged-event-driven-architecture&#34;&gt;SEDA - staged event driven architecture&lt;/h2&gt;

&lt;p&gt;A SEDA is intended to support massive concurrency demands and simplify the construction of well-conditioned services.
In SEDA, applications consist of a network of event-driven stages connected by explicit queues.
This architecture allows services to be well-conditioned to load, preventing resources from being overcommitted when demand exceeds service capacity.&lt;/p&gt;

&lt;p&gt;SEDA combines aspects of threads and event-based programming models to manage the concurrency, I/O, scheduling, and resource management needs of Internet services.&lt;/p&gt;

&lt;p&gt;Applications are constructed as a network of stages, each with an associated incoming event queue.
Each stage represents a robust building block that may be individually conditioned to load by thresholding or filtering its event queue.&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;Service is well-conditioned if it behaves like a simple pipeline, where the depth of the pipeline is determined by the path through the network and the processing stages within the service itself.
As the offered load increases, the delivered throughput increases proportionally until the pipeline is full and the throughput saturates; additional load should not degrade throughput.&lt;/p&gt;

&lt;h4 id=&#34;thread-based-concurrency&#34;&gt;Thread based concurrency&lt;/h4&gt;

&lt;p&gt;Operating system overlaps computation and I/O by transparently switching among threads.
Although relatively easy to program, the overheads associated with threading — including cache and TLB misses, scheduling overhead,
and lock contention — can lead to serious performance degradation when the number of threads is large.&lt;/p&gt;

&lt;h4 id=&#34;bounded-thread-pools&#34;&gt;Bounded thread pools&lt;/h4&gt;

&lt;p&gt;To avoid the overuse of threads, a number of systems adopt a coarse form of load conditioning that serves to bound the size of the thread
pool associated with a service. When the number of requests in the server exceeds some fixed limit, additional connections are not accepted.
This approach is used by Web servers such as Apache, IIS, and Netscape Enterprise Server.
By limiting the number of concurrent threads, the server can avoid throughput degradation,
and the overall performance is more robust than the unconstrained thread-per-task model.&lt;/p&gt;

&lt;h4 id=&#34;event-driven-concurrency&#34;&gt;Event-driven concurrency&lt;/h4&gt;

&lt;p&gt;Server consists of a small number of threads (typically one per CPU) that loop continuously, processing events of different types from a queue.
Events may be generated by the operating system or internally by the application,
and generally correspond to network and disk I/O readiness and completion notifications, timers, or other application-specific events.&lt;/p&gt;

&lt;p&gt;Certain I/O operations (in this case, filesystem access) do not have asynchronous interfaces, the main server
process handles these events by dispatching them to helper processes via IPC.
Helper processes issue (blocking) I/O requests and return an event to the main process upon completion.&lt;/p&gt;

&lt;p&gt;Important limitation of this model is that it assumes that event handling threads do not block,
and for this reason nonblocking I/O mechanisms must be employed.&lt;/p&gt;

&lt;h4 id=&#34;structured-event-queues&#34;&gt;Structured event queues&lt;/h4&gt;

&lt;p&gt;Common aspect of these designs is to structure an event-driven application using a
set of event queues to improve code modularity and simplify application design.&lt;/p&gt;

&lt;h4 id=&#34;staged-event-driven-architecture&#34;&gt;Staged event driven architecture&lt;/h4&gt;

&lt;p&gt;Support massive concurrency: To avoid performance degradation due to threads,
SEDA makes use of event-driven execution wherever possible.
This also requires that the system provide efficient and scalable I/O primitives.&lt;/p&gt;

&lt;p&gt;Simplify the construction of well-conditioned services: To reduce the complexity of building Internet services,
SEDA shields application programmers from many of the details of scheduling and resource management.
The design also supports modular construction of these applications, and provides support for debugging and performance profiling.&lt;/p&gt;

&lt;p&gt;Enable introspection: Applications should be able to analyze the request stream to adapt behavior to
changing load conditions. For example, the system should be able to
prioritize and filter requests to support degraded service under heavy load.&lt;/p&gt;

&lt;p&gt;Support self-tuning resource management: Rather than mandate a priori
knowledge of application resource requirements and client load
characteristics, the system should adjust its resource management parameters dynamically
to meet performance targets. For example, the number of threads allocated to
a stage can be determined automatically based on perceived concurrency demands,
rather than hard-coded by the programmer or administrator.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Building blocks&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The fundamental unit of processing within SEDA is the stage.
Stage is a self-contained application component consisting of an event handler, an incoming event queue, and a thread pool.&lt;/p&gt;

&lt;p&gt;The core logic for each stage is provided by the event handler, the input to which is a batch of multiple events.
Event handlers do not have direct control over queue operations or threads.&lt;/p&gt;

&lt;p&gt;Event queues in SEDA is that they may be finite: that is, an enqueue operation may fail
if the queue wishes to reject new entries, say, because it has reached a threshold.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Interaction of Buffer Size and TCP Protocol Handling and its Impact</title>
      <link>http://out13.com/paper/the-interaction-of-buffer-size-and-tcp-protocol-handling/</link>
      <pubDate>Thu, 17 Nov 2016 19:23:07 +0200</pubDate>
      
      <guid>http://out13.com/paper/the-interaction-of-buffer-size-and-tcp-protocol-handling/</guid>
      <description>

&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;

&lt;p&gt;Miercom was engaged by Cisco Systems to conduct independent testing of two vendors’ top of the line,
data-center switch-routers, including the Cisco Nexus 92160YC-X and Nexus 9272Q switches and the Arista 7280SE-72 switch.&lt;/p&gt;

&lt;h4 id=&#34;tcp-congestion-control-versus-system-buffer-management&#34;&gt;TCP Congestion Control versus System Buffer Management&lt;/h4&gt;

&lt;p&gt;TCP congestion control. The Transmission Control Protocol (TCP) is the Layer-4 control
protocol (atop IP at Layer 3) that ensures a block of data that’s sent is received intact.
Invented 35 years ago, TCP handles how blocks of data are broken up, sequenced, sent,
reconstructed and verified at the recipient’s end. The congestion-control mechanism
was added to TCP in 1988 to avoid network congestion meltdown. It makes sure data
transfers are accelerated or slowed down, exploiting the bandwidth that’s available,
depending on network conditions.&lt;/p&gt;

&lt;p&gt;System buffer management. Every network device that transports data has buffers,
usually statically allocated on a per-port basis or dynamically shared by multiple ports, so
that periodic data bursts can be accommodated without having to drop packets.
Network systems such as switch-routers are architected differently, however, and can
vary significantly in the size of their buffers and how they manage different traffic flows.&lt;/p&gt;

&lt;h4 id=&#34;deep-buffer-vs-intelligent-buffer&#34;&gt;Deep buffer vs Intelligent buffer&lt;/h4&gt;

&lt;p&gt;A common practice is to put in as much buffer as possible. However, since the
buffer space is a common resource shared by the inevitable mixture of elephant and mice flows,
how to use this shared resource can significantly impact applications’ performance.&lt;/p&gt;

&lt;p&gt;The deeper the buffer, the longer the queue and the longer the latency. So more buffer does
not necessarily guarantee better small-flow performance, it often leads to longer queuing delay
and hence longer flow completion time.&lt;/p&gt;

&lt;p&gt;Therefore, no one benefits from simple deep buffering: mice flows aren’t guaranteed buffer
resources and can suffer from long queuing delays and bandwidth hungry elephant flows suffer
because large buffers do not create more link bandwidth.&lt;/p&gt;

&lt;h4 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;Since mice flows are often mission critical (including, for example, control and alarm messages,
Hadoop application communications, etc.), giving these flows a priority buffer pathway enables
them to complete faster and their applications to perform better overall. The above test results
show that expediting mice flows and regulating the elephant flows early under the intelligent
buffer architecture on the Cisco Nexus 92160YC-X and 9272Q switches can bring orders of
magnitude better performance for mission critical flows without causing elephant flows to slow
down.&lt;/p&gt;

&lt;p&gt;Intelligent buffering allows the elephant and mice flows to share network buffers gracefully:
there is enough buffer space for the bursts of mice flows while the elephant flows are properly
regulated to fully utilize the link capacity. Simple, deep buffering can lead to collateral damage
in the form of longer queuing latency, and hence longer flow completion time for all flow types.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Elephant - big flows&lt;/p&gt;

&lt;p&gt;Mice - small flows&lt;/p&gt;

&lt;p&gt;FCT - flow completion time&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Replication Under Scalable Hashing: A Family of Algorithms for Scalable Decentralized Data Distribution</title>
      <link>http://out13.com/paper/replication-under-scalable-hashing--a-family-of-algorithms-for-scalable-decentralized-data-distribution/</link>
      <pubDate>Thu, 10 Nov 2016 22:27:23 +0200</pubDate>
      
      <guid>http://out13.com/paper/replication-under-scalable-hashing--a-family-of-algorithms-for-scalable-decentralized-data-distribution/</guid>
      <description>

&lt;h2 id=&#34;replication-under-scalable-hashing&#34;&gt;Replication Under Scalable Hashing&lt;/h2&gt;

&lt;p&gt;Typical algorithms for decentralized data distribution work best in a system that is fully built before it first used;
adding or removing components results in either extensive reorganization of data or load imbalance in the system.&lt;/p&gt;

&lt;p&gt;RUSH variants also support weighting, allowing disks of different vintages to be added to a system.&lt;/p&gt;

&lt;p&gt;RUSH variants is optimal or near-optimal reorganization. When new disks are added to the system,
or old disks are retired, RUSH variants minimize the number of objects that need to
be moved in order to bring the system back into balance.&lt;/p&gt;

&lt;p&gt;RUSH variants can perform reorganization online without locking the filesystem for a long time to relocate data.&lt;/p&gt;

&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;

&lt;p&gt;Subcluster in a system managed by RUSH t must have at least as many disks as an object has replicas.&lt;/p&gt;

&lt;p&gt;RUSH t is the best algorithms for distributing data over very large clusters of disks.&lt;/p&gt;

&lt;p&gt;RUSH r may be the best option for systems which need to remove disks one at a time from the system.&lt;/p&gt;

&lt;p&gt;RUSH p may be the best option for smaller systems where storage space is at a premium.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;RUSH t - RUSH tree&lt;/p&gt;

&lt;p&gt;RUSH r - RUSH support for removal&lt;/p&gt;

&lt;p&gt;PUSH p - RUSH placement using prime numbers&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Dynamo: Amazon’s Highly Available Key-value Store</title>
      <link>http://out13.com/paper/dynamo-amazon-highly-available-key-value-store/</link>
      <pubDate>Sun, 06 Nov 2016 12:32:44 +0200</pubDate>
      
      <guid>http://out13.com/paper/dynamo-amazon-highly-available-key-value-store/</guid>
      <description>

&lt;h2 id=&#34;dynamo&#34;&gt;Dynamo&lt;/h2&gt;

&lt;p&gt;Dynamo sacrifices Consistency for Availability under certain failure scenarios.
It makes extensive use of object versioning and application-assisted conflict resolution in a manner that provides a novel interface for developers to use.&lt;/p&gt;

&lt;p&gt;Gossip based distributed failure detection and membership protocol.&lt;/p&gt;

&lt;h3 id=&#34;query-model&#34;&gt;Query Model&lt;/h3&gt;

&lt;p&gt;Read &amp;amp; Write operations to data item that is uniquely identified by a key.
State is stored as blobs.
Targets application that store objects up to 1MB.&lt;/p&gt;

&lt;h3 id=&#34;acid&#34;&gt;ACID&lt;/h3&gt;

&lt;p&gt;Dynamo targets applications that operate with weaker consistency (the “C” in ACID) if this results in high availability.&lt;/p&gt;

&lt;p&gt;No isolation guarantees. Permits only single key updates.&lt;/p&gt;

&lt;h3 id=&#34;design&#34;&gt;Design&lt;/h3&gt;

&lt;p&gt;Incremental scalability: Dynamo should be able to scale out one storage host (henceforth, referred to as “node”) at a time,
with minimal impact on both operators of the system and the system itself.&lt;/p&gt;

&lt;p&gt;Symmetry: Every node in Dynamo should have the same set of responsibilities as its peers; there should be no distinguished node
or nodes that take special roles or extra set of responsibilities. In our experience, symmetry simplifies the process of system
provisioning and maintenance.&lt;/p&gt;

&lt;p&gt;Decentralization: An extension of symmetry, the design should favor decentralized peer-to-peer techniques over centralized
control. In the past, centralized control has resulted in outages and the goal is to avoid it as much as possible. This leads to a simpler,
more scalable, and more available system.&lt;/p&gt;

&lt;p&gt;Heterogeneity: The system needs to be able to exploit heterogeneity in the infrastructure it runs on. e.g. the work
distribution must be proportional to the capabilities of the individual servers. This is essential in adding new nodes with
higher capacity without having to upgrade all hosts at once.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bigtable: A Distributed Storage System for Structured Data</title>
      <link>http://out13.com/paper/bigtable-a-distributed-storage-system-for-structured-data/</link>
      <pubDate>Thu, 03 Nov 2016 19:54:45 +0200</pubDate>
      
      <guid>http://out13.com/paper/bigtable-a-distributed-storage-system-for-structured-data/</guid>
      <description>

&lt;h2 id=&#34;bigtable&#34;&gt;Bigtable&lt;/h2&gt;

&lt;p&gt;Bigtable is a distributed storage system for managing structured data that is
designed to scale to a very large size: petabytes of data across thousands of commodity servers.&lt;/p&gt;

&lt;p&gt;Bigtable does not support a full relational data model; instead, it provides
clients with a simple data model that supports dynamic control over data layout
and format, and allows clients to reason about the locality properties of the data
represented in the underlying storage.&lt;/p&gt;

&lt;h3 id=&#34;data-model&#34;&gt;Data model&lt;/h3&gt;

&lt;p&gt;A Bigtable is a sparse, distributed, persistent multidimensional sorted map.
The map is indexed by a row key, column key, and a timestamp; each value in the map
is an uninterpreted array of bytes.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;(row:string, column:string, time:int64) → string&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Bigtable maintains data in lexicographic order by row key. The row range for a table is dynamically partitioned.
Each row range is called a tablet, which is the unit of distribution and load balancing.&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;File format to store data: SSTable provides a persistent, ordered immutable map from keys to values, where both keys and values are arbitrary byte strings.&lt;/p&gt;

&lt;p&gt;First find the appropriate block by performing a binary search in the in-memory index, and then reading the appropriate block from disk.&lt;/p&gt;

&lt;p&gt;Bigtable relies on a highly-available and persistent distributed lock service called Chubby.
Chubby service consists of five active replicas, one of which is elected to be the master and actively serve requests.&lt;/p&gt;

&lt;p&gt;Chubby uses the Paxos algorithm to keep its replicas consistent in the face of failure&lt;/p&gt;

&lt;h3 id=&#34;client&#34;&gt;Client&lt;/h3&gt;

&lt;p&gt;The client library caches tablet locations.
If the client does not know the location of a tablet, or if it discovers that cached
location information is incorrect, then it recursively moves up the tablet location hierarchy.&lt;/p&gt;

&lt;h3 id=&#34;caching&#34;&gt;Caching&lt;/h3&gt;

&lt;p&gt;To improve read performance, tablet servers use two levels of caching.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Scan Cache is a higher-level cache that caches the key-value pairs returned by the SSTable interface to the tablet server code.&lt;/li&gt;
&lt;li&gt;Block Cache is a lower-level cache that caches SSTables blocks that were read from GFS.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bloom filter allows us to ask whether an SSTable might contain any data for a specified row/column pair.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;GFS - Google File System&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Generating configuration from Ansible variables</title>
      <link>http://out13.com/posts/generating-configuration-from-ansible-variables/</link>
      <pubDate>Thu, 03 Nov 2016 08:48:29 +0200</pubDate>
      
      <guid>http://out13.com/posts/generating-configuration-from-ansible-variables/</guid>
      <description>

&lt;p&gt;If you have ever tried to render Ansible multi hash or list variable you probably something alike.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;global:
  scrape_interval: &amp;quot;{{ prometheus_config_global_scrape_interval | to_nice_yaml }}&amp;quot;
  evaluation_interval: &amp;quot;{{ prometheus_config_global_evaluation_interval | to_nice_yaml }}&amp;quot;
  scrape_timeout: &amp;quot;{{ prometheus_config_global_scrape_timeout | to_nice_yaml }}&amp;quot;
  external_labels: &amp;quot;{{ prometheus_config_global_external_labels | to_nice_yaml }}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This generates invalid and ugly YAML.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;global:
  scrape_interval: &amp;quot;15s
...
&amp;quot;
  evaluation_interval: &amp;quot;30s
...
&amp;quot;
  scrape_timeout: &amp;quot;10s
...
&amp;quot;
  external_labels: &amp;quot;null
...
&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;tldr&#34;&gt;TLDR&lt;/h3&gt;

&lt;p&gt;If you are persistent thus configuration maniac you probably found a way either by destructuring hash or made extra redundant variables around complex one.
But there is a better way that I came up with.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;{{
{
&#39;global&#39;: {
  &#39;scrape_interval&#39;: prometheus_config_global_scrape_interval,
  &#39;evaluation_interval&#39;: prometheus_config_global_evaluation_interval,
  &#39;scrape_timeout&#39;: prometheus_config_global_scrape_timeout,
  &#39;external_labels&#39;: prometheus_config_global_external_labels }
} | to_nice_yaml
}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we are using Jinja 2 hash syntax and creating new hash with wanted keys inside of block later piping through &lt;code&gt;to_nice_yaml&lt;/code&gt; filter as well.
This generates pretty and valid YAML.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;global:
    evaluation_interval: 30s
    external_labels: null
    scrape_interval: 15s
    scrape_timeout: 10s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Same applies to more complex variable definitions like this hash configuration inside of array/list.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;prometheus_config_scrape_configs:
  - job_name: &#39;prometheus&#39;
    honor_labels: true
    scrape_interval: &#39;15s&#39;
    scrape_timeout: &#39;2s&#39;
    metrics_path: &#39;/metrics&#39;
    scheme: &#39;http&#39;
    static_configs:
      - targets:
          - &amp;quot;{{ prometheus_web__listen_address }}&amp;quot; # Prometheus itself
          - &amp;quot;{{ prometheus_alert_manager_web__listen_address }}&amp;quot; # Alert manager

  - job_name: &#39;consul-services&#39;
    consul_sd_configs:
      - server: &amp;quot;consul.service.consul:8500&amp;quot;
        services:
          - nodeexporter
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Variable used in template.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;{% if prometheus_config_scrape_configs is not none and prometheus_config_scrape_configs | length &amp;gt; 0 %}
{{ {&#39;scrape_configs&#39;: prometheus_config_scrape_configs} | to_nice_yaml }}
{% endif %}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;End result here.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;scrape_configs:
-   honor_labels: true
    job_name: prometheus
    metrics_path: /metrics
    scheme: http
    scrape_interval: 15s
    scrape_timeout: 2s
    static_configs:
    -   targets:
        - 192.168.250.11:9090
        - 192.168.250.11:9093
-   consul_sd_configs:
    -   server: consul.service.consul:8500
        services:
        - nodeexporter
    job_name: consul-services
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Examples are taken from by ansible-prometheus playbook: &lt;a href=&#34;https://github.com/ernestas-poskus/ansible-prometheus&#34;&gt;https://github.com/ernestas-poskus/ansible-prometheus&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Testing Ansible on multiple platforms</title>
      <link>http://out13.com/posts/ansible-testing-multiple-platforms/</link>
      <pubDate>Sun, 25 Sep 2016 19:12:59 +0300</pubDate>
      
      <guid>http://out13.com/posts/ansible-testing-multiple-platforms/</guid>
      <description>

&lt;p&gt;It is very uncommon to find tests on Ansible playbooks. However when they exist it means playbook was created with care.&lt;/p&gt;

&lt;p&gt;When playbook is created with Ansible command &lt;code&gt;ansible-galaxy init myplaybook&lt;/code&gt; it creates number of directories and files, includes basic Travis CI .travis.yml file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;script:
  # Basic role syntax check
  - ansible-playbook tests/test.yml -i tests/inventory --syntax-check
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Basic Travis script brings standard virtual environment operating system which is Ubuntu 12.04 LTS Server Edition 64 bit by default.
This limits testing playbooks on multiple operating systems.
In fact Travis CI virtual environment is limited to Debian operating systems and includes BETA Ubuntu 14.04 LTS Server Edition 64 bit container configurable via.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;dist: trusty
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;fortunately-travis-supports-docker&#34;&gt;Fortunately Travis supports docker.&lt;/h4&gt;

&lt;p&gt;I have created bare docker containers that include ansible, can be found here &lt;a href=&#34;https://github.com/ansible-docker-images&#34;&gt;https://github.com/ansible-docker-images&lt;/a&gt; and in docker hub &lt;a href=&#34;https://hub.docker.com/r/ernestasposkus&#34;&gt;https://hub.docker.com/r/ernestasposkus&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Currently available containers are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ubuntu1404&lt;/li&gt;
&lt;li&gt;ubuntu1604&lt;/li&gt;
&lt;li&gt;centos6&lt;/li&gt;
&lt;li&gt;centos7&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you need extra platforms let me now or join organization and contribute.&lt;/p&gt;

&lt;h2 id=&#34;drop-in-replacement-travis-yml-script-for-testing-ansible-on-multiple-platforms&#34;&gt;Drop in replacement .travis.yml script for testing ansible on multiple platforms&lt;/h2&gt;

&lt;p&gt;Template can be found here: &lt;a href=&#34;https://github.com/ansible-docker-images/template&#34;&gt;https://github.com/ansible-docker-images/template&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This includes four files:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;.travis.yml for starting CI itself&lt;/li&gt;
&lt;li&gt;tests/dependencies.yml for extra dependencies to be installed before testing playbook&lt;/li&gt;
&lt;li&gt;playbook.yml for syntax check, first install and idempotence tests&lt;/li&gt;
&lt;li&gt;test.yml where tests are defined basically Ansible tasks with exit status checking&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are looking for real examples:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/AnsibleShipyard/ansible-zookeeper&#34;&gt;https://github.com/AnsibleShipyard/ansible-zookeeper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/AnsibleShipyard/ansible-mesos&#34;&gt;https://github.com/AnsibleShipyard/ansible-mesos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ernestas-poskus/ansible-nsq&#34;&gt;https://github.com/ernestas-poskus/ansible-nsq&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ernestas-poskus/ansible-prometheus&#34;&gt;https://github.com/ernestas-poskus/ansible-prometheus&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sample .travis.yml script below.
Includes testing on 4 platforms, dependencies installation, Ansible syntax check, idempotence test and actual tests.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;sudo: required

env:
  global:
    # https://github.com/travis-ci/travis-ci/issues/6461#issuecomment-239577306
    DOCKER_VERSION: &amp;quot;1.9.1-0~trusty&amp;quot;
  matrix:
    - distro: ernestasposkus/centos7
      init: /usr/lib/systemd/systemd
      run_opts: &amp;quot;--privileged --volume=/sys/fs/cgroup:/sys/fs/cgroup:ro&amp;quot;
    - distro: ernestasposkus/centos6
      init: /sbin/init
      run_opts: &amp;quot;&amp;quot;
    - distro: ernestasposkus/ubuntu1604
      init: /lib/systemd/systemd
      run_opts: &amp;quot;--privileged --volume=/sys/fs/cgroup:/sys/fs/cgroup:ro&amp;quot;
    - distro: ernestasposkus/ubuntu1404
      init: /sbin/init
      run_opts: &amp;quot;&amp;quot;

services:
  - docker

before_install:
  # Downgrade to specific version of Docker engine.
  - sudo apt-get update
  - sudo apt-get remove docker-engine -yq
  - sudo apt-get install docker-engine=$DOCKER_VERSION -yq --no-install-suggests --no-install-recommends --force-yes -o Dpkg::Options::=&amp;quot;--force-confnew&amp;quot;

  # Pull container.
  - &#39;sudo docker pull ${distro}:latest&#39;

script:
  - container_id=$(mktemp)
    # Run container in detached state.
  - &#39;sudo docker run --detach --volume=&amp;quot;${PWD}&amp;quot;:/etc/ansible/roles/role_under_test:ro ${run_opts} ${distro}:latest &amp;quot;${init}&amp;quot; &amp;gt; &amp;quot;${container_id}&amp;quot;&#39;

  # Inspect docker container
  - &#39;sudo docker inspect $(cat ${container_id})&#39;

  # Print ansible version
  - &#39;sudo docker exec --tty &amp;quot;$(cat ${container_id})&amp;quot; env TERM=xterm ansible --version&#39;

  # Check Ansible host setup
  - &#39;sudo docker exec --tty &amp;quot;$(cat ${container_id})&amp;quot; env TERM=xterm ansible all -i &amp;quot;localhost,&amp;quot; -c local -m setup&#39;

  # Install dependencies
  # Uncomment to install dependencies
  # - &#39;sudo docker exec --tty &amp;quot;$(cat ${container_id})&amp;quot; env TERM=xterm ansible-galaxy install geerlingguy.java&#39;
  # - &#39;sudo docker exec --tty &amp;quot;$(cat ${container_id})&amp;quot; env TERM=xterm ansible-playbook /etc/ansible/roles/role_under_test/tests/dependencies.yml&#39;

  # Ansible syntax check.
  - &#39;sudo docker exec --tty &amp;quot;$(cat ${container_id})&amp;quot; env TERM=xterm ansible-playbook /etc/ansible/roles/role_under_test/tests/playbook.yml --syntax-check&#39;

  # Test role.
  - &#39;sudo docker exec --tty &amp;quot;$(cat ${container_id})&amp;quot; env TERM=xterm ansible-playbook /etc/ansible/roles/role_under_test/tests/playbook.yml&#39;

  # Test role idempotence.
  - idempotence=$(mktemp)
  - sudo docker exec &amp;quot;$(cat ${container_id})&amp;quot; ansible-playbook /etc/ansible/roles/role_under_test/tests/playbook.yml | tee -a ${idempotence}
  - &amp;gt;
    tail ${idempotence}
    | grep -q &#39;changed=0.*failed=0&#39;
    &amp;amp;&amp;amp; (echo &#39;Idempotence test: pass&#39; &amp;amp;&amp;amp; exit 0)
    || (echo &#39;Idempotence test: fail&#39; &amp;amp;&amp;amp; exit 1)

  # Test role.
  - &#39;sudo docker exec --tty &amp;quot;$(cat ${container_id})&amp;quot; env TERM=xterm ansible-playbook /etc/ansible/roles/role_under_test/tests/test.yml&#39;

  # View container logs
  - &#39;sudo docker logs &amp;quot;$(cat ${container_id})&amp;quot;&#39;

  # Clean up.
  - &#39;sudo docker stop &amp;quot;$(cat ${container_id})&amp;quot;&#39;

notifications:
  webhooks: https://galaxy.ansible.com/api/v1/notifications/
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>