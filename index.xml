<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ernestas Poškus.io</title>
    <link>http://out13.com/index.xml</link>
    <description>Recent content on Ernestas Poškus.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Nov 2016 19:50:13 +0200</lastBuildDate>
    <atom:link href="http://out13.com/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>SEDA: An Architecture for Well-Conditioned, Scalable Internet Services</title>
      <link>http://out13.com/paper/seda-an-architecture-for-well-conditioned-scalable-internet-services/</link>
      <pubDate>Thu, 24 Nov 2016 19:50:13 +0200</pubDate>
      
      <guid>http://out13.com/paper/seda-an-architecture-for-well-conditioned-scalable-internet-services/</guid>
      <description>

&lt;h2 id=&#34;seda-staged-event-driven-architecture&#34;&gt;SEDA - staged event driven architecture&lt;/h2&gt;

&lt;p&gt;A SEDA is intended to support massive concurrency demands and simplify the construction of well-conditioned services.
In SEDA, applications consist of a network of event-driven stages connected by explicit queues.
This architecture allows services to be well-conditioned to load, preventing resources from being overcommitted when demand exceeds service capacity.&lt;/p&gt;

&lt;p&gt;SEDA combines aspects of threads and event-based programming models to manage the concurrency, I/O, scheduling, and resource management needs of Internet services.&lt;/p&gt;

&lt;p&gt;Applications are constructed as a network of stages, each with an associated incoming event queue.
Each stage represents a robust building block that may be individually conditioned to load by thresholding or filtering its event queue.&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;Service is well-conditioned if it behaves like a simple pipeline, where the depth of the pipeline is determined by the path through the network and the processing stages within the service itself.
As the offered load increases, the delivered throughput increases proportionally until the pipeline is full and the throughput saturates; additional load should not degrade throughput.&lt;/p&gt;

&lt;h4 id=&#34;thread-based-concurrency&#34;&gt;Thread based concurrency&lt;/h4&gt;

&lt;p&gt;Operating system overlaps computation and I/O by transparently switching among threads.
Although relatively easy to program, the overheads associated with threading — including cache and TLB misses, scheduling overhead,
and lock contention — can lead to serious performance degradation when the number of threads is large.&lt;/p&gt;

&lt;h4 id=&#34;bounded-thread-pools&#34;&gt;Bounded thread pools&lt;/h4&gt;

&lt;p&gt;To avoid the overuse of threads, a number of systems adopt a coarse form of load conditioning that serves to bound the size of the thread
pool associated with a service. When the number of requests in the server exceeds some fixed limit, additional connections are not accepted.
This approach is used by Web servers such as Apache, IIS, and Netscape Enterprise Server.
By limiting the number of concurrent threads, the server can avoid throughput degradation,
and the overall performance is more robust than the unconstrained thread-per-task model.&lt;/p&gt;

&lt;h4 id=&#34;event-driven-concurrency&#34;&gt;Event-driven concurrency&lt;/h4&gt;

&lt;p&gt;Server consists of a small number of threads (typically one per CPU) that loop continuously, processing events of different types from a queue.
Events may be generated by the operating system or internally by the application,
and generally correspond to network and disk I/O readiness and completion notifications, timers, or other application-specific events.&lt;/p&gt;

&lt;p&gt;Certain I/O operations (in this case, filesystem access) do not have asynchronous interfaces, the main server
process handles these events by dispatching them to helper processes via IPC.
Helper processes issue (blocking) I/O requests and return an event to the main process upon completion.&lt;/p&gt;

&lt;p&gt;Important limitation of this model is that it assumes that event handling threads do not block,
and for this reason nonblocking I/O mechanisms must be employed.&lt;/p&gt;

&lt;h4 id=&#34;structured-event-queues&#34;&gt;Structured event queues&lt;/h4&gt;

&lt;p&gt;Common aspect of these designs is to structure an event-driven application using a
set of event queues to improve code modularity and simplify application design.&lt;/p&gt;

&lt;h4 id=&#34;staged-event-driven-architecture&#34;&gt;Staged event driven architecture&lt;/h4&gt;

&lt;p&gt;Support massive concurrency: To avoid performance degradation due to threads,
SEDA makes use of event-driven execution wherever possible.
This also requires that the system provide efficient and scalable I/O primitives.&lt;/p&gt;

&lt;p&gt;Simplify the construction of well-conditioned services: To reduce the complexity of building Internet services,
SEDA shields application programmers from many of the details of scheduling and resource management.
The design also supports modular construction of these applications, and provides support for debugging and performance profiling.&lt;/p&gt;

&lt;p&gt;Enable introspection: Applications should be able to analyze the request stream to adapt behavior to
changing load conditions. For example, the system should be able to
prioritize and filter requests to support degraded service under heavy load.&lt;/p&gt;

&lt;p&gt;Support self-tuning resource management: Rather than mandate a priori
knowledge of application resource requirements and client load
characteristics, the system should adjust its resource management parameters dynamically
to meet performance targets. For example, the number of threads allocated to
a stage can be determined automatically based on perceived concurrency demands,
rather than hard-coded by the programmer or administrator.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Building blocks&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The fundamental unit of processing within SEDA is the stage.
Stage is a self-contained application component consisting of an event handler, an incoming event queue, and a thread pool.&lt;/p&gt;

&lt;p&gt;The core logic for each stage is provided by the event handler, the input to which is a batch of multiple events.
Event handlers do not have direct control over queue operations or threads.&lt;/p&gt;

&lt;p&gt;Event queues in SEDA is that they may be finite: that is, an enqueue operation may fail
if the queue wishes to reject new entries, say, because it has reached a threshold.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Interaction of Buffer Size and TCP Protocol Handling and its Impact</title>
      <link>http://out13.com/paper/the-interaction-of-buffer-size-and-tcp-protocol-handling/</link>
      <pubDate>Thu, 17 Nov 2016 19:23:07 +0200</pubDate>
      
      <guid>http://out13.com/paper/the-interaction-of-buffer-size-and-tcp-protocol-handling/</guid>
      <description>

&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;

&lt;p&gt;Miercom was engaged by Cisco Systems to conduct independent testing of two vendors’ top of the line,
data-center switch-routers, including the Cisco Nexus 92160YC-X and Nexus 9272Q switches and the Arista 7280SE-72 switch.&lt;/p&gt;

&lt;h4 id=&#34;tcp-congestion-control-versus-system-buffer-management&#34;&gt;TCP Congestion Control versus System Buffer Management&lt;/h4&gt;

&lt;p&gt;TCP congestion control. The Transmission Control Protocol (TCP) is the Layer-4 control
protocol (atop IP at Layer 3) that ensures a block of data that’s sent is received intact.
Invented 35 years ago, TCP handles how blocks of data are broken up, sequenced, sent,
reconstructed and verified at the recipient’s end. The congestion-control mechanism
was added to TCP in 1988 to avoid network congestion meltdown. It makes sure data
transfers are accelerated or slowed down, exploiting the bandwidth that’s available,
depending on network conditions.&lt;/p&gt;

&lt;p&gt;System buffer management. Every network device that transports data has buffers,
usually statically allocated on a per-port basis or dynamically shared by multiple ports, so
that periodic data bursts can be accommodated without having to drop packets.
Network systems such as switch-routers are architected differently, however, and can
vary significantly in the size of their buffers and how they manage different traffic flows.&lt;/p&gt;

&lt;h4 id=&#34;deep-buffer-vs-intelligent-buffer&#34;&gt;Deep buffer vs Intelligent buffer&lt;/h4&gt;

&lt;p&gt;A common practice is to put in as much buffer as possible. However, since the
buffer space is a common resource shared by the inevitable mixture of elephant and mice flows,
how to use this shared resource can significantly impact applications’ performance.&lt;/p&gt;

&lt;p&gt;The deeper the buffer, the longer the queue and the longer the latency. So more buffer does
not necessarily guarantee better small-flow performance, it often leads to longer queuing delay
and hence longer flow completion time.&lt;/p&gt;

&lt;p&gt;Therefore, no one benefits from simple deep buffering: mice flows aren’t guaranteed buffer
resources and can suffer from long queuing delays and bandwidth hungry elephant flows suffer
because large buffers do not create more link bandwidth.&lt;/p&gt;

&lt;h4 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;Since mice flows are often mission critical (including, for example, control and alarm messages,
Hadoop application communications, etc.), giving these flows a priority buffer pathway enables
them to complete faster and their applications to perform better overall. The above test results
show that expediting mice flows and regulating the elephant flows early under the intelligent
buffer architecture on the Cisco Nexus 92160YC-X and 9272Q switches can bring orders of
magnitude better performance for mission critical flows without causing elephant flows to slow
down.&lt;/p&gt;

&lt;p&gt;Intelligent buffering allows the elephant and mice flows to share network buffers gracefully:
there is enough buffer space for the bursts of mice flows while the elephant flows are properly
regulated to fully utilize the link capacity. Simple, deep buffering can lead to collateral damage
in the form of longer queuing latency, and hence longer flow completion time for all flow types.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Elephant - big flows&lt;/p&gt;

&lt;p&gt;Mice - small flows&lt;/p&gt;

&lt;p&gt;FCT - flow completion time&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Replication Under Scalable Hashing: A Family of Algorithms for Scalable Decentralized Data Distribution</title>
      <link>http://out13.com/paper/replication-under-scalable-hashing--a-family-of-algorithms-for-scalable-decentralized-data-distribution/</link>
      <pubDate>Thu, 10 Nov 2016 22:27:23 +0200</pubDate>
      
      <guid>http://out13.com/paper/replication-under-scalable-hashing--a-family-of-algorithms-for-scalable-decentralized-data-distribution/</guid>
      <description>

&lt;h2 id=&#34;replication-under-scalable-hashing&#34;&gt;Replication Under Scalable Hashing&lt;/h2&gt;

&lt;p&gt;Typical algorithms for decentralized data distribution work best in a system that is fully built before it first used;
adding or removing components results in either extensive reorganization of data or load imbalance in the system.&lt;/p&gt;

&lt;p&gt;RUSH variants also support weighting, allowing disks of different vintages to be added to a system.&lt;/p&gt;

&lt;p&gt;RUSH variants is optimal or near-optimal reorganization. When new disks are added to the system,
or old disks are retired, RUSH variants minimize the number of objects that need to
be moved in order to bring the system back into balance.&lt;/p&gt;

&lt;p&gt;RUSH variants can perform reorganization online without locking the filesystem for a long time to relocate data.&lt;/p&gt;

&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;

&lt;p&gt;Subcluster in a system managed by RUSH t must have at least as many disks as an object has replicas.&lt;/p&gt;

&lt;p&gt;RUSH t is the best algorithms for distributing data over very large clusters of disks.&lt;/p&gt;

&lt;p&gt;RUSH r may be the best option for systems which need to remove disks one at a time from the system.&lt;/p&gt;

&lt;p&gt;RUSH p may be the best option for smaller systems where storage space is at a premium.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;RUSH t - RUSH tree&lt;/p&gt;

&lt;p&gt;RUSH r - RUSH support for removal&lt;/p&gt;

&lt;p&gt;PUSH p - RUSH placement using prime numbers&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Dynamo: Amazon’s Highly Available Key-value Store</title>
      <link>http://out13.com/paper/dynamo-amazon-highly-available-key-value-store/</link>
      <pubDate>Sun, 06 Nov 2016 12:32:44 +0200</pubDate>
      
      <guid>http://out13.com/paper/dynamo-amazon-highly-available-key-value-store/</guid>
      <description>

&lt;h2 id=&#34;dynamo&#34;&gt;Dynamo&lt;/h2&gt;

&lt;p&gt;Dynamo sacrifices Consistency for Availability under certain failure scenarios.
It makes extensive use of object versioning and application-assisted conflict resolution in a manner that provides a novel interface for developers to use.&lt;/p&gt;

&lt;p&gt;Gossip based distributed failure detection and membership protocol.&lt;/p&gt;

&lt;h3 id=&#34;query-model&#34;&gt;Query Model&lt;/h3&gt;

&lt;p&gt;Read &amp;amp; Write operations to data item that is uniquely identified by a key.
State is stored as blobs.
Targets application that store objects up to 1MB.&lt;/p&gt;

&lt;h3 id=&#34;acid&#34;&gt;ACID&lt;/h3&gt;

&lt;p&gt;Dynamo targets applications that operate with weaker consistency (the “C” in ACID) if this results in high availability.&lt;/p&gt;

&lt;p&gt;No isolation guarantees. Permits only single key updates.&lt;/p&gt;

&lt;h3 id=&#34;design&#34;&gt;Design&lt;/h3&gt;

&lt;p&gt;Incremental scalability: Dynamo should be able to scale out one storage host (henceforth, referred to as “node”) at a time,
with minimal impact on both operators of the system and the system itself.&lt;/p&gt;

&lt;p&gt;Symmetry: Every node in Dynamo should have the same set of responsibilities as its peers; there should be no distinguished node
or nodes that take special roles or extra set of responsibilities. In our experience, symmetry simplifies the process of system
provisioning and maintenance.&lt;/p&gt;

&lt;p&gt;Decentralization: An extension of symmetry, the design should favor decentralized peer-to-peer techniques over centralized
control. In the past, centralized control has resulted in outages and the goal is to avoid it as much as possible. This leads to a simpler,
more scalable, and more available system.&lt;/p&gt;

&lt;p&gt;Heterogeneity: The system needs to be able to exploit heterogeneity in the infrastructure it runs on. e.g. the work
distribution must be proportional to the capabilities of the individual servers. This is essential in adding new nodes with
higher capacity without having to upgrade all hosts at once.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bigtable: A Distributed Storage System for Structured Data</title>
      <link>http://out13.com/paper/bigtable-a-distributed-storage-system-for-structured-data/</link>
      <pubDate>Thu, 03 Nov 2016 19:54:45 +0200</pubDate>
      
      <guid>http://out13.com/paper/bigtable-a-distributed-storage-system-for-structured-data/</guid>
      <description>

&lt;h2 id=&#34;bigtable&#34;&gt;Bigtable&lt;/h2&gt;

&lt;p&gt;Bigtable is a distributed storage system for managing structured data that is
designed to scale to a very large size: petabytes of data across thousands of commodity servers.&lt;/p&gt;

&lt;p&gt;Bigtable does not support a full relational data model; instead, it provides
clients with a simple data model that supports dynamic control over data layout
and format, and allows clients to reason about the locality properties of the data
represented in the underlying storage.&lt;/p&gt;

&lt;h3 id=&#34;data-model&#34;&gt;Data model&lt;/h3&gt;

&lt;p&gt;A Bigtable is a sparse, distributed, persistent multidimensional sorted map.
The map is indexed by a row key, column key, and a timestamp; each value in the map
is an uninterpreted array of bytes.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;(row:string, column:string, time:int64) → string&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Bigtable maintains data in lexicographic order by row key. The row range for a table is dynamically partitioned.
Each row range is called a tablet, which is the unit of distribution and load balancing.&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;File format to store data: SSTable provides a persistent, ordered immutable map from keys to values, where both keys and values are arbitrary byte strings.&lt;/p&gt;

&lt;p&gt;First find the appropriate block by performing a binary search in the in-memory index, and then reading the appropriate block from disk.&lt;/p&gt;

&lt;p&gt;Bigtable relies on a highly-available and persistent distributed lock service called Chubby.
Chubby service consists of five active replicas, one of which is elected to be the master and actively serve requests.&lt;/p&gt;

&lt;p&gt;Chubby uses the Paxos algorithm to keep its replicas consistent in the face of failure&lt;/p&gt;

&lt;h3 id=&#34;client&#34;&gt;Client&lt;/h3&gt;

&lt;p&gt;The client library caches tablet locations.
If the client does not know the location of a tablet, or if it discovers that cached
location information is incorrect, then it recursively moves up the tablet location hierarchy.&lt;/p&gt;

&lt;h3 id=&#34;caching&#34;&gt;Caching&lt;/h3&gt;

&lt;p&gt;To improve read performance, tablet servers use two levels of caching.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Scan Cache is a higher-level cache that caches the key-value pairs returned by the SSTable interface to the tablet server code.&lt;/li&gt;
&lt;li&gt;Block Cache is a lower-level cache that caches SSTables blocks that were read from GFS.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bloom filter allows us to ask whether an SSTable might contain any data for a specified row/column pair.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;GFS - Google File System&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Generating configuration from Ansible variables</title>
      <link>http://out13.com/posts/generating-configuration-from-ansible-variables/</link>
      <pubDate>Thu, 03 Nov 2016 08:48:29 +0200</pubDate>
      
      <guid>http://out13.com/posts/generating-configuration-from-ansible-variables/</guid>
      <description>

&lt;p&gt;If you have ever tried to render Ansible multi hash or list variable you probably something alike.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;global:
  scrape_interval: &amp;quot;{{ prometheus_config_global_scrape_interval | to_nice_yaml }}&amp;quot;
  evaluation_interval: &amp;quot;{{ prometheus_config_global_evaluation_interval | to_nice_yaml }}&amp;quot;
  scrape_timeout: &amp;quot;{{ prometheus_config_global_scrape_timeout | to_nice_yaml }}&amp;quot;
  external_labels: &amp;quot;{{ prometheus_config_global_external_labels | to_nice_yaml }}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This generates invalid and ugly YAML.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;global:
  scrape_interval: &amp;quot;15s
...
&amp;quot;
  evaluation_interval: &amp;quot;30s
...
&amp;quot;
  scrape_timeout: &amp;quot;10s
...
&amp;quot;
  external_labels: &amp;quot;null
...
&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;tldr&#34;&gt;TLDR&lt;/h3&gt;

&lt;p&gt;If you are persistent thus configuration maniac you probably found a way either by destructuring hash or made extra redundant variables around complex one.
But there is a better way that I came up with.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;{{
{
&#39;global&#39;: {
  &#39;scrape_interval&#39;: prometheus_config_global_scrape_interval,
  &#39;evaluation_interval&#39;: prometheus_config_global_evaluation_interval,
  &#39;scrape_timeout&#39;: prometheus_config_global_scrape_timeout,
  &#39;external_labels&#39;: prometheus_config_global_external_labels }
} | to_nice_yaml
}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we are using Jinja 2 hash syntax and creating new hash with wanted keys inside of block later piping through &lt;code&gt;to_nice_yaml&lt;/code&gt; filter as well.
This generates pretty and valid YAML.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;global:
    evaluation_interval: 30s
    external_labels: null
    scrape_interval: 15s
    scrape_timeout: 10s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Same applies to more complex variable definitions like this hash configuration inside of array/list.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;prometheus_config_scrape_configs:
  - job_name: &#39;prometheus&#39;
    honor_labels: true
    scrape_interval: &#39;15s&#39;
    scrape_timeout: &#39;2s&#39;
    metrics_path: &#39;/metrics&#39;
    scheme: &#39;http&#39;
    static_configs:
      - targets:
          - &amp;quot;{{ prometheus_web__listen_address }}&amp;quot; # Prometheus itself
          - &amp;quot;{{ prometheus_alert_manager_web__listen_address }}&amp;quot; # Alert manager

  - job_name: &#39;consul-services&#39;
    consul_sd_configs:
      - server: &amp;quot;consul.service.consul:8500&amp;quot;
        services:
          - nodeexporter
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Variable used in template.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;{% if prometheus_config_scrape_configs is not none and prometheus_config_scrape_configs | length &amp;gt; 0 %}
{{ {&#39;scrape_configs&#39;: prometheus_config_scrape_configs} | to_nice_yaml }}
{% endif %}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;End result here.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;scrape_configs:
-   honor_labels: true
    job_name: prometheus
    metrics_path: /metrics
    scheme: http
    scrape_interval: 15s
    scrape_timeout: 2s
    static_configs:
    -   targets:
        - 192.168.250.11:9090
        - 192.168.250.11:9093
-   consul_sd_configs:
    -   server: consul.service.consul:8500
        services:
        - nodeexporter
    job_name: consul-services
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Examples are taken from by ansible-prometheus playbook: &lt;a href=&#34;https://github.com/ernestas-poskus/ansible-prometheus&#34;&gt;https://github.com/ernestas-poskus/ansible-prometheus&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Testing Ansible on multiple platforms</title>
      <link>http://out13.com/posts/ansible-testing-multiple-platforms/</link>
      <pubDate>Sun, 25 Sep 2016 19:12:59 +0300</pubDate>
      
      <guid>http://out13.com/posts/ansible-testing-multiple-platforms/</guid>
      <description>

&lt;p&gt;It is very uncommon to find tests on Ansible playbooks. However when they exist it means playbook was created with care.&lt;/p&gt;

&lt;p&gt;When playbook is created with Ansible command &lt;code&gt;ansible-galaxy init myplaybook&lt;/code&gt; it creates number of directories and files, includes basic Travis CI .travis.yml file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;script:
  # Basic role syntax check
  - ansible-playbook tests/test.yml -i tests/inventory --syntax-check
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Basic Travis script brings standard virtual environment operating system which is Ubuntu 12.04 LTS Server Edition 64 bit by default.
This limits testing playbooks on multiple operating systems.
In fact Travis CI virtual environment is limited to Debian operating systems and includes BETA Ubuntu 14.04 LTS Server Edition 64 bit container configurable via.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;dist: trusty
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;fortunately-travis-supports-docker&#34;&gt;Fortunately Travis supports docker.&lt;/h4&gt;

&lt;p&gt;I have created bare docker containers that include ansible, can be found here &lt;a href=&#34;https://github.com/ansible-docker-images&#34;&gt;https://github.com/ansible-docker-images&lt;/a&gt; and in docker hub &lt;a href=&#34;https://hub.docker.com/r/ernestasposkus&#34;&gt;https://hub.docker.com/r/ernestasposkus&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Currently available containers are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ubuntu1404&lt;/li&gt;
&lt;li&gt;ubuntu1604&lt;/li&gt;
&lt;li&gt;centos6&lt;/li&gt;
&lt;li&gt;centos7&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you need extra platforms let me now or join organization and contribute.&lt;/p&gt;

&lt;h2 id=&#34;drop-in-replacement-travis-yml-script-for-testing-ansible-on-multiple-platforms&#34;&gt;Drop in replacement .travis.yml script for testing ansible on multiple platforms&lt;/h2&gt;

&lt;p&gt;Template can be found here: &lt;a href=&#34;https://github.com/ansible-docker-images/template&#34;&gt;https://github.com/ansible-docker-images/template&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This includes four files:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;.travis.yml for starting CI itself&lt;/li&gt;
&lt;li&gt;tests/dependencies.yml for extra dependencies to be installed before testing playbook&lt;/li&gt;
&lt;li&gt;playbook.yml for syntax check, first install and idempotence tests&lt;/li&gt;
&lt;li&gt;test.yml where tests are defined basically Ansible tasks with exit status checking&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are looking for real examples:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/AnsibleShipyard/ansible-zookeeper&#34;&gt;https://github.com/AnsibleShipyard/ansible-zookeeper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/AnsibleShipyard/ansible-mesos&#34;&gt;https://github.com/AnsibleShipyard/ansible-mesos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ernestas-poskus/ansible-nsq&#34;&gt;https://github.com/ernestas-poskus/ansible-nsq&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ernestas-poskus/ansible-prometheus&#34;&gt;https://github.com/ernestas-poskus/ansible-prometheus&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sample .travis.yml script below.
Includes testing on 4 platforms, dependencies installation, Ansible syntax check, idempotence test and actual tests.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;sudo: required

env:
  global:
    # https://github.com/travis-ci/travis-ci/issues/6461#issuecomment-239577306
    DOCKER_VERSION: &amp;quot;1.9.1-0~trusty&amp;quot;
  matrix:
    - distro: ernestasposkus/centos7
      init: /usr/lib/systemd/systemd
      run_opts: &amp;quot;--privileged --volume=/sys/fs/cgroup:/sys/fs/cgroup:ro&amp;quot;
    - distro: ernestasposkus/centos6
      init: /sbin/init
      run_opts: &amp;quot;&amp;quot;
    - distro: ernestasposkus/ubuntu1604
      init: /lib/systemd/systemd
      run_opts: &amp;quot;--privileged --volume=/sys/fs/cgroup:/sys/fs/cgroup:ro&amp;quot;
    - distro: ernestasposkus/ubuntu1404
      init: /sbin/init
      run_opts: &amp;quot;&amp;quot;

services:
  - docker

before_install:
  # Downgrade to specific version of Docker engine.
  - sudo apt-get update
  - sudo apt-get remove docker-engine -yq
  - sudo apt-get install docker-engine=$DOCKER_VERSION -yq --no-install-suggests --no-install-recommends --force-yes -o Dpkg::Options::=&amp;quot;--force-confnew&amp;quot;

  # Pull container.
  - &#39;sudo docker pull ${distro}:latest&#39;

script:
  - container_id=$(mktemp)
    # Run container in detached state.
  - &#39;sudo docker run --detach --volume=&amp;quot;${PWD}&amp;quot;:/etc/ansible/roles/role_under_test:ro ${run_opts} ${distro}:latest &amp;quot;${init}&amp;quot; &amp;gt; &amp;quot;${container_id}&amp;quot;&#39;

  # Inspect docker container
  - &#39;sudo docker inspect $(cat ${container_id})&#39;

  # Print ansible version
  - &#39;sudo docker exec --tty &amp;quot;$(cat ${container_id})&amp;quot; env TERM=xterm ansible --version&#39;

  # Check Ansible host setup
  - &#39;sudo docker exec --tty &amp;quot;$(cat ${container_id})&amp;quot; env TERM=xterm ansible all -i &amp;quot;localhost,&amp;quot; -c local -m setup&#39;

  # Install dependencies
  # Uncomment to install dependencies
  # - &#39;sudo docker exec --tty &amp;quot;$(cat ${container_id})&amp;quot; env TERM=xterm ansible-galaxy install geerlingguy.java&#39;
  # - &#39;sudo docker exec --tty &amp;quot;$(cat ${container_id})&amp;quot; env TERM=xterm ansible-playbook /etc/ansible/roles/role_under_test/tests/dependencies.yml&#39;

  # Ansible syntax check.
  - &#39;sudo docker exec --tty &amp;quot;$(cat ${container_id})&amp;quot; env TERM=xterm ansible-playbook /etc/ansible/roles/role_under_test/tests/playbook.yml --syntax-check&#39;

  # Test role.
  - &#39;sudo docker exec --tty &amp;quot;$(cat ${container_id})&amp;quot; env TERM=xterm ansible-playbook /etc/ansible/roles/role_under_test/tests/playbook.yml&#39;

  # Test role idempotence.
  - idempotence=$(mktemp)
  - sudo docker exec &amp;quot;$(cat ${container_id})&amp;quot; ansible-playbook /etc/ansible/roles/role_under_test/tests/playbook.yml | tee -a ${idempotence}
  - &amp;gt;
    tail ${idempotence}
    | grep -q &#39;changed=0.*failed=0&#39;
    &amp;amp;&amp;amp; (echo &#39;Idempotence test: pass&#39; &amp;amp;&amp;amp; exit 0)
    || (echo &#39;Idempotence test: fail&#39; &amp;amp;&amp;amp; exit 1)

  # Test role.
  - &#39;sudo docker exec --tty &amp;quot;$(cat ${container_id})&amp;quot; env TERM=xterm ansible-playbook /etc/ansible/roles/role_under_test/tests/test.yml&#39;

  # View container logs
  - &#39;sudo docker logs &amp;quot;$(cat ${container_id})&amp;quot;&#39;

  # Clean up.
  - &#39;sudo docker stop &amp;quot;$(cat ${container_id})&amp;quot;&#39;

notifications:
  webhooks: https://galaxy.ansible.com/api/v1/notifications/
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Ownership is theft experiences building an embedded os in rust</title>
      <link>http://out13.com/paper/ownership-is-theft-experiences-building-an-embedded-os-in-rust/</link>
      <pubDate>Thu, 25 Aug 2016 20:39:03 +0300</pubDate>
      
      <guid>http://out13.com/paper/ownership-is-theft-experiences-building-an-embedded-os-in-rust/</guid>
      <description>

&lt;h2 id=&#34;embedded-os-in-rust&#34;&gt;Embedded OS in Rust&lt;/h2&gt;

&lt;p&gt;Embedded systems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;lack hardware protection mechanism&lt;/li&gt;
&lt;li&gt;less tolerant to crashes&lt;/li&gt;
&lt;li&gt;no easy way for debugging&lt;/li&gt;
&lt;li&gt;GC introduces non-deterministic delay&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;rust&#34;&gt;Rust&lt;/h3&gt;

&lt;p&gt;Rust, a new systems programming language, provides compile-time memory safety checks to help eliminate runtime bugs that manifest from improper memory management.&lt;/p&gt;

&lt;p&gt;Rust’s ownership model prevents otherwise safe resource sharing common in the embedded domain, conflicts with the reality of hardware resources, and hinders using closures for programming asynchronously.&lt;/p&gt;

&lt;p&gt;Rust achieves memory and type safety without garbage collection by using mechanism, derived from affine type and unique pointers, called ownership.&lt;/p&gt;

&lt;p&gt;Preserved type safety without relying on a runtime GC for memory management.&lt;/p&gt;

&lt;p&gt;Allows the programmer to explicitly separate code which is strictly bound to the type system from code which may subvert it.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Borrowing&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Value can only be mutably borrowed if there are no other borrows of the value.&lt;/li&gt;
&lt;li&gt;Borrows cannot outlive the value they borrow. This prevents dangling pointer bugs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;execution-context-extension-for-rust&#34;&gt;Execution context (extension for Rust)&lt;/h3&gt;

&lt;p&gt;Reflects the thread of a value&amp;rsquo;s owner in its type.&lt;/p&gt;

&lt;p&gt;Allows multiple borrows of a value from within same thread, but not across threads.&lt;/p&gt;

&lt;p&gt;The goal of execution context is to allow program mutably borrow values multiple times as long as those borrows are never shared between threads.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On the fly garbage collection</title>
      <link>http://out13.com/paper/on-the-fly-garbage-collection/</link>
      <pubDate>Thu, 25 Aug 2016 19:13:56 +0300</pubDate>
      
      <guid>http://out13.com/paper/on-the-fly-garbage-collection/</guid>
      <description>

&lt;p&gt;In our abstract form of the problem, we consider a
directed graph of varying structure but with a fixed
number of nodes, in which each node has at most two
outgoing edges. More precisely, each node may have a
left-hand outgoing edge and may have a right-hand
outgoing edge, but either of them or both may be missing.
In this graph a fixed set of nodes exists, called &amp;ldquo;the
roots.&amp;rdquo; A node is called &amp;ldquo;reachable&amp;rdquo; if it is reachable
from at least one root via a directed path along the edges.&lt;/p&gt;

&lt;p&gt;The subgraph consists of all reachable nodes and their interconnections is
called &amp;lsquo;the data structure&amp;rsquo;; nonreachable nodes that do not belong to the
data structure are called garbage.&lt;/p&gt;

&lt;p&gt;Data structure can modified:
 - Redirecting an outgoing edge of a reachable node towards an already reachable one.
 - Redirecting an outgoing edge of a reachable node towards a not yet reachable one without outgoing edges.
 - Adding&amp;ndash;where an outgoing edge was missing an edge pointing from a reachable node towards an already reachable one.
 - Adding&amp;ndash;where an outgoing edge was missing an edge pointing from a reachable node towards a not yet reachable one without outgoing edges.
 - Removing an outgoing edge of a reachable node&lt;/p&gt;

&lt;p&gt;Mutator: redirect an outgoing edge of reachable node towards an already reachable one.&lt;/p&gt;

&lt;p&gt;Collector:
 - marking phase: mark all reachable nodes
 - appending phase: append all unmarked nodes to the free list and remove the markings from all marked nodes&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Free list - collection of nodes that have been identified as garbage.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Queues Are Databases</title>
      <link>http://out13.com/paper/queues-are-databases/</link>
      <pubDate>Fri, 12 Aug 2016 16:57:55 +0300</pubDate>
      
      <guid>http://out13.com/paper/queues-are-databases/</guid>
      <description>

&lt;h2 id=&#34;queued-transaction-processing-over-pure-client-server-transaction-processing&#34;&gt;Queued transaction processing over pure client-server transaction processing.&lt;/h2&gt;

&lt;p&gt;Queued systems are build on top of direct systems.&lt;/p&gt;

&lt;p&gt;TP systems offer both queued and direct transaction processing. They offer both client-server and P2P direct processing.&lt;/p&gt;

&lt;p&gt;Queue manager is best built as a naive resource manager atop an object-relational database system.
That system must have good concurrency control, recovery, triggers, security, operations interfaces, and utilities.&lt;/p&gt;

&lt;p&gt;Queues pose difficult problems when implemented atop a database:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Performance: An enqueue transaction is an insert followed by a commit. This places
extreme performance demands on the concurrency control and recovery components
of a database &amp;ndash; it exposes hotspots and high-overhead code.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Concurrency control: The dequeue transaction typically involves deleting a record from
the queue, processing the request, enqueuing results in other queues, and then
committing. Serializable isolation requires that there can be at most one dequeue
executing at a time against each queue. This suggests that queues need lower, indeed specialized, isolation levels.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Read past: locks allow a program to skip over dirty (uncommitted records) to find the
first committed record. This is what a dequeue() operation wants.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Read through: locks allow a program to examine records that have not yet been
committed. This is useful in polling the status of a queued request that is currently
being processed.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Notify: allow a program to wait for a state change in a lock. This allows a
dequeue() operation to wait for one or more queues to become non-empty.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;MOM - message oriented middleware&lt;/p&gt;

&lt;p&gt;TP - transaction processing&lt;/p&gt;

&lt;p&gt;P2P - peer to peer&lt;/p&gt;

&lt;p&gt;ORB - object request broker&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>An Argument for Increasing TCP’s Initial Congestion Window</title>
      <link>http://out13.com/paper/an-argument-for-increasing-tcp-initial-congestion-window/</link>
      <pubDate>Thu, 04 Aug 2016 22:02:54 +0300</pubDate>
      
      <guid>http://out13.com/paper/an-argument-for-increasing-tcp-initial-congestion-window/</guid>
      <description>

&lt;h2 id=&#34;tcp-congestion-window&#34;&gt;TCP congestion window&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;TCP flows start with initial congestion window of 4 segments (4KB of data).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Window if critical for how quickly flows can finish.&lt;/p&gt;

&lt;p&gt;Increase in 15KB congestion window improves average HTTP latency by 10%, mostly benefits RTT and BDP.&lt;/p&gt;

&lt;p&gt;Slow start increases congestion window by the number of data segments acknowledged for each received ACK.&lt;/p&gt;

&lt;p&gt;TCP latency is dominated by the number of round-trip times in slow-start phase.&lt;/p&gt;

&lt;p&gt;Increasing init_cwnd enables transfers to finish in fewer RTT.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;BDP - bandwidth delay product.&lt;/p&gt;

&lt;p&gt;RTT - round trip delay time.&lt;/p&gt;

&lt;p&gt;Wep page average size - 384KB.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center</title>
      <link>http://out13.com/paper/mesos-platform-for-resource-sharing/</link>
      <pubDate>Thu, 28 Apr 2016 19:50:29 +0300</pubDate>
      
      <guid>http://out13.com/paper/mesos-platform-for-resource-sharing/</guid>
      <description>

&lt;h2 id=&#34;platform-for-resource-sharing&#34;&gt;Platform for resource sharing&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Sharing improves cluster utilization and avoids per-framework data repli-cation
Organizations will want to run multiple frameworks in the same cluster, picking the best one for each application.
Sharing a cluster between frameworks improves utilization and allows applications to share access to large datasets that may be too costly to replicate&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;Mesos decides how many resources to offer each framework, based on an organizational policy such as fair sharing, while frameworks decide which resources to accept and which tasks to run on them.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Design philosophy - define a minimal interface that enables efficient resource sharing across frameworks, and otherwise push control of task scheduling and execution to the frameworks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The master decides how many resources to offer to each framework according to a given organizational policy, such as fair sharing, or strict priority.&lt;/p&gt;

&lt;p&gt;A framework running on top of Mesos consists of two components: a scheduler that registers with the master to be offered resources, and an executor process that is launched on slave nodes to run the framework’s tasks.&lt;/p&gt;

&lt;p&gt;Master determines how many resources are offered to each framework, the frameworks’ schedulers select which of the offered resources to use.&lt;/p&gt;

&lt;p&gt;When a frameworks accepts offered resources, it passes to Mesos a description of the tasks it wants to run on them.&lt;/p&gt;

&lt;p&gt;Frameworks achieve data locality by rejecting offers.&lt;/p&gt;

&lt;p&gt;Mesos can reallocate resources if cluster becomes filled with long tasks by revoking (killing) tasks with grace period.&lt;/p&gt;

&lt;p&gt;Isolation through existing OS isolation mechanisms usually system containers. These technologies can limit the CPU, memory, network bandwidth and I/O usage of a process tree.&lt;/p&gt;

&lt;p&gt;Mesos lets them short-circuit the rejection process and avoid communication by providing filters to the master. We support two types of filters: “only offer nodes from list L” and “only offer nodes with at least R resources free”.&lt;/p&gt;

&lt;p&gt;Two types of resources: mandatory and preferred&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A resource is mandatory if a framework must acquire it in order to run.&lt;/li&gt;
&lt;li&gt;Preferred if a framework performs “better” using it, but can also run using another equivalent resource.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Two-level scheduling mechanism called resource offers&lt;/p&gt;

&lt;p&gt;Delegating control over scheduling to the framework&lt;/p&gt;

&lt;p&gt;Resource offer - encapsulates a bundle of resources that a framework can allocate on a cluster node to run tasks&lt;/p&gt;

&lt;p&gt;Framework ramp-up time - time it takes a new framework to achieve its allocation&lt;/p&gt;

&lt;p&gt;Job completion time - time it takes a job to complete, assuming one job per framework;&lt;/p&gt;

&lt;p&gt;System utilization - total cluster utilization.&lt;/p&gt;

&lt;p&gt;Scale up - frameworks can elastically increase their allocation to take advantage of free resources.&lt;/p&gt;

&lt;p&gt;Scale down - frameworks can relinquish resources without significantly impacting their performance.&lt;/p&gt;

&lt;p&gt;Minimum allocation - frameworks require a certain minimum number of slots before they can start using their slots.&lt;/p&gt;

&lt;p&gt;Task distribution - distribution of the task durations. We consider both homogeneous and heterogeneous distributions.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Tiny LFU highly efficient cache admission policy</title>
      <link>http://out13.com/paper/tiny-lfu-highly-efficient-cache-admission-policy/</link>
      <pubDate>Fri, 22 Apr 2016 21:26:15 +0300</pubDate>
      
      <guid>http://out13.com/paper/tiny-lfu-highly-efficient-cache-admission-policy/</guid>
      <description>

&lt;h2 id=&#34;frequency-based-cache-admission-policy&#34;&gt;Frequency based cache admission policy&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Approximate LFU structure called TinyLFU, which maintains an approximate representation of the access frequency of a large sample of recently accessed items.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;TinyLFU is very compact and light-weight as it builds upon Bloom filter theory.&lt;/p&gt;

&lt;h3 id=&#34;usage&#34;&gt;Usage&lt;/h3&gt;

&lt;p&gt;The intuitive reason why caching works is that data accesses in many
domains of computer science exhibit a considerable degree of “locality”.&lt;/p&gt;

&lt;p&gt;When a data item is accessed, if it already appears in the cache, we say that there is a cache hit; otherwise, it is a cache miss. The ratio between the number of cache hits and the total number of data accesses is known as the cache hit-ratio.&lt;/p&gt;

&lt;p&gt;Admission policy - caching architecture in which an accessed item is only inserted into the cache if an admission policy decides that the cache hit ratio is likely to benefit from replacing it with the cache victim (as chosen by the cache’s replacement policy).&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;The cache eviction policy picks a cache victim, while TinyLFU decides if replacing the cache victim with the new item is expected to increase the hit-ratio.
To do so, TinyLFU maintains statistics of items frequency over a sizable recent history. Storing these statistics is considered prohibitively expensive for practical implementation and therefore TinyLFU approximates them in a highly efficient manner. To keep the history fresh an aging process is performed periodically or incrementally to halve all of the counters.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Time locality - access pattern, and consequently the corresponding probability distribution, change over time&lt;/p&gt;

&lt;p&gt;WLFU - Window Least Frequently Used, access frequency for a window, needs to keep track order of requests. Samples of the request stream (called window).&lt;/p&gt;

&lt;p&gt;PLFU - Perfect LFU, popularity based has metadata with counters&lt;/p&gt;

&lt;p&gt;In-memory LFU, outperformed by WLFU at the cost of larger meta-data&lt;/p&gt;

&lt;p&gt;SLRU - Segmented Least Recently Used, policy captures recent popularity by distinguishing between tem-porally popular items that are accessed at least twice in a short window vs. items accessed only once during that period&lt;/p&gt;

&lt;p&gt;LRU-K - combination of LRU &amp;amp; LFU the last K occurrences of each element are remembered. Using this data, LRU-K statistically estimates the momentary frequency of items in order to keep the most frequent pages in memory.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Container based operating system virtualization</title>
      <link>http://out13.com/paper/container-based-operating-system-virtualization/</link>
      <pubDate>Tue, 19 Apr 2016 19:30:48 +0300</pubDate>
      
      <guid>http://out13.com/paper/container-based-operating-system-virtualization/</guid>
      <description>

&lt;h2 id=&#34;alternative-to-hypervisors&#34;&gt;Alternative to hypervisors.&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Workload requirements for a given system will direct users to the point in the design space that
requires the least trade-off.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;sharing-over-isolation&#34;&gt;Sharing over isolation?&lt;/h3&gt;

&lt;p&gt;Hypervisors often deployed to let a single machine host multiple, unrelated
applications, which may run on behalf of independent organizations, as is common when a data center
consolidates multiple physical servers. Hypervisors favor full isolation over sharing.
However, when each virtual machine is running the same kernel and similar operating system
distributions, the degree of isolation offered by hypervisors comes at the cost of efficiency
relative to running all applications on a single kernel.&lt;/p&gt;

&lt;h3 id=&#34;usage&#34;&gt;Usage&lt;/h3&gt;

&lt;p&gt;Software configuration problems incompatibilities between specific OS distributions.&lt;/p&gt;

&lt;p&gt;Resource isolation corresponds to the ability to account for and enforce the resource consumption of one VM such that guarantees and fair shares are preserved for other VM&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;Many hybrid approaches are also possible: for instance, a system may enforce fair sharing of resources between classes of VMs, which lets one overbook available resources while preventing starvation in overload scenarios.&lt;/p&gt;

&lt;p&gt;The key point is that both hypervisors and COS&amp;rsquo;s incorporate sophisticated resource schedulers to avoid or minimize crosstalk.&lt;/p&gt;

&lt;h3 id=&#34;security-isolation&#34;&gt;Security isolation&lt;/h3&gt;

&lt;p&gt;Configuration independence - cannot conflict with other VM&amp;rsquo;s
Safety - global namespace shared&lt;/p&gt;

&lt;h3 id=&#34;fair-share-and-reservations&#34;&gt;Fair share and Reservations&lt;/h3&gt;

&lt;p&gt;Vserver implements CPU isolation by overlaying a token TBF on top of standard O(1) Linux CPU scheduler.&lt;/p&gt;

&lt;p&gt;For memory storage one can specify the following limits:
 * a) the maximum resident set size (RSS)
 * b) number of anonymous memory pages have (ANON)
 * c) number of pages that may be pinned into memory using mlock() and mlockall() that processes may have within a VM (MEMLOCK).&lt;/p&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Xen is able to support multiple kernels while by design VServer cannot.
Xen also has greater support for virtualizing the network stack and allows for the possibility of VM migration, a feature that is possible for a COS design, but not yet available in VServer. VServer, in turn, maintains a small kernel footprint and performs equally with native Linux kernels in most cases.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Undesired interactions between VMs are sometimes called cross-talk.&lt;/p&gt;

&lt;p&gt;COS - Container based Operating System&lt;/p&gt;

&lt;p&gt;TBF - token bucker filter&lt;/p&gt;

&lt;p&gt;HTB - Hierarchical Token Bucket&lt;/p&gt;

&lt;p&gt;RSS - maximum resident set size&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Golang dynamic struct decoration</title>
      <link>http://out13.com/posts/golang-dynamic-struct-decoration/</link>
      <pubDate>Thu, 28 Jan 2016 20:42:45 +0200</pubDate>
      
      <guid>http://out13.com/posts/golang-dynamic-struct-decoration/</guid>
      <description>

&lt;h2 id=&#34;dynamic-struct-decoration-using-type-assertion&#34;&gt;Dynamic struct decoration using type assertion.&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Use cases: API / templates.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Having simple map of:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;  input :=  map[string]interface{}{
    &amp;quot;Key1&amp;quot;: []string{&amp;quot;some&amp;quot;, &amp;quot;key&amp;quot;},
    &amp;quot;key3&amp;quot;: nil,
    &amp;quot;val&amp;quot;: 2,
    &amp;quot;val2&amp;quot;: &amp;quot;str&amp;quot;,
    &amp;quot;val3&amp;quot;: 4,
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One can decorate it using type assertion by iterating over it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;  for key, value := range input {
    slice, ok := value.([]string)
    if ok {
      input[&amp;quot;Count&amp;quot;+key] = len(slice)
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This becomes very useful when serializing struct into json.
To serialize struct use &lt;code&gt;github.com/fatih/structs&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;  input := structs.Map(...)

  for key, value := range input {
    slice, ok := value.([]string)
    if ok {
      input[&amp;quot;Count&amp;quot;+key] = len(slice)
    }
  }
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>