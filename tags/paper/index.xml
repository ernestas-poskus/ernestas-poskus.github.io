<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper on Ernestas Poškus.io</title>
    <link>http://out13.com/tags/paper/index.xml</link>
    <description>Recent content in Paper on Ernestas Poškus.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://out13.com/tags/paper/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Large-scale cluster management at Google with Borg</title>
      <link>http://out13.com/paper/large-scale-cluster-management-at-google-with-borg/</link>
      <pubDate>Thu, 09 Feb 2017 20:27:52 +0200</pubDate>
      
      <guid>http://out13.com/paper/large-scale-cluster-management-at-google-with-borg/</guid>
      <description>

&lt;h2 id=&#34;borg&#34;&gt;Borg&lt;/h2&gt;

&lt;p&gt;Cluster manager that runs hundreds of thousands of jobs, from many thousands of
different applications, across a number of clusters each with up to tens of thousands of machines.&lt;/p&gt;

&lt;p&gt;3 main benefits:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;hides the details of resource management and failure handling so its users can
focus on application development instead&lt;/li&gt;
&lt;li&gt;operates with very high reliability and availability, and supports applications that do the same&lt;/li&gt;
&lt;li&gt;lets us run workloads across tens of thousands of machines effectively&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A key design feature in Borg is that already-running tasks
continue to run even if the Borgmaster or a task’s Borglet
goes down. But keeping the master up is still important
because when it is down new jobs cannot be submitted
or existing ones updated, and tasks from failed machines
cannot be rescheduled.&lt;/p&gt;

&lt;p&gt;Each job runs in one Borg cell, a set of machines that are managed as a unit.&lt;/p&gt;

&lt;p&gt;The machines in a cell belong to a single cluster. A cluster lives inside a single datacenter
building, and a collection of buildings makes up a site.&lt;/p&gt;

&lt;p&gt;Median cell size is about 10 k machines after excluding test cells; some are
much larger.
The machines in a cell are heterogeneous in many dimensions: sizes (CPU, RAM,
disk, network), processor type, performance, and capabilities such as an
external IP address or flash storage.
Borg isolates users from most of these differences by determining where in a
cell to run tasks, allocating their resources, installing their programs and
other dependencies, monitoring their health, and restarting them if they fail.&lt;/p&gt;

&lt;h3 id=&#34;jobs&#34;&gt;Jobs&lt;/h3&gt;

&lt;p&gt;A Borg alloc (short for allocation) is a reserved set of resources on a machine
in which one or more tasks can be run; the resources remain assigned whether or
not they are used.&lt;/p&gt;

&lt;p&gt;Quota is used to decide which jobs to admit for scheduling.
Quota is expressed as a vector of resource quantities (CPU, RAM, disk, etc.)
at a given priority, for a period of time (typically months).&lt;/p&gt;

&lt;p&gt;Every job has a priority, a small positive integer. A high priority task
can obtain resources at the expense of a lower priority one,
even if that involves preempting (killing) the latter.&lt;/p&gt;

&lt;h3 id=&#34;naming-and-monitoring&#34;&gt;Naming and monitoring&lt;/h3&gt;

&lt;p&gt;BNS (DNS) for Borg jobs for each task that includes the cell name, job name, and task number.
Borg writes the task’s hostname and port into a consistent,
highly-available file in Chubby with this name, which
is used by our RPC system to find the task endpoint.&lt;/p&gt;

&lt;p&gt;Borg also writes job size and task health information into
Chubby whenever it changes, so load balancers can see
where to route requests to.&lt;/p&gt;

&lt;p&gt;Borg monitors the health-check URL and restarts
tasks that do not respond promptly or return an HTTP error code.&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;A Borg cell consists of a set of machines, a logically centralized
controller called the Borgmaster, and an agent process
called the Borglet that runs on each machine in a cell.&lt;/p&gt;

&lt;p&gt;Borgmaster process handles client RPCs that either
mutate state (e.g., create job) or provide read-only access
to data (e.g., lookup job).&lt;/p&gt;

&lt;h2 id=&#34;scheduling&#34;&gt;Scheduling&lt;/h2&gt;

&lt;p&gt;The scheduling algorithm has two parts: feasibility checking, to find
machines on which the task could run, and scoring, which picks
one of the feasible machines.&lt;/p&gt;

&lt;p&gt;To reduce task startup time, the scheduler prefers to assign
tasks to machines that already have the necessary packages.&lt;/p&gt;

&lt;p&gt;Borg distributes packages to machines in parallel using tree-
and torrent-like protocols.&lt;/p&gt;

&lt;h2 id=&#34;borglet&#34;&gt;Borglet&lt;/h2&gt;

&lt;p&gt;Borglet is a local Borg agent that is present on every
machine in a cell. It starts and stops tasks; restarts them if
they fail; manages local resources by manipulating OS kernel settings;
rolls over debug logs; and reports the state of the
machine to the Borgmaster and other monitoring systems.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;BNS - Borg name system&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Aerospike: Architecture of a Real-Time Operational DBMS</title>
      <link>http://out13.com/paper/aerospike-architecture-of-a-real-time-operational-dbms/</link>
      <pubDate>Sun, 29 Jan 2017 13:47:18 +0200</pubDate>
      
      <guid>http://out13.com/paper/aerospike-architecture-of-a-real-time-operational-dbms/</guid>
      <description>

&lt;h2 id=&#34;aerospike-architecture&#34;&gt;Aerospike architecture&lt;/h2&gt;

&lt;p&gt;Modeled on the classic shared-nothing database architecture&lt;/p&gt;

&lt;p&gt;Objectives of the cluster management subsystem:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Arrive at a single consistent view of current cluster members across all nodes in the cluster.&lt;/li&gt;
&lt;li&gt;Automatically detect new node arrival/departure and seamless cluster reconfiguration.&lt;/li&gt;
&lt;li&gt;Detect network faults and be resilient to such network flakiness.&lt;/li&gt;
&lt;li&gt;Minimize time to detect and adapt to cluster membership changes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;discovery&#34;&gt;Discovery&lt;/h3&gt;

&lt;p&gt;Node arrival or departure is detected via heartbeat messages
exchanged periodically between nodes.&lt;/p&gt;

&lt;h3 id=&#34;surrogate-heartbeats&#34;&gt;Surrogate heartbeats&lt;/h3&gt;

&lt;p&gt;In addition to regular heartbeat messages, nodes use other messages that are regularly exchanged
between nodes as an alternative secondary heartbeat mechanism.&lt;/p&gt;

&lt;h3 id=&#34;node-health-score&#34;&gt;Node Health Score&lt;/h3&gt;

&lt;p&gt;Every node in the cluster evaluates the health score of each of its
neighboring nodes by computing the average message loss, which
is an estimate of how many incoming messages from that node are lost.&lt;/p&gt;

&lt;h3 id=&#34;data-distribution&#34;&gt;Data Distribution&lt;/h3&gt;

&lt;p&gt;A record’s primary key is hashed into a 160-byte digest using the RipeMD160 algorithm.&lt;/p&gt;

&lt;p&gt;Colocated indexes and data to avoid any cross-node traffic when running read operations or queries.&lt;/p&gt;

&lt;p&gt;A partition assignment algorithm generates a replication list for every
partition. The replication list is a permutation of the cluster succession list.&lt;/p&gt;

&lt;p&gt;Reads can also be uniformly spread across all the
replicas via a runtime configuration setting.&lt;/p&gt;

&lt;h3 id=&#34;master-partition-without-data&#34;&gt;Master Partition Without Data&lt;/h3&gt;

&lt;p&gt;An empty node newly added to a running cluster will be master
for a proportional fraction of the partitions and have no data for
those partitions.&lt;/p&gt;

&lt;h3 id=&#34;migration-ordering&#34;&gt;Migration Ordering&lt;/h3&gt;

&lt;h4 id=&#34;smallest-partition-first&#34;&gt;Smallest Partition First&lt;/h4&gt;

&lt;p&gt;Migration is coordinated in such a manner as to let nodes with the
fewest records in their partition versions start migration first. This
strategy quickly reduces the number of different copies of a
specific partition, and does this faster than any other strategy.&lt;/p&gt;

&lt;h4 id=&#34;hottest-partition-first&#34;&gt;Hottest Partition First&lt;/h4&gt;

&lt;p&gt;At times, client accesses are skewed to a very small number of
keys from the key space. Therefore the latency on these accesses
could be improved quickly by migrating these hot partitions
before other partitions.&lt;/p&gt;

&lt;h3 id=&#34;defragmentation&#34;&gt;Defragmentation&lt;/h3&gt;

&lt;p&gt;Aerospike uses a log-structured file system with a copy-on-write
mechanism. Hence, it needs to reclaim space by continuously
running a background defragmentation process. Each device
stores a MAP of block and information relating to the fill-factor of
each block. The fill-factor of the block is the block fraction
utilized by valid records. At boot time, this information is loaded
and kept up-to-date on every write. When the fill-factor of a block
falls below a certain threshold, the block becomes a candidate for
defragmentation and is then queued up for the defragmentation
process.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Approximating Data with the Count-Min Data Structure</title>
      <link>http://out13.com/paper/approximating-data-with-the-count-min-data-structure/</link>
      <pubDate>Thu, 29 Dec 2016 20:25:26 +0200</pubDate>
      
      <guid>http://out13.com/paper/approximating-data-with-the-count-min-data-structure/</guid>
      <description>

&lt;h2 id=&#34;count-min-data-structure&#34;&gt;Count-Min Data Structure&lt;/h2&gt;

&lt;p&gt;Algorithmic problems such as tracking the contents of a set arise frequently in the course of building
systems. Given the variety of possible solutions, the choice of appropriate data structures for
such tasks is at the heart of building efficient and effective software.&lt;/p&gt;

&lt;p&gt;The Count-Min sketch provides a different kind of solution to count tracking.
It allocates a fixed amount of space to store count information, which does not vary over time even
as more and more counts are updated.&lt;/p&gt;

&lt;h3 id=&#34;implementation&#34;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;With all data structures, it is important to understand the data organization
and algorithms for updating the structure, to make clear the relative merits of different choices of
structure for a given task. The Count-Min Sketch data structure primarily consists of a fixed array
of counters, of width w and depth d. The counters are initialized to all zeros. Each row of counters
is associated with a different hash function.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TAO: Facebook’s Distributed Data Store for the Social Graph</title>
      <link>http://out13.com/paper/tao-facebooks-distributed-data-store-for-the-social-graph/</link>
      <pubDate>Thu, 15 Dec 2016 19:36:32 +0200</pubDate>
      
      <guid>http://out13.com/paper/tao-facebooks-distributed-data-store-for-the-social-graph/</guid>
      <description>

&lt;h2 id=&#34;distributed-data-store-for-social-graph&#34;&gt;Distributed data store for social graph&lt;/h2&gt;

&lt;p&gt;TAO is geographically distributed data store that provides efficient and timely
access to the social graph using a fixed set of queries.
Read optimized, persisted in MySQL.&lt;/p&gt;

&lt;p&gt;Inefficient edge lists: A key-value cache is not a good
semantic fit for lists of edges; queries must always fetch
the entire edge list and changes to a single edge require
the entire list to be reloaded.&lt;/p&gt;

&lt;p&gt;Distributed control logic: In a lookaside cache architecture
the control logic is run on clients that don’t communicate
with each other. This increases the number of
failure modes, and makes it difficult to avoid thundering herds.&lt;/p&gt;

&lt;p&gt;Expensive read-after-write consistency: Facebook
uses asynchronous master/slave replication for MySQL,
which poses a problem for caches in data centers using a
replica. Writes are forwarded to the master, but some
time will elapse before they are reflected in the local
replica. By restricting the data model
to objects and associations we can update the replica’s
cache at write time, then use graph semantics to interpret
cache maintenance messages from concurrent updates.&lt;/p&gt;

&lt;h3 id=&#34;data-model-and-api&#34;&gt;Data model and API&lt;/h3&gt;

&lt;p&gt;Facebook focuses on people, actions, and relationships.
We model these entities and connections as nodes and
edges in a graph. This representation is very flexible;
it directly models real-life objects, and can also be used
to store an application’s internal implementation-specific
data.&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;TAO needs to handle a far larger volume of data than can be stored on a
single MySQL server, therefore data is divided into logical shards.&lt;/p&gt;

&lt;h3 id=&#34;mysql-mapping&#34;&gt;MySQL mapping&lt;/h3&gt;

&lt;p&gt;Each shard is assigned to a logical MySQL database
that has a table for objects and a table
for associations. All of the fields of an object are serialized into a
single ‘data‘ column. This approach allows
us to store objects of different types within the same table,
Objects that benefit from separate data management
polices are stored in separate custom tables.
Associations are stored similarly to objects, but to support
range queries, their tables have an additional index
based on id1, atype, and time. To avoid potentially expensive
SELECT COUNT queries, association counts
are stored in a separate table.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Efficient Reconciliation and Flow Control for Anti-Entropy Protocols</title>
      <link>http://out13.com/paper/efficient-reconciliation-and-flow-control-for-anti-entropy-protocols/</link>
      <pubDate>Thu, 01 Dec 2016 16:05:39 +0200</pubDate>
      
      <guid>http://out13.com/paper/efficient-reconciliation-and-flow-control-for-anti-entropy-protocols/</guid>
      <description>

&lt;h2 id=&#34;flow-gossip&#34;&gt;Flow Gossip&lt;/h2&gt;

&lt;p&gt;Anti-entropy, or gossip, is an attractive way of replicating state that does not have strong consistency requirements.
With few limitations, updates spread in expected time that grows logarithmic in the number of participating hosts, even in the face of host failures and message loss.
The behavior of update propagation is easily modeled with well-known epidemic analysis techniques.&lt;/p&gt;

&lt;h3 id=&#34;gossip-basics&#34;&gt;Gossip basics&lt;/h3&gt;

&lt;p&gt;There are two classes of gossip: anti-entropy and rumor mongering protocols.
Anti-entropy protocols gossip information until it is made obsolete by newer information,
and are useful for reliably sharing information among a group of participants.
Rumor-mongering has participants gossip information for some amount of time chosen sufficiently
high so that with high likelihood all participants receive the information.&lt;/p&gt;

&lt;p&gt;3 Gossip styles:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;push: push everything and apply everything&lt;/li&gt;
&lt;li&gt;pull: sends its state with values removed, leaving only keys and version numbers, then returns only necessary updates&lt;/li&gt;
&lt;li&gt;push-pull: like pull but sends a list of participant-key pairs for which if has outdated entries (most efficient)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;precise-reconciliation&#34;&gt;Precise reconciliation&lt;/h3&gt;

&lt;p&gt;The two participants in a gossip exchange send exactly those mappings that are more recent
than those of the peer. Thus, if the participants are p and q, p sends to q the set of deltas.&lt;/p&gt;

&lt;h3 id=&#34;scuttlebutt-reconciliation&#34;&gt;Scuttlebutt reconciliation&lt;/h3&gt;

&lt;p&gt;A gossiper never transmits updates that were already known at the receiver.
If gossip messages were unlimited in size, then the sets contains the exact differences, just like with precise reconciliation.
If a set does not fit in the gossip message, then it is not allowed to use an arbitrary subsetas in precise reconciliation.&lt;/p&gt;

&lt;h3 id=&#34;flow-control&#34;&gt;Flow control&lt;/h3&gt;

&lt;p&gt;The objective of a flow control mechanism for gossip is to determine, adaptively,
the maximum rate at which a participant can submit updates without creating a backlog of updates.
A flow control mechanism should be fair, and under high load afford each participant that wants to submit updates the same update rate.
As there is no global oversight, the flow control mechanism has to be decentralized,
where the desired behavior emerges from participants responding to local events.&lt;/p&gt;

&lt;h3 id=&#34;local-adaptation&#34;&gt;Local adaptation&lt;/h3&gt;

&lt;p&gt;For local adaptation, we use an approach inspired by TCP flow control.
In TCP, the send window adapts according to a strategy called Additive Increase Multiplicative decrease.&lt;/p&gt;

&lt;p&gt;In this strategy, window size grows linearly with each successful transmission,
but is decreased by a certain factor whenever overflow occurs.
In the case of TCP, the overflow signal is the absence of an acknowledgment.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Anti-entropy - gossip information until it is made obsolete.&lt;/p&gt;

&lt;p&gt;Rumor-mongering - gossip information for some of high amount of time with high likelihood all participants received the information.&lt;/p&gt;

&lt;p&gt;AIMD - additive increase multiplicative decrease&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>SEDA: An Architecture for Well-Conditioned, Scalable Internet Services</title>
      <link>http://out13.com/paper/seda-an-architecture-for-well-conditioned-scalable-internet-services/</link>
      <pubDate>Thu, 24 Nov 2016 19:50:13 +0200</pubDate>
      
      <guid>http://out13.com/paper/seda-an-architecture-for-well-conditioned-scalable-internet-services/</guid>
      <description>

&lt;h2 id=&#34;seda-staged-event-driven-architecture&#34;&gt;SEDA - staged event driven architecture&lt;/h2&gt;

&lt;p&gt;A SEDA is intended to support massive concurrency demands and simplify the construction of well-conditioned services.
In SEDA, applications consist of a network of event-driven stages connected by explicit queues.
This architecture allows services to be well-conditioned to load, preventing resources from being overcommitted when demand exceeds service capacity.&lt;/p&gt;

&lt;p&gt;SEDA combines aspects of threads and event-based programming models to manage the concurrency, I/O, scheduling, and resource management needs of Internet services.&lt;/p&gt;

&lt;p&gt;Applications are constructed as a network of stages, each with an associated incoming event queue.
Each stage represents a robust building block that may be individually conditioned to load by thresholding or filtering its event queue.&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;Service is well-conditioned if it behaves like a simple pipeline, where the depth of the pipeline is determined by the path through the network and the processing stages within the service itself.
As the offered load increases, the delivered throughput increases proportionally until the pipeline is full and the throughput saturates; additional load should not degrade throughput.&lt;/p&gt;

&lt;h4 id=&#34;thread-based-concurrency&#34;&gt;Thread based concurrency&lt;/h4&gt;

&lt;p&gt;Operating system overlaps computation and I/O by transparently switching among threads.
Although relatively easy to program, the overheads associated with threading — including cache and TLB misses, scheduling overhead,
and lock contention — can lead to serious performance degradation when the number of threads is large.&lt;/p&gt;

&lt;h4 id=&#34;bounded-thread-pools&#34;&gt;Bounded thread pools&lt;/h4&gt;

&lt;p&gt;To avoid the overuse of threads, a number of systems adopt a coarse form of load conditioning that serves to bound the size of the thread
pool associated with a service. When the number of requests in the server exceeds some fixed limit, additional connections are not accepted.
This approach is used by Web servers such as Apache, IIS, and Netscape Enterprise Server.
By limiting the number of concurrent threads, the server can avoid throughput degradation,
and the overall performance is more robust than the unconstrained thread-per-task model.&lt;/p&gt;

&lt;h4 id=&#34;event-driven-concurrency&#34;&gt;Event-driven concurrency&lt;/h4&gt;

&lt;p&gt;Server consists of a small number of threads (typically one per CPU) that loop continuously, processing events of different types from a queue.
Events may be generated by the operating system or internally by the application,
and generally correspond to network and disk I/O readiness and completion notifications, timers, or other application-specific events.&lt;/p&gt;

&lt;p&gt;Certain I/O operations (in this case, filesystem access) do not have asynchronous interfaces, the main server
process handles these events by dispatching them to helper processes via IPC.
Helper processes issue (blocking) I/O requests and return an event to the main process upon completion.&lt;/p&gt;

&lt;p&gt;Important limitation of this model is that it assumes that event handling threads do not block,
and for this reason nonblocking I/O mechanisms must be employed.&lt;/p&gt;

&lt;h4 id=&#34;structured-event-queues&#34;&gt;Structured event queues&lt;/h4&gt;

&lt;p&gt;Common aspect of these designs is to structure an event-driven application using a
set of event queues to improve code modularity and simplify application design.&lt;/p&gt;

&lt;h4 id=&#34;staged-event-driven-architecture&#34;&gt;Staged event driven architecture&lt;/h4&gt;

&lt;p&gt;Support massive concurrency: To avoid performance degradation due to threads,
SEDA makes use of event-driven execution wherever possible.
This also requires that the system provide efficient and scalable I/O primitives.&lt;/p&gt;

&lt;p&gt;Simplify the construction of well-conditioned services: To reduce the complexity of building Internet services,
SEDA shields application programmers from many of the details of scheduling and resource management.
The design also supports modular construction of these applications, and provides support for debugging and performance profiling.&lt;/p&gt;

&lt;p&gt;Enable introspection: Applications should be able to analyze the request stream to adapt behavior to
changing load conditions. For example, the system should be able to
prioritize and filter requests to support degraded service under heavy load.&lt;/p&gt;

&lt;p&gt;Support self-tuning resource management: Rather than mandate a priori
knowledge of application resource requirements and client load
characteristics, the system should adjust its resource management parameters dynamically
to meet performance targets. For example, the number of threads allocated to
a stage can be determined automatically based on perceived concurrency demands,
rather than hard-coded by the programmer or administrator.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Building blocks&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The fundamental unit of processing within SEDA is the stage.
Stage is a self-contained application component consisting of an event handler, an incoming event queue, and a thread pool.&lt;/p&gt;

&lt;p&gt;The core logic for each stage is provided by the event handler, the input to which is a batch of multiple events.
Event handlers do not have direct control over queue operations or threads.&lt;/p&gt;

&lt;p&gt;Event queues in SEDA is that they may be finite: that is, an enqueue operation may fail
if the queue wishes to reject new entries, say, because it has reached a threshold.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Interaction of Buffer Size and TCP Protocol Handling and its Impact</title>
      <link>http://out13.com/paper/the-interaction-of-buffer-size-and-tcp-protocol-handling/</link>
      <pubDate>Thu, 17 Nov 2016 19:23:07 +0200</pubDate>
      
      <guid>http://out13.com/paper/the-interaction-of-buffer-size-and-tcp-protocol-handling/</guid>
      <description>

&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;

&lt;p&gt;Miercom was engaged by Cisco Systems to conduct independent testing of two vendors’ top of the line,
data-center switch-routers, including the Cisco Nexus 92160YC-X and Nexus 9272Q switches and the Arista 7280SE-72 switch.&lt;/p&gt;

&lt;h4 id=&#34;tcp-congestion-control-versus-system-buffer-management&#34;&gt;TCP Congestion Control versus System Buffer Management&lt;/h4&gt;

&lt;p&gt;TCP congestion control. The Transmission Control Protocol (TCP) is the Layer-4 control
protocol (atop IP at Layer 3) that ensures a block of data that’s sent is received intact.
Invented 35 years ago, TCP handles how blocks of data are broken up, sequenced, sent,
reconstructed and verified at the recipient’s end. The congestion-control mechanism
was added to TCP in 1988 to avoid network congestion meltdown. It makes sure data
transfers are accelerated or slowed down, exploiting the bandwidth that’s available,
depending on network conditions.&lt;/p&gt;

&lt;p&gt;System buffer management. Every network device that transports data has buffers,
usually statically allocated on a per-port basis or dynamically shared by multiple ports, so
that periodic data bursts can be accommodated without having to drop packets.
Network systems such as switch-routers are architected differently, however, and can
vary significantly in the size of their buffers and how they manage different traffic flows.&lt;/p&gt;

&lt;h4 id=&#34;deep-buffer-vs-intelligent-buffer&#34;&gt;Deep buffer vs Intelligent buffer&lt;/h4&gt;

&lt;p&gt;A common practice is to put in as much buffer as possible. However, since the
buffer space is a common resource shared by the inevitable mixture of elephant and mice flows,
how to use this shared resource can significantly impact applications’ performance.&lt;/p&gt;

&lt;p&gt;The deeper the buffer, the longer the queue and the longer the latency. So more buffer does
not necessarily guarantee better small-flow performance, it often leads to longer queuing delay
and hence longer flow completion time.&lt;/p&gt;

&lt;p&gt;Therefore, no one benefits from simple deep buffering: mice flows aren’t guaranteed buffer
resources and can suffer from long queuing delays and bandwidth hungry elephant flows suffer
because large buffers do not create more link bandwidth.&lt;/p&gt;

&lt;h4 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;Since mice flows are often mission critical (including, for example, control and alarm messages,
Hadoop application communications, etc.), giving these flows a priority buffer pathway enables
them to complete faster and their applications to perform better overall. The above test results
show that expediting mice flows and regulating the elephant flows early under the intelligent
buffer architecture on the Cisco Nexus 92160YC-X and 9272Q switches can bring orders of
magnitude better performance for mission critical flows without causing elephant flows to slow
down.&lt;/p&gt;

&lt;p&gt;Intelligent buffering allows the elephant and mice flows to share network buffers gracefully:
there is enough buffer space for the bursts of mice flows while the elephant flows are properly
regulated to fully utilize the link capacity. Simple, deep buffering can lead to collateral damage
in the form of longer queuing latency, and hence longer flow completion time for all flow types.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Elephant - big flows&lt;/p&gt;

&lt;p&gt;Mice - small flows&lt;/p&gt;

&lt;p&gt;FCT - flow completion time&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Replication Under Scalable Hashing: A Family of Algorithms for Scalable Decentralized Data Distribution</title>
      <link>http://out13.com/paper/replication-under-scalable-hashing--a-family-of-algorithms-for-scalable-decentralized-data-distribution/</link>
      <pubDate>Thu, 10 Nov 2016 22:27:23 +0200</pubDate>
      
      <guid>http://out13.com/paper/replication-under-scalable-hashing--a-family-of-algorithms-for-scalable-decentralized-data-distribution/</guid>
      <description>

&lt;h2 id=&#34;replication-under-scalable-hashing&#34;&gt;Replication Under Scalable Hashing&lt;/h2&gt;

&lt;p&gt;Typical algorithms for decentralized data distribution work best in a system that is fully built before it first used;
adding or removing components results in either extensive reorganization of data or load imbalance in the system.&lt;/p&gt;

&lt;p&gt;RUSH variants also support weighting, allowing disks of different vintages to be added to a system.&lt;/p&gt;

&lt;p&gt;RUSH variants is optimal or near-optimal reorganization. When new disks are added to the system,
or old disks are retired, RUSH variants minimize the number of objects that need to
be moved in order to bring the system back into balance.&lt;/p&gt;

&lt;p&gt;RUSH variants can perform reorganization online without locking the filesystem for a long time to relocate data.&lt;/p&gt;

&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;

&lt;p&gt;Subcluster in a system managed by RUSH t must have at least as many disks as an object has replicas.&lt;/p&gt;

&lt;p&gt;RUSH t is the best algorithms for distributing data over very large clusters of disks.&lt;/p&gt;

&lt;p&gt;RUSH r may be the best option for systems which need to remove disks one at a time from the system.&lt;/p&gt;

&lt;p&gt;RUSH p may be the best option for smaller systems where storage space is at a premium.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;RUSH t - RUSH tree&lt;/p&gt;

&lt;p&gt;RUSH r - RUSH support for removal&lt;/p&gt;

&lt;p&gt;PUSH p - RUSH placement using prime numbers&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Dynamo: Amazon’s Highly Available Key-value Store</title>
      <link>http://out13.com/paper/dynamo-amazon-highly-available-key-value-store/</link>
      <pubDate>Sun, 06 Nov 2016 12:32:44 +0200</pubDate>
      
      <guid>http://out13.com/paper/dynamo-amazon-highly-available-key-value-store/</guid>
      <description>

&lt;h2 id=&#34;dynamo&#34;&gt;Dynamo&lt;/h2&gt;

&lt;p&gt;Dynamo sacrifices Consistency for Availability under certain failure scenarios.
It makes extensive use of object versioning and application-assisted conflict resolution in a manner that provides a novel interface for developers to use.&lt;/p&gt;

&lt;p&gt;Gossip based distributed failure detection and membership protocol.&lt;/p&gt;

&lt;h3 id=&#34;query-model&#34;&gt;Query Model&lt;/h3&gt;

&lt;p&gt;Read &amp;amp; Write operations to data item that is uniquely identified by a key.
State is stored as blobs.
Targets application that store objects up to 1MB.&lt;/p&gt;

&lt;h3 id=&#34;acid&#34;&gt;ACID&lt;/h3&gt;

&lt;p&gt;Dynamo targets applications that operate with weaker consistency (the “C” in ACID) if this results in high availability.&lt;/p&gt;

&lt;p&gt;No isolation guarantees. Permits only single key updates.&lt;/p&gt;

&lt;h3 id=&#34;design&#34;&gt;Design&lt;/h3&gt;

&lt;p&gt;Incremental scalability: Dynamo should be able to scale out one storage host (henceforth, referred to as “node”) at a time,
with minimal impact on both operators of the system and the system itself.&lt;/p&gt;

&lt;p&gt;Symmetry: Every node in Dynamo should have the same set of responsibilities as its peers; there should be no distinguished node
or nodes that take special roles or extra set of responsibilities. In our experience, symmetry simplifies the process of system
provisioning and maintenance.&lt;/p&gt;

&lt;p&gt;Decentralization: An extension of symmetry, the design should favor decentralized peer-to-peer techniques over centralized
control. In the past, centralized control has resulted in outages and the goal is to avoid it as much as possible. This leads to a simpler,
more scalable, and more available system.&lt;/p&gt;

&lt;p&gt;Heterogeneity: The system needs to be able to exploit heterogeneity in the infrastructure it runs on. e.g. the work
distribution must be proportional to the capabilities of the individual servers. This is essential in adding new nodes with
higher capacity without having to upgrade all hosts at once.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bigtable: A Distributed Storage System for Structured Data</title>
      <link>http://out13.com/paper/bigtable-a-distributed-storage-system-for-structured-data/</link>
      <pubDate>Thu, 03 Nov 2016 19:54:45 +0200</pubDate>
      
      <guid>http://out13.com/paper/bigtable-a-distributed-storage-system-for-structured-data/</guid>
      <description>

&lt;h2 id=&#34;bigtable&#34;&gt;Bigtable&lt;/h2&gt;

&lt;p&gt;Bigtable is a distributed storage system for managing structured data that is
designed to scale to a very large size: petabytes of data across thousands of commodity servers.&lt;/p&gt;

&lt;p&gt;Bigtable does not support a full relational data model; instead, it provides
clients with a simple data model that supports dynamic control over data layout
and format, and allows clients to reason about the locality properties of the data
represented in the underlying storage.&lt;/p&gt;

&lt;h3 id=&#34;data-model&#34;&gt;Data model&lt;/h3&gt;

&lt;p&gt;A Bigtable is a sparse, distributed, persistent multidimensional sorted map.
The map is indexed by a row key, column key, and a timestamp; each value in the map
is an uninterpreted array of bytes.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;(row:string, column:string, time:int64) → string&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Bigtable maintains data in lexicographic order by row key. The row range for a table is dynamically partitioned.
Each row range is called a tablet, which is the unit of distribution and load balancing.&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;File format to store data: SSTable provides a persistent, ordered immutable map from keys to values, where both keys and values are arbitrary byte strings.&lt;/p&gt;

&lt;p&gt;First find the appropriate block by performing a binary search in the in-memory index, and then reading the appropriate block from disk.&lt;/p&gt;

&lt;p&gt;Bigtable relies on a highly-available and persistent distributed lock service called Chubby.
Chubby service consists of five active replicas, one of which is elected to be the master and actively serve requests.&lt;/p&gt;

&lt;p&gt;Chubby uses the Paxos algorithm to keep its replicas consistent in the face of failure&lt;/p&gt;

&lt;h3 id=&#34;client&#34;&gt;Client&lt;/h3&gt;

&lt;p&gt;The client library caches tablet locations.
If the client does not know the location of a tablet, or if it discovers that cached
location information is incorrect, then it recursively moves up the tablet location hierarchy.&lt;/p&gt;

&lt;h3 id=&#34;caching&#34;&gt;Caching&lt;/h3&gt;

&lt;p&gt;To improve read performance, tablet servers use two levels of caching.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Scan Cache is a higher-level cache that caches the key-value pairs returned by the SSTable interface to the tablet server code.&lt;/li&gt;
&lt;li&gt;Block Cache is a lower-level cache that caches SSTables blocks that were read from GFS.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bloom filter allows us to ask whether an SSTable might contain any data for a specified row/column pair.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;GFS - Google File System&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Ownership is theft experiences building an embedded os in rust</title>
      <link>http://out13.com/paper/ownership-is-theft-experiences-building-an-embedded-os-in-rust/</link>
      <pubDate>Thu, 25 Aug 2016 20:39:03 +0300</pubDate>
      
      <guid>http://out13.com/paper/ownership-is-theft-experiences-building-an-embedded-os-in-rust/</guid>
      <description>

&lt;h2 id=&#34;embedded-os-in-rust&#34;&gt;Embedded OS in Rust&lt;/h2&gt;

&lt;p&gt;Embedded systems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;lack hardware protection mechanism&lt;/li&gt;
&lt;li&gt;less tolerant to crashes&lt;/li&gt;
&lt;li&gt;no easy way for debugging&lt;/li&gt;
&lt;li&gt;GC introduces non-deterministic delay&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;rust&#34;&gt;Rust&lt;/h3&gt;

&lt;p&gt;Rust, a new systems programming language, provides compile-time memory safety checks to help eliminate runtime bugs that manifest from improper memory management.&lt;/p&gt;

&lt;p&gt;Rust’s ownership model prevents otherwise safe resource sharing common in the embedded domain, conflicts with the reality of hardware resources, and hinders using closures for programming asynchronously.&lt;/p&gt;

&lt;p&gt;Rust achieves memory and type safety without garbage collection by using mechanism, derived from affine type and unique pointers, called ownership.&lt;/p&gt;

&lt;p&gt;Preserved type safety without relying on a runtime GC for memory management.&lt;/p&gt;

&lt;p&gt;Allows the programmer to explicitly separate code which is strictly bound to the type system from code which may subvert it.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Borrowing&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Value can only be mutably borrowed if there are no other borrows of the value.&lt;/li&gt;
&lt;li&gt;Borrows cannot outlive the value they borrow. This prevents dangling pointer bugs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;execution-context-extension-for-rust&#34;&gt;Execution context (extension for Rust)&lt;/h3&gt;

&lt;p&gt;Reflects the thread of a value&amp;rsquo;s owner in its type.&lt;/p&gt;

&lt;p&gt;Allows multiple borrows of a value from within same thread, but not across threads.&lt;/p&gt;

&lt;p&gt;The goal of execution context is to allow program mutably borrow values multiple times as long as those borrows are never shared between threads.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On the fly garbage collection</title>
      <link>http://out13.com/paper/on-the-fly-garbage-collection/</link>
      <pubDate>Thu, 25 Aug 2016 19:13:56 +0300</pubDate>
      
      <guid>http://out13.com/paper/on-the-fly-garbage-collection/</guid>
      <description>

&lt;p&gt;In our abstract form of the problem, we consider a
directed graph of varying structure but with a fixed
number of nodes, in which each node has at most two
outgoing edges. More precisely, each node may have a
left-hand outgoing edge and may have a right-hand
outgoing edge, but either of them or both may be missing.
In this graph a fixed set of nodes exists, called &amp;ldquo;the
roots.&amp;rdquo; A node is called &amp;ldquo;reachable&amp;rdquo; if it is reachable
from at least one root via a directed path along the edges.&lt;/p&gt;

&lt;p&gt;The subgraph consists of all reachable nodes and their interconnections is
called &amp;lsquo;the data structure&amp;rsquo;; nonreachable nodes that do not belong to the
data structure are called garbage.&lt;/p&gt;

&lt;p&gt;Data structure can modified:
 - Redirecting an outgoing edge of a reachable node towards an already reachable one.
 - Redirecting an outgoing edge of a reachable node towards a not yet reachable one without outgoing edges.
 - Adding&amp;ndash;where an outgoing edge was missing an edge pointing from a reachable node towards an already reachable one.
 - Adding&amp;ndash;where an outgoing edge was missing an edge pointing from a reachable node towards a not yet reachable one without outgoing edges.
 - Removing an outgoing edge of a reachable node&lt;/p&gt;

&lt;p&gt;Mutator: redirect an outgoing edge of reachable node towards an already reachable one.&lt;/p&gt;

&lt;p&gt;Collector:
 - marking phase: mark all reachable nodes
 - appending phase: append all unmarked nodes to the free list and remove the markings from all marked nodes&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Free list - collection of nodes that have been identified as garbage.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Queues Are Databases</title>
      <link>http://out13.com/paper/queues-are-databases/</link>
      <pubDate>Fri, 12 Aug 2016 16:57:55 +0300</pubDate>
      
      <guid>http://out13.com/paper/queues-are-databases/</guid>
      <description>

&lt;h2 id=&#34;queued-transaction-processing-over-pure-client-server-transaction-processing&#34;&gt;Queued transaction processing over pure client-server transaction processing.&lt;/h2&gt;

&lt;p&gt;Queued systems are build on top of direct systems.&lt;/p&gt;

&lt;p&gt;TP systems offer both queued and direct transaction processing. They offer both client-server and P2P direct processing.&lt;/p&gt;

&lt;p&gt;Queue manager is best built as a naive resource manager atop an object-relational database system.
That system must have good concurrency control, recovery, triggers, security, operations interfaces, and utilities.&lt;/p&gt;

&lt;p&gt;Queues pose difficult problems when implemented atop a database:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Performance: An enqueue transaction is an insert followed by a commit. This places
extreme performance demands on the concurrency control and recovery components
of a database &amp;ndash; it exposes hotspots and high-overhead code.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Concurrency control: The dequeue transaction typically involves deleting a record from
the queue, processing the request, enqueuing results in other queues, and then
committing. Serializable isolation requires that there can be at most one dequeue
executing at a time against each queue. This suggests that queues need lower, indeed specialized, isolation levels.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Read past: locks allow a program to skip over dirty (uncommitted records) to find the
first committed record. This is what a dequeue() operation wants.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Read through: locks allow a program to examine records that have not yet been
committed. This is useful in polling the status of a queued request that is currently
being processed.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Notify: allow a program to wait for a state change in a lock. This allows a
dequeue() operation to wait for one or more queues to become non-empty.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;MOM - message oriented middleware&lt;/p&gt;

&lt;p&gt;TP - transaction processing&lt;/p&gt;

&lt;p&gt;P2P - peer to peer&lt;/p&gt;

&lt;p&gt;ORB - object request broker&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>An Argument for Increasing TCP’s Initial Congestion Window</title>
      <link>http://out13.com/paper/an-argument-for-increasing-tcp-initial-congestion-window/</link>
      <pubDate>Thu, 04 Aug 2016 22:02:54 +0300</pubDate>
      
      <guid>http://out13.com/paper/an-argument-for-increasing-tcp-initial-congestion-window/</guid>
      <description>

&lt;h2 id=&#34;tcp-congestion-window&#34;&gt;TCP congestion window&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;TCP flows start with initial congestion window of 4 segments (4KB of data).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Window if critical for how quickly flows can finish.&lt;/p&gt;

&lt;p&gt;Increase in 15KB congestion window improves average HTTP latency by 10%, mostly benefits RTT and BDP.&lt;/p&gt;

&lt;p&gt;Slow start increases congestion window by the number of data segments acknowledged for each received ACK.&lt;/p&gt;

&lt;p&gt;TCP latency is dominated by the number of round-trip times in slow-start phase.&lt;/p&gt;

&lt;p&gt;Increasing init_cwnd enables transfers to finish in fewer RTT.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;BDP - bandwidth delay product.&lt;/p&gt;

&lt;p&gt;RTT - round trip delay time.&lt;/p&gt;

&lt;p&gt;Wep page average size - 384KB.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center</title>
      <link>http://out13.com/paper/mesos-platform-for-resource-sharing/</link>
      <pubDate>Thu, 28 Apr 2016 19:50:29 +0300</pubDate>
      
      <guid>http://out13.com/paper/mesos-platform-for-resource-sharing/</guid>
      <description>

&lt;h2 id=&#34;platform-for-resource-sharing&#34;&gt;Platform for resource sharing&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Sharing improves cluster utilization and avoids per-framework data repli-cation
Organizations will want to run multiple frameworks in the same cluster, picking the best one for each application.
Sharing a cluster between frameworks improves utilization and allows applications to share access to large datasets that may be too costly to replicate&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;Mesos decides how many resources to offer each framework, based on an organizational policy such as fair sharing, while frameworks decide which resources to accept and which tasks to run on them.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Design philosophy - define a minimal interface that enables efficient resource sharing across frameworks, and otherwise push control of task scheduling and execution to the frameworks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The master decides how many resources to offer to each framework according to a given organizational policy, such as fair sharing, or strict priority.&lt;/p&gt;

&lt;p&gt;A framework running on top of Mesos consists of two components: a scheduler that registers with the master to be offered resources, and an executor process that is launched on slave nodes to run the framework’s tasks.&lt;/p&gt;

&lt;p&gt;Master determines how many resources are offered to each framework, the frameworks’ schedulers select which of the offered resources to use.&lt;/p&gt;

&lt;p&gt;When a frameworks accepts offered resources, it passes to Mesos a description of the tasks it wants to run on them.&lt;/p&gt;

&lt;p&gt;Frameworks achieve data locality by rejecting offers.&lt;/p&gt;

&lt;p&gt;Mesos can reallocate resources if cluster becomes filled with long tasks by revoking (killing) tasks with grace period.&lt;/p&gt;

&lt;p&gt;Isolation through existing OS isolation mechanisms usually system containers. These technologies can limit the CPU, memory, network bandwidth and I/O usage of a process tree.&lt;/p&gt;

&lt;p&gt;Mesos lets them short-circuit the rejection process and avoid communication by providing filters to the master. We support two types of filters: “only offer nodes from list L” and “only offer nodes with at least R resources free”.&lt;/p&gt;

&lt;p&gt;Two types of resources: mandatory and preferred&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A resource is mandatory if a framework must acquire it in order to run.&lt;/li&gt;
&lt;li&gt;Preferred if a framework performs “better” using it, but can also run using another equivalent resource.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Two-level scheduling mechanism called resource offers&lt;/p&gt;

&lt;p&gt;Delegating control over scheduling to the framework&lt;/p&gt;

&lt;p&gt;Resource offer - encapsulates a bundle of resources that a framework can allocate on a cluster node to run tasks&lt;/p&gt;

&lt;p&gt;Framework ramp-up time - time it takes a new framework to achieve its allocation&lt;/p&gt;

&lt;p&gt;Job completion time - time it takes a job to complete, assuming one job per framework;&lt;/p&gt;

&lt;p&gt;System utilization - total cluster utilization.&lt;/p&gt;

&lt;p&gt;Scale up - frameworks can elastically increase their allocation to take advantage of free resources.&lt;/p&gt;

&lt;p&gt;Scale down - frameworks can relinquish resources without significantly impacting their performance.&lt;/p&gt;

&lt;p&gt;Minimum allocation - frameworks require a certain minimum number of slots before they can start using their slots.&lt;/p&gt;

&lt;p&gt;Task distribution - distribution of the task durations. We consider both homogeneous and heterogeneous distributions.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>