<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Review on Ernestas Poškus.io</title>
    <link>http://out13.com/tags/review/index.xml</link>
    <description>Recent content in Review on Ernestas Poškus.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://out13.com/tags/review/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Ownership is theft experiences building an embedded os in rust</title>
      <link>http://out13.com/paper/ownership-is-theft-experiences-building-an-embedded-os-in-rust/</link>
      <pubDate>Thu, 25 Aug 2016 20:39:03 +0300</pubDate>
      
      <guid>http://out13.com/paper/ownership-is-theft-experiences-building-an-embedded-os-in-rust/</guid>
      <description>

&lt;h2 id=&#34;embedded-os-in-rust&#34;&gt;Embedded OS in Rust&lt;/h2&gt;

&lt;p&gt;Embedded systems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;lack hardware protection mechanism&lt;/li&gt;
&lt;li&gt;less tolerant to crashes&lt;/li&gt;
&lt;li&gt;no easy way for debugging&lt;/li&gt;
&lt;li&gt;GC introduces non-deterministic delay&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;rust&#34;&gt;Rust&lt;/h3&gt;

&lt;p&gt;Rust, a new systems programming language, provides compile-time memory safety checks to help eliminate runtime bugs that manifest from improper memory management.&lt;/p&gt;

&lt;p&gt;Rust’s ownership model prevents otherwise safe resource sharing common in the embedded domain, conflicts with the reality of hardware resources, and hinders using closures for programming asynchronously.&lt;/p&gt;

&lt;p&gt;Rust achieves memory and type safety without garbage collection by using mechanism, derived from affine type and unique pointers, called ownership.&lt;/p&gt;

&lt;p&gt;Preserved type safety without relying on a runtime GC for memory management.&lt;/p&gt;

&lt;p&gt;Allows the programmer to explicitly separate code which is strictly bound to the type system from code which may subvert it.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Borrowing&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Value can only be mutably borrowed if there are no other borrows of the value.&lt;/li&gt;
&lt;li&gt;Borrows cannot outlive the value they borrow. This prevents dangling pointer bugs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;execution-context-extension-for-rust&#34;&gt;Execution context (extension for Rust)&lt;/h3&gt;

&lt;p&gt;Reflects the thread of a value&amp;rsquo;s owner in its type.&lt;/p&gt;

&lt;p&gt;Allows multiple borrows of a value from within same thread, but not across threads.&lt;/p&gt;

&lt;p&gt;The goal of execution context is to allow program mutably borrow values multiple times as long as those borrows are never shared between threads.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On the fly garbage collection</title>
      <link>http://out13.com/paper/on-the-fly-garbage-collection/</link>
      <pubDate>Thu, 25 Aug 2016 19:13:56 +0300</pubDate>
      
      <guid>http://out13.com/paper/on-the-fly-garbage-collection/</guid>
      <description>

&lt;p&gt;In our abstract form of the problem, we consider a
directed graph of varying structure but with a fixed
number of nodes, in which each node has at most two
outgoing edges. More precisely, each node may have a
left-hand outgoing edge and may have a right-hand
outgoing edge, but either of them or both may be missing.
In this graph a fixed set of nodes exists, called &amp;ldquo;the
roots.&amp;rdquo; A node is called &amp;ldquo;reachable&amp;rdquo; if it is reachable
from at least one root via a directed path along the edges.&lt;/p&gt;

&lt;p&gt;The subgraph consists of all reachable nodes and their interconnections is
called &amp;lsquo;the data structure&amp;rsquo;; nonreachable nodes that do not belong to the
data structure are called garbage.&lt;/p&gt;

&lt;p&gt;Data structure can modified:
 - Redirecting an outgoing edge of a reachable node towards an already reachable one.
 - Redirecting an outgoing edge of a reachable node towards a not yet reachable one without outgoing edges.
 - Adding&amp;ndash;where an outgoing edge was missing an edge pointing from a reachable node towards an already reachable one.
 - Adding&amp;ndash;where an outgoing edge was missing an edge pointing from a reachable node towards a not yet reachable one without outgoing edges.
 - Removing an outgoing edge of a reachable node&lt;/p&gt;

&lt;p&gt;Mutator: redirect an outgoing edge of reachable node towards an already reachable one.&lt;/p&gt;

&lt;p&gt;Collector:
 - marking phase: mark all reachable nodes
 - appending phase: append all unmarked nodes to the free list and remove the markings from all marked nodes&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Free list - collection of nodes that have been identified as garbage.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Queues Are Databases</title>
      <link>http://out13.com/paper/queues-are-databases/</link>
      <pubDate>Fri, 12 Aug 2016 16:57:55 +0300</pubDate>
      
      <guid>http://out13.com/paper/queues-are-databases/</guid>
      <description>

&lt;h2 id=&#34;queued-transaction-processing-over-pure-client-server-transaction-processing&#34;&gt;Queued transaction processing over pure client-server transaction processing.&lt;/h2&gt;

&lt;p&gt;Queued systems are build on top of direct systems.&lt;/p&gt;

&lt;p&gt;TP systems offer both queued and direct transaction processing. They offer both client-server and P2P direct processing.&lt;/p&gt;

&lt;p&gt;Queue manager is best built as a naive resource manager atop an object-relational database system.
That system must have good concurrency control, recovery, triggers, security, operations interfaces, and utilities.&lt;/p&gt;

&lt;p&gt;Queues pose difficult problems when implemented atop a database:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Performance: An enqueue transaction is an insert followed by a commit. This places
extreme performance demands on the concurrency control and recovery components
of a database &amp;ndash; it exposes hotspots and high-overhead code.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Concurrency control: The dequeue transaction typically involves deleting a record from
the queue, processing the request, enqueuing results in other queues, and then
committing. Serializable isolation requires that there can be at most one dequeue
executing at a time against each queue. This suggests that queues need lower, indeed specialized, isolation levels.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Read past: locks allow a program to skip over dirty (uncommitted records) to find the
first committed record. This is what a dequeue() operation wants.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Read through: locks allow a program to examine records that have not yet been
committed. This is useful in polling the status of a queued request that is currently
being processed.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Notify: allow a program to wait for a state change in a lock. This allows a
dequeue() operation to wait for one or more queues to become non-empty.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;MOM - message oriented middleware&lt;/p&gt;

&lt;p&gt;TP - transaction processing&lt;/p&gt;

&lt;p&gt;P2P - peer to peer&lt;/p&gt;

&lt;p&gt;ORB - object request broker&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>An Argument for Increasing TCP’s Initial Congestion Window</title>
      <link>http://out13.com/paper/an-argument-for-increasing-tcp-initial-congestion-window/</link>
      <pubDate>Thu, 04 Aug 2016 22:02:54 +0300</pubDate>
      
      <guid>http://out13.com/paper/an-argument-for-increasing-tcp-initial-congestion-window/</guid>
      <description>

&lt;h2 id=&#34;tcp-congestion-window&#34;&gt;TCP congestion window&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;TCP flows start with initial congestion window of 4 segments (4KB of data).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Window if critical for how quickly flows can finish.&lt;/p&gt;

&lt;p&gt;Increase in 15KB congestion window improves average HTTP latency by 10%, mostly benefits RTT and BDP.&lt;/p&gt;

&lt;p&gt;Slow start increases congestion window by the number of data segments acknowledged for each received ACK.&lt;/p&gt;

&lt;p&gt;TCP latency is dominated by the number of round-trip times in slow-start phase.&lt;/p&gt;

&lt;p&gt;Increasing init_cwnd enables transfers to finish in fewer RTT.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;BDP - bandwidth delay product.&lt;/p&gt;

&lt;p&gt;RTT - round trip delay time.&lt;/p&gt;

&lt;p&gt;Wep page average size - 384KB.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center</title>
      <link>http://out13.com/paper/mesos-platform-for-resource-sharing/</link>
      <pubDate>Thu, 28 Apr 2016 19:50:29 +0300</pubDate>
      
      <guid>http://out13.com/paper/mesos-platform-for-resource-sharing/</guid>
      <description>

&lt;h2 id=&#34;platform-for-resource-sharing&#34;&gt;Platform for resource sharing&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Sharing improves cluster utilization and avoids per-framework data repli-cation
Organizations will want to run multiple frameworks in the same cluster, picking the best one for each application.
Sharing a cluster between frameworks improves utilization and allows applications to share access to large datasets that may be too costly to replicate&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;Mesos decides how many resources to offer each framework, based on an organizational policy such as fair sharing, while frameworks decide which resources to accept and which tasks to run on them.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Design philosophy - define a minimal interface that enables efficient resource sharing across frameworks, and otherwise push control of task scheduling and execution to the frameworks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The master decides how many resources to offer to each framework according to a given organizational policy, such as fair sharing, or strict priority.&lt;/p&gt;

&lt;p&gt;A framework running on top of Mesos consists of two components: a scheduler that registers with the master to be offered resources, and an executor process that is launched on slave nodes to run the framework’s tasks.&lt;/p&gt;

&lt;p&gt;Master determines how many resources are offered to each framework, the frameworks’ schedulers select which of the offered resources to use.&lt;/p&gt;

&lt;p&gt;When a frameworks accepts offered resources, it passes to Mesos a description of the tasks it wants to run on them.&lt;/p&gt;

&lt;p&gt;Frameworks achieve data locality by rejecting offers.&lt;/p&gt;

&lt;p&gt;Mesos can reallocate resources if cluster becomes filled with long tasks by revoking (killing) tasks with grace period.&lt;/p&gt;

&lt;p&gt;Isolation through existing OS isolation mechanisms usually system containers. These technologies can limit the CPU, memory, network bandwidth and I/O usage of a process tree.&lt;/p&gt;

&lt;p&gt;Mesos lets them short-circuit the rejection process and avoid communication by providing filters to the master. We support two types of filters: “only offer nodes from list L” and “only offer nodes with at least R resources free”.&lt;/p&gt;

&lt;p&gt;Two types of resources: mandatory and preferred&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A resource is mandatory if a framework must acquire it in order to run.&lt;/li&gt;
&lt;li&gt;Preferred if a framework performs “better” using it, but can also run using another equivalent resource.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Two-level scheduling mechanism called resource offers&lt;/p&gt;

&lt;p&gt;Delegating control over scheduling to the framework&lt;/p&gt;

&lt;p&gt;Resource offer - encapsulates a bundle of resources that a framework can allocate on a cluster node to run tasks&lt;/p&gt;

&lt;p&gt;Framework ramp-up time - time it takes a new framework to achieve its allocation&lt;/p&gt;

&lt;p&gt;Job completion time - time it takes a job to complete, assuming one job per framework;&lt;/p&gt;

&lt;p&gt;System utilization - total cluster utilization.&lt;/p&gt;

&lt;p&gt;Scale up - frameworks can elastically increase their allocation to take advantage of free resources.&lt;/p&gt;

&lt;p&gt;Scale down - frameworks can relinquish resources without significantly impacting their performance.&lt;/p&gt;

&lt;p&gt;Minimum allocation - frameworks require a certain minimum number of slots before they can start using their slots.&lt;/p&gt;

&lt;p&gt;Task distribution - distribution of the task durations. We consider both homogeneous and heterogeneous distributions.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Tiny LFU highly efficient cache admission policy</title>
      <link>http://out13.com/paper/tiny-lfu-highly-efficient-cache-admission-policy/</link>
      <pubDate>Fri, 22 Apr 2016 21:26:15 +0300</pubDate>
      
      <guid>http://out13.com/paper/tiny-lfu-highly-efficient-cache-admission-policy/</guid>
      <description>

&lt;h2 id=&#34;frequency-based-cache-admission-policy&#34;&gt;Frequency based cache admission policy&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Approximate LFU structure called TinyLFU, which maintains an approximate representation of the access frequency of a large sample of recently accessed items.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;TinyLFU is very compact and light-weight as it builds upon Bloom filter theory.&lt;/p&gt;

&lt;h3 id=&#34;usage&#34;&gt;Usage&lt;/h3&gt;

&lt;p&gt;The intuitive reason why caching works is that data accesses in many
domains of computer science exhibit a considerable degree of “locality”.&lt;/p&gt;

&lt;p&gt;When a data item is accessed, if it already appears in the cache, we say that there is a cache hit; otherwise, it is a cache miss. The ratio between the number of cache hits and the total number of data accesses is known as the cache hit-ratio.&lt;/p&gt;

&lt;p&gt;Admission policy - caching architecture in which an accessed item is only inserted into the cache if an admission policy decides that the cache hit ratio is likely to benefit from replacing it with the cache victim (as chosen by the cache’s replacement policy).&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;The cache eviction policy picks a cache victim, while TinyLFU decides if replacing the cache victim with the new item is expected to increase the hit-ratio.
To do so, TinyLFU maintains statistics of items frequency over a sizable recent history. Storing these statistics is considered prohibitively expensive for practical implementation and therefore TinyLFU approximates them in a highly efficient manner. To keep the history fresh an aging process is performed periodically or incrementally to halve all of the counters.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Time locality - access pattern, and consequently the corresponding probability distribution, change over time&lt;/p&gt;

&lt;p&gt;WLFU - Window Least Frequently Used, access frequency for a window, needs to keep track order of requests. Samples of the request stream (called window).&lt;/p&gt;

&lt;p&gt;PLFU - Perfect LFU, popularity based has metadata with counters&lt;/p&gt;

&lt;p&gt;In-memory LFU, outperformed by WLFU at the cost of larger meta-data&lt;/p&gt;

&lt;p&gt;SLRU - Segmented Least Recently Used, policy captures recent popularity by distinguishing between tem-porally popular items that are accessed at least twice in a short window vs. items accessed only once during that period&lt;/p&gt;

&lt;p&gt;LRU-K - combination of LRU &amp;amp; LFU the last K occurrences of each element are remembered. Using this data, LRU-K statistically estimates the momentary frequency of items in order to keep the most frequent pages in memory.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Container based operating system virtualization</title>
      <link>http://out13.com/paper/container-based-operating-system-virtualization/</link>
      <pubDate>Tue, 19 Apr 2016 19:30:48 +0300</pubDate>
      
      <guid>http://out13.com/paper/container-based-operating-system-virtualization/</guid>
      <description>

&lt;h2 id=&#34;alternative-to-hypervisors&#34;&gt;Alternative to hypervisors.&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Workload requirements for a given system will direct users to the point in the design space that
requires the least trade-off.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;sharing-over-isolation&#34;&gt;Sharing over isolation?&lt;/h3&gt;

&lt;p&gt;Hypervisors often deployed to let a single machine host multiple, unrelated
applications, which may run on behalf of independent organizations, as is common when a data center
consolidates multiple physical servers. Hypervisors favor full isolation over sharing.
However, when each virtual machine is running the same kernel and similar operating system
distributions, the degree of isolation offered by hypervisors comes at the cost of efficiency
relative to running all applications on a single kernel.&lt;/p&gt;

&lt;h3 id=&#34;usage&#34;&gt;Usage&lt;/h3&gt;

&lt;p&gt;Software configuration problems incompatibilities between specific OS distributions.&lt;/p&gt;

&lt;p&gt;Resource isolation corresponds to the ability to account for and enforce the resource consumption of one VM such that guarantees and fair shares are preserved for other VM&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;Many hybrid approaches are also possible: for instance, a system may enforce fair sharing of resources between classes of VMs, which lets one overbook available resources while preventing starvation in overload scenarios.&lt;/p&gt;

&lt;p&gt;The key point is that both hypervisors and COS&amp;rsquo;s incorporate sophisticated resource schedulers to avoid or minimize crosstalk.&lt;/p&gt;

&lt;h3 id=&#34;security-isolation&#34;&gt;Security isolation&lt;/h3&gt;

&lt;p&gt;Configuration independence - cannot conflict with other VM&amp;rsquo;s
Safety - global namespace shared&lt;/p&gt;

&lt;h3 id=&#34;fair-share-and-reservations&#34;&gt;Fair share and Reservations&lt;/h3&gt;

&lt;p&gt;Vserver implements CPU isolation by overlaying a token TBF on top of standard O(1) Linux CPU scheduler.&lt;/p&gt;

&lt;p&gt;For memory storage one can specify the following limits:
 * a) the maximum resident set size (RSS)
 * b) number of anonymous memory pages have (ANON)
 * c) number of pages that may be pinned into memory using mlock() and mlockall() that processes may have within a VM (MEMLOCK).&lt;/p&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Xen is able to support multiple kernels while by design VServer cannot.
Xen also has greater support for virtualizing the network stack and allows for the possibility of VM migration, a feature that is possible for a COS design, but not yet available in VServer. VServer, in turn, maintains a small kernel footprint and performs equally with native Linux kernels in most cases.&lt;/p&gt;

&lt;h4 id=&#34;notes&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Undesired interactions between VMs are sometimes called cross-talk.&lt;/p&gt;

&lt;p&gt;COS - Container based Operating System&lt;/p&gt;

&lt;p&gt;TBF - token bucker filter&lt;/p&gt;

&lt;p&gt;HTB - Hierarchical Token Bucket&lt;/p&gt;

&lt;p&gt;RSS - maximum resident set size&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>