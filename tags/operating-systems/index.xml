<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Operating Systems on Ernestas Poškus.io</title>
    <link>http://out13.com/tags/operating-systems/</link>
    <description>Recent content in Operating Systems on Ernestas Poškus.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Aug 2017 21:48:19 +0300</lastBuildDate>
    
	<atom:link href="http://out13.com/tags/operating-systems/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Cooperative Task Management without Manual Stack Management</title>
      <link>http://out13.com/paper/cooperative-task-management-without-manual-stack-management/</link>
      <pubDate>Thu, 10 Aug 2017 21:48:19 +0300</pubDate>
      
      <guid>http://out13.com/paper/cooperative-task-management-without-manual-stack-management/</guid>
      <description>Or, Event-driven Programming is Not the Opposite of Threaded Programming Two programming styles as a conflation of two concepts: task management and stack management.
Those two concerns define a two-axis space in which &amp;lsquo;multithreaded&amp;rsquo; and &amp;lsquo;event-driven&amp;rsquo; programming are diagonally opposite; there is a third &amp;lsquo;sweet spot&amp;rsquo; in the space that combines the advantages of both programming styles.
Different task management approaches offer different granularities of atomicity on shared state. Conflict management considers how to convert available atomicity to a meaningful mechanism for avoiding resource conflicts.</description>
    </item>
    
    <item>
      <title>/proc/meminfo - memory usage statistics</title>
      <link>http://out13.com/tools/proc-meminfo/</link>
      <pubDate>Mon, 31 Jul 2017 19:25:47 +0300</pubDate>
      
      <guid>http://out13.com/tools/proc-meminfo/</guid>
      <description>Reports statistics about memory usage on the system. Useful for inspecting more granular memory usage.
 Sample output below.
 $ cat /proc/meminfo MemTotal: 12189912 kB MemFree: 231992 kB MemAvailable: 4174992 kB Buffers: 430884 kB Cached: 4515856 kB SwapCached: 60 kB Active: 8019760 kB Inactive: 3120804 kB Active(anon): 6121448 kB Inactive(anon): 1099620 kB Active(file): 1898312 kB Inactive(file): 2021184 kB Unevictable: 3088 kB Mlocked: 3088 kB SwapTotal: 12467708 kB SwapFree: 12467352 kB Dirty: 1568 kB Writeback: 0 kB AnonPages: 6196916 kB Mapped: 1339276 kB Shmem: 1027248 kB Slab: 584576 kB SReclaimable: 349700 kB SUnreclaim: 234876 kB KernelStack: 20800 kB PageTables: 93864 kB NFS_Unstable: 0 kB Bounce: 0 kB WritebackTmp: 0 kB CommitLimit: 18562664 kB Committed_AS: 19186116 kB VmallocTotal: 34359738367 kB VmallocUsed: 0 kB VmallocChunk: 0 kB HardwareCorrupted: 0 kB AnonHugePages: 1019904 kB CmaTotal: 0 kB CmaFree: 0 kB HugePages_Total: 0 HugePages_Free: 0 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 2048 kB DirectMap4k: 386324 kB DirectMap2M: 12083200 kB DirectMap1G: 0 kB  Interpreting each one  MemTotal: 12189912 kB</description>
    </item>
    
    <item>
      <title>Scaling Memcache at Facebook</title>
      <link>http://out13.com/paper/scaling-memcache-at-facebook/</link>
      <pubDate>Thu, 27 Jul 2017 19:47:58 +0300</pubDate>
      
      <guid>http://out13.com/paper/scaling-memcache-at-facebook/</guid>
      <description>Memcache at Facebook Largest memcached installation in the world, processing over a billion requests per second and storing trillions of items.
Items are distributed across the memcached servers through consistent hashing.
all web servers communicate with every memcached server in a short period of time. This all-to-all communication pattern can cause incast congestion or allow a single server to become the bottleneck for many web servers. Reduce latency mainly by focusing on the memcache client, which runs on each web server.</description>
    </item>
    
    <item>
      <title>slabtop - kernel slab cache information in real time</title>
      <link>http://out13.com/tools/slabtop-kernel-slab-cache-information-in-real-time/</link>
      <pubDate>Wed, 28 Jun 2017 18:57:58 +0300</pubDate>
      
      <guid>http://out13.com/tools/slabtop-kernel-slab-cache-information-in-real-time/</guid>
      <description>Displays detailed kernel slab cache information by aggregating /proc/slabinfo.
Tool shows a glimpse into kernel data structures.
 Sample output below.
 root@ow:~# slabtop -osc | head -n 20 Active / Total Objects (% used) : 4649227 / 4694474 (99.0%) Active / Total Slabs (% used) : 153429 / 153429 (100.0%) Active / Total Caches (% used) : 82 / 118 (69.5%) Active / Total Size (% used) : 1259115.61K / 1273939.</description>
    </item>
    
    <item>
      <title>In Search of an Understandable Consensus Algorithm</title>
      <link>http://out13.com/paper/in-search-of-an-understandable-consensus-algorithm/</link>
      <pubDate>Thu, 20 Apr 2017 19:13:57 +0300</pubDate>
      
      <guid>http://out13.com/paper/in-search-of-an-understandable-consensus-algorithm/</guid>
      <description>Raft Consensus algorithm for managing a replicated log.
Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces a stronger degree of coherency to reduce the number of states that must be considered.
Paxos first defines a protocol capable of reaching agreement on a single decision, such as a single replicated log entry.
Raft implements consensus by first electing a distinguished leader, then giving the leader complete responsibility for managing the replicated log.</description>
    </item>
    
    <item>
      <title>LIRS: An Efficient Low Inter-reference Recency Set Replacement Policy to Improve Buffer Cache Performance</title>
      <link>http://out13.com/paper/lirs-efficient-low-inter-reference-recency-set-replacement-policy-to-improve-buffer-cache-performance/</link>
      <pubDate>Thu, 09 Mar 2017 19:34:52 +0200</pubDate>
      
      <guid>http://out13.com/paper/lirs-efficient-low-inter-reference-recency-set-replacement-policy-to-improve-buffer-cache-performance/</guid>
      <description>LIRS LRU replacement policy has been commonly used in the buffer cache management, it is well known for its inability to cope with access patterns with weak locality.
LIRS effectively addresses the limits of LRU by using recency to evaluate Inter-Reference Recency (IRR) for making a replacement decision.
LRU inefficiency  Under the LRU policy, a burst of references to infrequently used blocks such as “sequential scans” through a large file, may cause replacement of commonly referenced blocks in the cache.</description>
    </item>
    
    <item>
      <title>TAO: Facebook’s Distributed Data Store for the Social Graph</title>
      <link>http://out13.com/paper/tao-facebooks-distributed-data-store-for-the-social-graph/</link>
      <pubDate>Thu, 15 Dec 2016 19:36:32 +0200</pubDate>
      
      <guid>http://out13.com/paper/tao-facebooks-distributed-data-store-for-the-social-graph/</guid>
      <description>Distributed data store for social graph TAO is geographically distributed data store that provides efficient and timely access to the social graph using a fixed set of queries. Read optimized, persisted in MySQL.
Inefficient edge lists: A key-value cache is not a good semantic fit for lists of edges; queries must always fetch the entire edge list and changes to a single edge require the entire list to be reloaded.</description>
    </item>
    
    <item>
      <title>Efficient Reconciliation and Flow Control for Anti-Entropy Protocols</title>
      <link>http://out13.com/paper/efficient-reconciliation-and-flow-control-for-anti-entropy-protocols/</link>
      <pubDate>Thu, 01 Dec 2016 16:05:39 +0200</pubDate>
      
      <guid>http://out13.com/paper/efficient-reconciliation-and-flow-control-for-anti-entropy-protocols/</guid>
      <description>Flow Gossip Anti-entropy, or gossip, is an attractive way of replicating state that does not have strong consistency requirements. With few limitations, updates spread in expected time that grows logarithmic in the number of participating hosts, even in the face of host failures and message loss. The behavior of update propagation is easily modeled with well-known epidemic analysis techniques.
Gossip basics There are two classes of gossip: anti-entropy and rumor mongering protocols.</description>
    </item>
    
    <item>
      <title>SEDA: An Architecture for Well-Conditioned, Scalable Internet Services</title>
      <link>http://out13.com/paper/seda-an-architecture-for-well-conditioned-scalable-internet-services/</link>
      <pubDate>Thu, 24 Nov 2016 19:50:13 +0200</pubDate>
      
      <guid>http://out13.com/paper/seda-an-architecture-for-well-conditioned-scalable-internet-services/</guid>
      <description>SEDA - staged event driven architecture A SEDA is intended to support massive concurrency demands and simplify the construction of well-conditioned services. In SEDA, applications consist of a network of event-driven stages connected by explicit queues. This architecture allows services to be well-conditioned to load, preventing resources from being overcommitted when demand exceeds service capacity.
SEDA combines aspects of threads and event-based programming models to manage the concurrency, I/O, scheduling, and resource management needs of Internet services.</description>
    </item>
    
    <item>
      <title>The Interaction of Buffer Size and TCP Protocol Handling and its Impact</title>
      <link>http://out13.com/paper/the-interaction-of-buffer-size-and-tcp-protocol-handling/</link>
      <pubDate>Thu, 17 Nov 2016 19:23:07 +0200</pubDate>
      
      <guid>http://out13.com/paper/the-interaction-of-buffer-size-and-tcp-protocol-handling/</guid>
      <description>Abstract Miercom was engaged by Cisco Systems to conduct independent testing of two vendors’ top of the line, data-center switch-routers, including the Cisco Nexus 92160YC-X and Nexus 9272Q switches and the Arista 7280SE-72 switch.
TCP Congestion Control versus System Buffer Management TCP congestion control. The Transmission Control Protocol (TCP) is the Layer-4 control protocol (atop IP at Layer 3) that ensures a block of data that’s sent is received intact.</description>
    </item>
    
    <item>
      <title>Replication Under Scalable Hashing: A Family of Algorithms for Scalable Decentralized Data Distribution</title>
      <link>http://out13.com/paper/replication-under-scalable-hashing--a-family-of-algorithms-for-scalable-decentralized-data-distribution/</link>
      <pubDate>Thu, 10 Nov 2016 22:27:23 +0200</pubDate>
      
      <guid>http://out13.com/paper/replication-under-scalable-hashing--a-family-of-algorithms-for-scalable-decentralized-data-distribution/</guid>
      <description>Replication Under Scalable Hashing Typical algorithms for decentralized data distribution work best in a system that is fully built before it first used; adding or removing components results in either extensive reorganization of data or load imbalance in the system.
RUSH variants also support weighting, allowing disks of different vintages to be added to a system.
RUSH variants is optimal or near-optimal reorganization. When new disks are added to the system, or old disks are retired, RUSH variants minimize the number of objects that need to be moved in order to bring the system back into balance.</description>
    </item>
    
    <item>
      <title>Dynamo: Amazon’s Highly Available Key-value Store</title>
      <link>http://out13.com/paper/dynamo-amazon-highly-available-key-value-store/</link>
      <pubDate>Sun, 06 Nov 2016 12:32:44 +0200</pubDate>
      
      <guid>http://out13.com/paper/dynamo-amazon-highly-available-key-value-store/</guid>
      <description>Dynamo Dynamo sacrifices Consistency for Availability under certain failure scenarios. It makes extensive use of object versioning and application-assisted conflict resolution in a manner that provides a novel interface for developers to use.
Gossip based distributed failure detection and membership protocol.
Query Model Read &amp;amp; Write operations to data item that is uniquely identified by a key. State is stored as blobs. Targets application that store objects up to 1MB.</description>
    </item>
    
    <item>
      <title>Bigtable: A Distributed Storage System for Structured Data</title>
      <link>http://out13.com/paper/bigtable-a-distributed-storage-system-for-structured-data/</link>
      <pubDate>Thu, 03 Nov 2016 19:54:45 +0200</pubDate>
      
      <guid>http://out13.com/paper/bigtable-a-distributed-storage-system-for-structured-data/</guid>
      <description>Bigtable Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers.
Bigtable does not support a full relational data model; instead, it provides clients with a simple data model that supports dynamic control over data layout and format, and allows clients to reason about the locality properties of the data represented in the underlying storage.</description>
    </item>
    
    <item>
      <title>Ownership is theft experiences building an embedded os in rust</title>
      <link>http://out13.com/paper/ownership-is-theft-experiences-building-an-embedded-os-in-rust/</link>
      <pubDate>Thu, 25 Aug 2016 20:39:03 +0300</pubDate>
      
      <guid>http://out13.com/paper/ownership-is-theft-experiences-building-an-embedded-os-in-rust/</guid>
      <description>Embedded OS in Rust Embedded systems:
 lack hardware protection mechanism less tolerant to crashes no easy way for debugging GC introduces non-deterministic delay  Rust Rust, a new systems programming language, provides compile-time memory safety checks to help eliminate runtime bugs that manifest from improper memory management.
Rust’s ownership model prevents otherwise safe resource sharing common in the embedded domain, conflicts with the reality of hardware resources, and hinders using closures for programming asynchronously.</description>
    </item>
    
    <item>
      <title>On the fly garbage collection</title>
      <link>http://out13.com/paper/on-the-fly-garbage-collection/</link>
      <pubDate>Thu, 25 Aug 2016 19:13:56 +0300</pubDate>
      
      <guid>http://out13.com/paper/on-the-fly-garbage-collection/</guid>
      <description>In our abstract form of the problem, we consider a directed graph of varying structure but with a fixed number of nodes, in which each node has at most two outgoing edges. More precisely, each node may have a left-hand outgoing edge and may have a right-hand outgoing edge, but either of them or both may be missing. In this graph a fixed set of nodes exists, called &amp;ldquo;the roots.</description>
    </item>
    
    <item>
      <title>Queues Are Databases</title>
      <link>http://out13.com/paper/queues-are-databases/</link>
      <pubDate>Fri, 12 Aug 2016 16:57:55 +0300</pubDate>
      
      <guid>http://out13.com/paper/queues-are-databases/</guid>
      <description>Queued transaction processing over pure client-server transaction processing. Queued systems are build on top of direct systems.
TP systems offer both queued and direct transaction processing. They offer both client-server and P2P direct processing.
Queue manager is best built as a naive resource manager atop an object-relational database system. That system must have good concurrency control, recovery, triggers, security, operations interfaces, and utilities.
Queues pose difficult problems when implemented atop a database:</description>
    </item>
    
    <item>
      <title>An Argument for Increasing TCP’s Initial Congestion Window</title>
      <link>http://out13.com/paper/an-argument-for-increasing-tcp-initial-congestion-window/</link>
      <pubDate>Thu, 04 Aug 2016 22:02:54 +0300</pubDate>
      
      <guid>http://out13.com/paper/an-argument-for-increasing-tcp-initial-congestion-window/</guid>
      <description>TCP congestion window  TCP flows start with initial congestion window of 4 segments (4KB of data).
 Window if critical for how quickly flows can finish.
Increase in 15KB congestion window improves average HTTP latency by 10%, mostly benefits RTT and BDP.
Slow start increases congestion window by the number of data segments acknowledged for each received ACK.
TCP latency is dominated by the number of round-trip times in slow-start phase.</description>
    </item>
    
    <item>
      <title>Tiny LFU highly efficient cache admission policy</title>
      <link>http://out13.com/paper/tiny-lfu-highly-efficient-cache-admission-policy/</link>
      <pubDate>Fri, 22 Apr 2016 21:26:15 +0300</pubDate>
      
      <guid>http://out13.com/paper/tiny-lfu-highly-efficient-cache-admission-policy/</guid>
      <description>Frequency based cache admission policy  Approximate LFU structure called TinyLFU, which maintains an approximate representation of the access frequency of a large sample of recently accessed items.
 TinyLFU is very compact and light-weight as it builds upon Bloom filter theory.
Usage The intuitive reason why caching works is that data accesses in many domains of computer science exhibit a considerable degree of “locality”.
When a data item is accessed, if it already appears in the cache, we say that there is a cache hit; otherwise, it is a cache miss.</description>
    </item>
    
    <item>
      <title>Container based operating system virtualization</title>
      <link>http://out13.com/paper/container-based-operating-system-virtualization/</link>
      <pubDate>Tue, 19 Apr 2016 19:30:48 +0300</pubDate>
      
      <guid>http://out13.com/paper/container-based-operating-system-virtualization/</guid>
      <description>Alternative to hypervisors.  Workload requirements for a given system will direct users to the point in the design space that requires the least trade-off.
 Sharing over isolation? Hypervisors often deployed to let a single machine host multiple, unrelated applications, which may run on behalf of independent organizations, as is common when a data center consolidates multiple physical servers. Hypervisors favor full isolation over sharing. However, when each virtual machine is running the same kernel and similar operating system distributions, the degree of isolation offered by hypervisors comes at the cost of efficiency relative to running all applications on a single kernel.</description>
    </item>
    
  </channel>
</rss>