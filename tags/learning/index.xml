<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Learning on Ernestas Poškus.io</title>
    <link>http://out13.com/tags/learning/</link>
    <description>Recent content in Learning on Ernestas Poškus.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Apr 2016 21:26:15 +0300</lastBuildDate>
    <atom:link href="http://out13.com/tags/learning/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Tiny LFU highly efficient cache admission policy</title>
      <link>http://out13.com/paper/tiny-lfu-highly-efficient-cache-admission-policy/</link>
      <pubDate>Fri, 22 Apr 2016 21:26:15 +0300</pubDate>
      
      <guid>http://out13.com/paper/tiny-lfu-highly-efficient-cache-admission-policy/</guid>
      <description>

&lt;h2 id=&#34;frequency-based-cache-admission-policy:68edccfde492ba8dc936e9e5aa09f425&#34;&gt;Frequency based cache admission policy&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Approximate LFU structure called TinyLFU, which maintains an approximate representation of the access frequency of a large sample of recently accessed items.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;TinyLFU is very compact and light-weight as it builds upon Bloom filter theory.&lt;/p&gt;

&lt;h3 id=&#34;usage:68edccfde492ba8dc936e9e5aa09f425&#34;&gt;Usage&lt;/h3&gt;

&lt;p&gt;The intuitive reason why caching works is that data accesses in many
domains of computer science exhibit a considerable degree of “locality”.&lt;/p&gt;

&lt;p&gt;When a data item is accessed, if it already appears in the cache, we say that there is a cache hit; otherwise, it is a cache miss. The ratio between the number of cache hits and the total number of data accesses is known as the cache hit-ratio.&lt;/p&gt;

&lt;p&gt;Admission policy - caching architecture in which an accessed item is only inserted into the cache if an admission policy decides that the cache hit ratio is likely to benefit from replacing it with the cache victim (as chosen by the cache’s replacement policy).&lt;/p&gt;

&lt;h3 id=&#34;architecture:68edccfde492ba8dc936e9e5aa09f425&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;The cache eviction policy picks a cache victim, while TinyLFU decides if replacing the cache victim with the new item is expected to increase the hit-ratio.
To do so, TinyLFU maintains statistics of items frequency over a sizable recent history. Storing these statistics is considered prohibitively expensive for practical implementation and therefore TinyLFU approximates them in a highly efficient manner. To keep the history fresh an aging process is performed periodically or incrementally to halve all of the counters.&lt;/p&gt;

&lt;h4 id=&#34;notes:68edccfde492ba8dc936e9e5aa09f425&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Time locality - access pattern, and consequently the corresponding probability distribution, change over time&lt;/p&gt;

&lt;p&gt;WLFU - Window Least Frequently Used, access frequency for a window, needs to keep track order of requests. Samples of the request stream (called window).&lt;/p&gt;

&lt;p&gt;PLFU - Perfect LFU, popularity based has metadata with counters&lt;/p&gt;

&lt;p&gt;In-memory LFU, outperformed by WLFU at the cost of larger meta-data&lt;/p&gt;

&lt;p&gt;SLRU - Segmented Least Recently Used, policy captures recent popularity by distinguishing between tem-porally popular items that are accessed at least twice in a short window vs. items accessed only once during that period&lt;/p&gt;

&lt;p&gt;LRU-K - combination of LRU &amp;amp; LFU the last K occurrences of each element are remembered. Using this data, LRU-K statistically estimates the momentary frequency of items in order to keep the most frequent pages in memory.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Container based operating system virtualization</title>
      <link>http://out13.com/paper/container-based-operating-system-virtualization/</link>
      <pubDate>Tue, 19 Apr 2016 19:30:48 +0300</pubDate>
      
      <guid>http://out13.com/paper/container-based-operating-system-virtualization/</guid>
      <description>

&lt;h2 id=&#34;alternative-to-hypervisors:88e79775d07b2065d6114eda0760b392&#34;&gt;Alternative to hypervisors.&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Workload requirements for a given system will direct users to the point in the design space that
requires the least trade-off.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;sharing-over-isolation:88e79775d07b2065d6114eda0760b392&#34;&gt;Sharing over isolation?&lt;/h3&gt;

&lt;p&gt;Hypervisors often deployed to let a single machine host multiple, unrelated
applications, which may run on behalf of independent organizations, as is common when a data center
consolidates multiple physical servers. Hypervisors favor full isolation over sharing.
However, when each virtual machine is running the same kernel and similar operating system
distributions, the degree of isolation offered by hypervisors comes at the cost of efficiency
relative to running all applications on a single kernel.&lt;/p&gt;

&lt;h3 id=&#34;usage:88e79775d07b2065d6114eda0760b392&#34;&gt;Usage&lt;/h3&gt;

&lt;p&gt;Software configuration problems incompatibilities between specific OS distributions.&lt;/p&gt;

&lt;p&gt;Resource isolation corresponds to the ability to account for and enforce the resource consumption of one VM such that guarantees and fair shares are preserved for other VM&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;Many hybrid approaches are also possible: for instance, a system may enforce fair sharing of resources between classes of VMs, which lets one overbook available resources while preventing starvation in overload scenarios.&lt;/p&gt;

&lt;p&gt;The key point is that both hypervisors and COS&amp;rsquo;s incorporate sophisticated resource schedulers to avoid or minimize crosstalk.&lt;/p&gt;

&lt;h3 id=&#34;security-isolation:88e79775d07b2065d6114eda0760b392&#34;&gt;Security isolation&lt;/h3&gt;

&lt;p&gt;Configuration independence - cannot conflict with other VM&amp;rsquo;s
Safety - global namespace shared&lt;/p&gt;

&lt;h3 id=&#34;fair-share-and-reservations:88e79775d07b2065d6114eda0760b392&#34;&gt;Fair share and Reservations&lt;/h3&gt;

&lt;p&gt;Vserver implements CPU isolation by overlaying a token TBF on top of standard O(1) Linux CPU scheduler.&lt;/p&gt;

&lt;p&gt;For memory storage one can specify the following limits:
 * a) the maximum resident set size (RSS)
 * b) number of anonymous memory pages have (ANON)
 * c) number of pages that may be pinned into memory using mlock() and mlockall() that processes may have within a VM (MEMLOCK).&lt;/p&gt;

&lt;h3 id=&#34;conclusion:88e79775d07b2065d6114eda0760b392&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Xen is able to support multiple kernels while by design VServer cannot.
Xen also has greater support for virtualizing the network stack and allows for the possibility of VM migration, a feature that is possible for a COS design, but not yet available in VServer. VServer, in turn, maintains a small kernel footprint and performs equally with native Linux kernels in most cases.&lt;/p&gt;

&lt;h4 id=&#34;notes:88e79775d07b2065d6114eda0760b392&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Undesired interactions between VMs are sometimes called cross-talk.&lt;/p&gt;

&lt;p&gt;COS - Container based Operating System&lt;/p&gt;

&lt;p&gt;TBF - token bucker filter&lt;/p&gt;

&lt;p&gt;HTB - Hierarchical Token Bucket&lt;/p&gt;

&lt;p&gt;RSS - maximum resident set size&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>