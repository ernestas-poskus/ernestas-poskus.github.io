<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Software Engineering on Ernestas Poškus.io</title>
    <link>http://out13.com/tags/software-engineering/</link>
    <description>Recent content in Software Engineering on Ernestas Poškus.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Apr 2016 19:50:29 +0300</lastBuildDate>
    <atom:link href="http://out13.com/tags/software-engineering/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center</title>
      <link>http://out13.com/paper/mesos-platform-for-resource-sharing/</link>
      <pubDate>Thu, 28 Apr 2016 19:50:29 +0300</pubDate>
      
      <guid>http://out13.com/paper/mesos-platform-for-resource-sharing/</guid>
      <description>

&lt;h2 id=&#34;platform-for-resource-sharing:7ec476dc4fb778dc0cf421ef9caa0b43&#34;&gt;Platform for resource sharing&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Sharing improves cluster utilization and avoids per-framework data repli-cation
Organizations will want to run multiple frameworks in the same cluster, picking the best one for each application.
Sharing a cluster between frameworks improves utilization and allows applications to share access to large datasets that may be too costly to replicate&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;architecture:7ec476dc4fb778dc0cf421ef9caa0b43&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;Mesos decides how many resources to offer each framework, based on an organizational policy such as fair sharing, while frameworks decide which resources to accept and which tasks to run on them.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Design philosophy - define a minimal interface that enables efficient resource sharing across frameworks, and otherwise push control of task scheduling and execution to the frameworks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The master decides how many resources to offer to each framework according to a given organizational policy, such as fair sharing, or strict priority.&lt;/p&gt;

&lt;p&gt;A framework running on top of Mesos consists of two components: a scheduler that registers with the master to be offered resources, and an executor process that is launched on slave nodes to run the framework’s tasks.&lt;/p&gt;

&lt;p&gt;Master determines how many resources are offered to each framework, the frameworks’ schedulers select which of the offered resources to use.&lt;/p&gt;

&lt;p&gt;When a frameworks accepts offered resources, it passes to Mesos a description of the tasks it wants to run on them.&lt;/p&gt;

&lt;p&gt;Frameworks achieve data locality by rejecting offers.&lt;/p&gt;

&lt;p&gt;Mesos can reallocate resources if cluster becomes filled with long tasks by revoking (killing) tasks with grace period.&lt;/p&gt;

&lt;p&gt;Isolation through existing OS isolation mechanisms usually system containers. These technologies can limit the CPU, memory, network bandwidth and I/O usage of a process tree.&lt;/p&gt;

&lt;p&gt;Mesos lets them short-circuit the rejection process and avoid communication by providing filters to the master. We support two types of filters: “only offer nodes from list L” and “only offer nodes with at least R resources free”.&lt;/p&gt;

&lt;p&gt;Two types of resources: mandatory and preferred&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A resource is mandatory if a framework must acquire it in order to run.&lt;/li&gt;
&lt;li&gt;Preferred if a framework performs “better” using it, but can also run using another equivalent resource.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;notes:7ec476dc4fb778dc0cf421ef9caa0b43&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Two-level scheduling mechanism called resource offers&lt;/p&gt;

&lt;p&gt;Delegating control over scheduling to the framework&lt;/p&gt;

&lt;p&gt;Resource offer - encapsulates a bundle of resources that a framework can allocate on a cluster node to run tasks&lt;/p&gt;

&lt;p&gt;Framework ramp-up time - time it takes a new framework to achieve its allocation&lt;/p&gt;

&lt;p&gt;Job completion time - time it takes a job to complete, assuming one job per framework;&lt;/p&gt;

&lt;p&gt;System utilization - total cluster utilization.&lt;/p&gt;

&lt;p&gt;Scale up - frameworks can elastically increase their allocation to take advantage of free resources.&lt;/p&gt;

&lt;p&gt;Scale down - frameworks can relinquish resources without significantly impacting their performance.&lt;/p&gt;

&lt;p&gt;Minimum allocation - frameworks require a certain minimum number of slots before they can start using their slots.&lt;/p&gt;

&lt;p&gt;Task distribution - distribution of the task durations. We consider both homogeneous and heterogeneous distributions.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Tiny LFU highly efficient cache admission policy</title>
      <link>http://out13.com/paper/tiny-lfu-highly-efficient-cache-admission-policy/</link>
      <pubDate>Fri, 22 Apr 2016 21:26:15 +0300</pubDate>
      
      <guid>http://out13.com/paper/tiny-lfu-highly-efficient-cache-admission-policy/</guid>
      <description>

&lt;h2 id=&#34;frequency-based-cache-admission-policy:68edccfde492ba8dc936e9e5aa09f425&#34;&gt;Frequency based cache admission policy&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Approximate LFU structure called TinyLFU, which maintains an approximate representation of the access frequency of a large sample of recently accessed items.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;TinyLFU is very compact and light-weight as it builds upon Bloom filter theory.&lt;/p&gt;

&lt;h3 id=&#34;usage:68edccfde492ba8dc936e9e5aa09f425&#34;&gt;Usage&lt;/h3&gt;

&lt;p&gt;The intuitive reason why caching works is that data accesses in many
domains of computer science exhibit a considerable degree of “locality”.&lt;/p&gt;

&lt;p&gt;When a data item is accessed, if it already appears in the cache, we say that there is a cache hit; otherwise, it is a cache miss. The ratio between the number of cache hits and the total number of data accesses is known as the cache hit-ratio.&lt;/p&gt;

&lt;p&gt;Admission policy - caching architecture in which an accessed item is only inserted into the cache if an admission policy decides that the cache hit ratio is likely to benefit from replacing it with the cache victim (as chosen by the cache’s replacement policy).&lt;/p&gt;

&lt;h3 id=&#34;architecture:68edccfde492ba8dc936e9e5aa09f425&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;The cache eviction policy picks a cache victim, while TinyLFU decides if replacing the cache victim with the new item is expected to increase the hit-ratio.
To do so, TinyLFU maintains statistics of items frequency over a sizable recent history. Storing these statistics is considered prohibitively expensive for practical implementation and therefore TinyLFU approximates them in a highly efficient manner. To keep the history fresh an aging process is performed periodically or incrementally to halve all of the counters.&lt;/p&gt;

&lt;h4 id=&#34;notes:68edccfde492ba8dc936e9e5aa09f425&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Time locality - access pattern, and consequently the corresponding probability distribution, change over time&lt;/p&gt;

&lt;p&gt;WLFU - Window Least Frequently Used, access frequency for a window, needs to keep track order of requests. Samples of the request stream (called window).&lt;/p&gt;

&lt;p&gt;PLFU - Perfect LFU, popularity based has metadata with counters&lt;/p&gt;

&lt;p&gt;In-memory LFU, outperformed by WLFU at the cost of larger meta-data&lt;/p&gt;

&lt;p&gt;SLRU - Segmented Least Recently Used, policy captures recent popularity by distinguishing between tem-porally popular items that are accessed at least twice in a short window vs. items accessed only once during that period&lt;/p&gt;

&lt;p&gt;LRU-K - combination of LRU &amp;amp; LFU the last K occurrences of each element are remembered. Using this data, LRU-K statistically estimates the momentary frequency of items in order to keep the most frequent pages in memory.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Container based operating system virtualization</title>
      <link>http://out13.com/paper/container-based-operating-system-virtualization/</link>
      <pubDate>Tue, 19 Apr 2016 19:30:48 +0300</pubDate>
      
      <guid>http://out13.com/paper/container-based-operating-system-virtualization/</guid>
      <description>

&lt;h2 id=&#34;alternative-to-hypervisors:88e79775d07b2065d6114eda0760b392&#34;&gt;Alternative to hypervisors.&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Workload requirements for a given system will direct users to the point in the design space that
requires the least trade-off.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;sharing-over-isolation:88e79775d07b2065d6114eda0760b392&#34;&gt;Sharing over isolation?&lt;/h3&gt;

&lt;p&gt;Hypervisors often deployed to let a single machine host multiple, unrelated
applications, which may run on behalf of independent organizations, as is common when a data center
consolidates multiple physical servers. Hypervisors favor full isolation over sharing.
However, when each virtual machine is running the same kernel and similar operating system
distributions, the degree of isolation offered by hypervisors comes at the cost of efficiency
relative to running all applications on a single kernel.&lt;/p&gt;

&lt;h3 id=&#34;usage:88e79775d07b2065d6114eda0760b392&#34;&gt;Usage&lt;/h3&gt;

&lt;p&gt;Software configuration problems incompatibilities between specific OS distributions.&lt;/p&gt;

&lt;p&gt;Resource isolation corresponds to the ability to account for and enforce the resource consumption of one VM such that guarantees and fair shares are preserved for other VM&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;Many hybrid approaches are also possible: for instance, a system may enforce fair sharing of resources between classes of VMs, which lets one overbook available resources while preventing starvation in overload scenarios.&lt;/p&gt;

&lt;p&gt;The key point is that both hypervisors and COS&amp;rsquo;s incorporate sophisticated resource schedulers to avoid or minimize crosstalk.&lt;/p&gt;

&lt;h3 id=&#34;security-isolation:88e79775d07b2065d6114eda0760b392&#34;&gt;Security isolation&lt;/h3&gt;

&lt;p&gt;Configuration independence - cannot conflict with other VM&amp;rsquo;s
Safety - global namespace shared&lt;/p&gt;

&lt;h3 id=&#34;fair-share-and-reservations:88e79775d07b2065d6114eda0760b392&#34;&gt;Fair share and Reservations&lt;/h3&gt;

&lt;p&gt;Vserver implements CPU isolation by overlaying a token TBF on top of standard O(1) Linux CPU scheduler.&lt;/p&gt;

&lt;p&gt;For memory storage one can specify the following limits:
 * a) the maximum resident set size (RSS)
 * b) number of anonymous memory pages have (ANON)
 * c) number of pages that may be pinned into memory using mlock() and mlockall() that processes may have within a VM (MEMLOCK).&lt;/p&gt;

&lt;h3 id=&#34;conclusion:88e79775d07b2065d6114eda0760b392&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Xen is able to support multiple kernels while by design VServer cannot.
Xen also has greater support for virtualizing the network stack and allows for the possibility of VM migration, a feature that is possible for a COS design, but not yet available in VServer. VServer, in turn, maintains a small kernel footprint and performs equally with native Linux kernels in most cases.&lt;/p&gt;

&lt;h4 id=&#34;notes:88e79775d07b2065d6114eda0760b392&#34;&gt;Notes&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Undesired interactions between VMs are sometimes called cross-talk.&lt;/p&gt;

&lt;p&gt;COS - Container based Operating System&lt;/p&gt;

&lt;p&gt;TBF - token bucker filter&lt;/p&gt;

&lt;p&gt;HTB - Hierarchical Token Bucket&lt;/p&gt;

&lt;p&gt;RSS - maximum resident set size&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>